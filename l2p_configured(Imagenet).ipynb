{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90TRrimFstfu",
        "outputId": "354d56dd-d188-4951-cddf-22cddbbda375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'l2p-pytorch'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 172 (delta 91), reused 105 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (172/172), 67.80 KiB | 5.65 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JH-LEE-KR/l2p-pytorch\n",
        "!cd l2p-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/l2p-pytorch/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "vfW9EfPptjAf",
        "outputId": "237e8239-e905-4d1c-e255-5ad16c593c49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.3\n",
            "  Downloading matplotlib-3.5.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchprofile==0.0.4\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (0.15.1+cu118)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: pillow, matplotlib, torchprofile, timm\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed matplotlib-3.5.3 pillow-9.2.0 timm-0.6.7 torchprofile-0.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xv1qg6OpQvM",
        "outputId": "01b0b3bc-8664-4324-ae4c-97580f0ff554"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/Shareddrives/Colab/AIP_PROJECT/l2p_configs_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm3QGoxWpc_H",
        "outputId": "22bc2213-3cfb-412d-a5f5-a2bcf1587ff0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/Shareddrives/Colab/AIP_PROJECT/l2p_configs_files.zip\n",
            "  inflating: cifar100_l2p.py         \n",
            "  inflating: datasets.py             \n",
            "  inflating: Imagenet_R.py           \n",
            "  inflating: main.py                 \n",
            "  inflating: TinyImagenet.py         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/l2p-pytorch/main.py\n",
        "!rm /content/l2p-pytorch/datasets.py\n",
        "!rm /content/l2p-pytorch/configs/cifar100_l2p.py\n",
        "!cp /content/cifar100_l2p.py /content/l2p-pytorch/configs/cifar100_l2p.py\n",
        "!cp /content/TinyImagenet.py /content/l2p-pytorch/configs/TinyImagenet.py\n",
        "!cp /content/Imagenet_R.py /content/l2p-pytorch/configs/Imagenet_R.py\n",
        "!cp /content/datasets.py /content/l2p-pytorch/datasets.py\n",
        "!cp /content/main.py /content/l2p-pytorch/main.py"
      ],
      "metadata": {
        "id": "S8X6GXjIovvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        cifar100_l2p \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB7KPZw1tpvm",
        "outputId": "d856b762-abdc-415f-adf7-1c332b2c028b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "MAIN OK\n",
            "| distributed init (rank 0): env://\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /local_datasets/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:06<00:00, 27228007.24it/s]\n",
            "Extracting /local_datasets/cifar-100-python.tar.gz to /local_datasets/\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='cifar100_l2p', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 122980\n",
            "Start training for 5 epochs\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:14:54  Lr: 0.001875  Loss: 2.3091  Acc@1: 25.0000 (25.0000)  Acc@5: 43.7500 (43.7500)  time: 2.8576  data: 0.3082  max mem: 2371\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:51  Lr: 0.001875  Loss: 2.0322  Acc@1: 43.7500 (40.9091)  Acc@5: 75.0000 (73.2955)  time: 0.7649  data: 0.0289  max mem: 2372\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2680 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2680 closing signal SIGTERM\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2659 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 730, in run\n",
            "    self._shutdown(e.sigval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py\", line 289, in _shutdown\n",
            "    self._pcontext.close(death_sig)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 331, in close\n",
            "    self._close(death_sig=death_sig, timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 708, in _close\n",
            "    handler.proc.wait(time_to_wait)\n",
            "  File \"/usr/lib/python3.9/subprocess.py\", line 1189, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "  File \"/usr/lib/python3.9/subprocess.py\", line 1911, in _wait\n",
            "    time.sleep(delay)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2659 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        TinyImagenet \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45kqXo8eqRXL",
        "outputId": "83a0ffb5-8113-475c-a16d-507723499f70"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading from http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Downloading http://cs231n.stanford.edu/tiny-imagenet-200.zip to /local_datasets/tiny-imagenet-200.zip\n",
            "100% 248100043/248100043 [00:16<00:00, 15451121.80it/s]\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='TinyImagenet', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='TinyImagenet', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=200)\n",
            "number of params: 199880\n",
            "Start training for 5 epochs\n",
            "Train: Epoch[1/5]  [  0/625]  eta: 0:15:38  Lr: 0.001875  Loss: 3.0040  Acc@1: 6.2500 (6.2500)  Acc@5: 18.7500 (18.7500)  time: 1.5012  data: 0.2465  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 10/625]  eta: 0:06:39  Lr: 0.001875  Loss: 2.8529  Acc@1: 18.7500 (18.7500)  Acc@5: 50.0000 (41.4773)  time: 0.6498  data: 0.0239  max mem: 2374\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2877 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1086, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2877 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    log.warning(f\"Received {e.sigval} death signal, shutting down workers\")\n",
            "Message: 'Received 2 death signal, shutting down workers'\n",
            "Arguments: ()\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2886 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/l2p-pytorch/main.py\", line 175, in <module>\n",
            "    main(args)\n",
            "  File \"/content/l2p-pytorch/main.py\", line 135, in main\n",
            "    train_and_evaluate(model, model_without_ddp, original_model,\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 232, in train_and_evaluate\n",
            "    train_stats = train_one_epoch(model=model, original_model=original_model, criterion=criterion, \n",
            "  File \"/content/l2p-pytorch/engine.py\", line 56, in train_one_epoch\n",
            "    output = model(input, task_id=task_id, cls_features=cls_features, train=set_training_mode)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 514, in forward\n",
            "    res = self.forward_features(x, task_id=task_id, cls_features=cls_features, train=train)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 470, in forward_features\n",
            "    res = self.prompt(x, prompt_mask=prompt_mask, cls_features=cls_features)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 64, in forward\n",
            "    prompt_norm = self.l2_normalize(self.prompt_key, dim=1) # Pool_size, C\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 44, in l2_normalize\n",
            "    x_inv_norm = torch.rsqrt(torch.maximum(square_sum, torch.tensor(epsilon, device=x.device)))\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2877 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        Imagenet-R \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aBGWfUqUZI",
        "outputId": "2fdc7266-2285-4cdd-8165-306b6e8d13e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Downloading https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar to /local_datasets/imagenet-r.tar\n",
            "100% 2191079936/2191079936 [01:50<00:00, 19912988.56it/s]\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='Imagenet-R', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='Imagenet-R', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=200)\n",
            "number of params: 199880\n",
            "Start training for 5 epochs\n",
            "Train: Epoch[1/5]  [  0/162]  eta: 0:06:09  Lr: 0.001875  Loss: 3.0009  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  time: 2.2787  data: 0.6327  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 10/162]  eta: 0:01:49  Lr: 0.001875  Loss: 2.8325  Acc@1: 18.7500 (20.4545)  Acc@5: 43.7500 (42.6136)  time: 0.7224  data: 0.0578  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 20/162]  eta: 0:01:32  Lr: 0.001875  Loss: 2.7956  Acc@1: 18.7500 (17.8571)  Acc@5: 43.7500 (41.0714)  time: 0.5698  data: 0.0004  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 30/162]  eta: 0:01:22  Lr: 0.001875  Loss: 2.5015  Acc@1: 18.7500 (24.5968)  Acc@5: 50.0000 (48.5887)  time: 0.5741  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 40/162]  eta: 0:01:15  Lr: 0.001875  Loss: 2.3596  Acc@1: 43.7500 (29.8780)  Acc@5: 68.7500 (53.6585)  time: 0.5789  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 50/162]  eta: 0:01:08  Lr: 0.001875  Loss: 2.5327  Acc@1: 43.7500 (31.9853)  Acc@5: 68.7500 (57.3529)  time: 0.5835  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 60/162]  eta: 0:01:01  Lr: 0.001875  Loss: 2.4281  Acc@1: 43.7500 (33.5041)  Acc@5: 68.7500 (59.3238)  time: 0.5869  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 70/162]  eta: 0:00:55  Lr: 0.001875  Loss: 2.6011  Acc@1: 43.7500 (35.0352)  Acc@5: 68.7500 (61.0035)  time: 0.5928  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 80/162]  eta: 0:00:49  Lr: 0.001875  Loss: 2.1350  Acc@1: 37.5000 (35.9568)  Acc@5: 68.7500 (62.5772)  time: 0.5998  data: 0.0033  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 90/162]  eta: 0:00:43  Lr: 0.001875  Loss: 2.1097  Acc@1: 43.7500 (37.6374)  Acc@5: 75.0000 (64.2170)  time: 0.6035  data: 0.0023  max mem: 2374\n",
            "Train: Epoch[1/5]  [100/162]  eta: 0:00:37  Lr: 0.001875  Loss: 2.0415  Acc@1: 50.0000 (38.7995)  Acc@5: 81.2500 (65.7797)  time: 0.6051  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[1/5]  [110/162]  eta: 0:00:31  Lr: 0.001875  Loss: 1.9966  Acc@1: 50.0000 (39.8086)  Acc@5: 81.2500 (67.5676)  time: 0.6115  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[1/5]  [120/162]  eta: 0:00:25  Lr: 0.001875  Loss: 1.9253  Acc@1: 50.0000 (40.5475)  Acc@5: 87.5000 (68.6983)  time: 0.6214  data: 0.0009  max mem: 2374\n",
            "Train: Epoch[1/5]  [130/162]  eta: 0:00:19  Lr: 0.001875  Loss: 1.6153  Acc@1: 56.2500 (42.4141)  Acc@5: 87.5000 (69.7996)  time: 0.6290  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[1/5]  [140/162]  eta: 0:00:13  Lr: 0.001875  Loss: 1.8425  Acc@1: 56.2500 (42.9965)  Acc@5: 81.2500 (70.6117)  time: 0.6299  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[1/5]  [150/162]  eta: 0:00:07  Lr: 0.001875  Loss: 1.6304  Acc@1: 56.2500 (44.2053)  Acc@5: 81.2500 (71.7301)  time: 0.6273  data: 0.0018  max mem: 2374\n",
            "Train: Epoch[1/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.7043  Acc@1: 50.0000 (44.5264)  Acc@5: 87.5000 (72.3602)  time: 0.6268  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[1/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 2.7187  Acc@1: 50.0000 (44.4789)  Acc@5: 87.5000 (72.2976)  time: 0.6086  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[1/5] Total time: 0:01:39 (0.6111 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 2.7187  Acc@1: 50.0000 (44.4789)  Acc@5: 87.5000 (72.2976)\n",
            "Train: Epoch[2/5]  [  0/162]  eta: 0:03:26  Lr: 0.001875  Loss: 1.5074  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 1.2773  data: 0.6862  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 10/162]  eta: 0:01:44  Lr: 0.001875  Loss: 1.5733  Acc@1: 62.5000 (61.9318)  Acc@5: 93.7500 (90.3409)  time: 0.6882  data: 0.0669  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 20/162]  eta: 0:01:33  Lr: 0.001875  Loss: 1.8015  Acc@1: 62.5000 (58.9286)  Acc@5: 87.5000 (87.7976)  time: 0.6295  data: 0.0026  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 30/162]  eta: 0:01:26  Lr: 0.001875  Loss: 1.5458  Acc@1: 56.2500 (58.4677)  Acc@5: 87.5000 (87.0968)  time: 0.6314  data: 0.0006  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 40/162]  eta: 0:01:19  Lr: 0.001875  Loss: 1.3075  Acc@1: 56.2500 (59.2988)  Acc@5: 87.5000 (87.0427)  time: 0.6350  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 50/162]  eta: 0:01:12  Lr: 0.001875  Loss: 1.2734  Acc@1: 56.2500 (59.1912)  Acc@5: 87.5000 (86.8873)  time: 0.6398  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 60/162]  eta: 0:01:06  Lr: 0.001875  Loss: 1.8823  Acc@1: 56.2500 (58.9139)  Acc@5: 81.2500 (86.3730)  time: 0.6455  data: 0.0030  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 70/162]  eta: 0:00:59  Lr: 0.001875  Loss: 1.5845  Acc@1: 56.2500 (59.1549)  Acc@5: 81.2500 (86.1796)  time: 0.6504  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 80/162]  eta: 0:00:53  Lr: 0.001875  Loss: 1.0828  Acc@1: 56.2500 (59.1049)  Acc@5: 87.5000 (86.4198)  time: 0.6540  data: 0.0026  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 90/162]  eta: 0:00:46  Lr: 0.001875  Loss: 1.2943  Acc@1: 62.5000 (59.6841)  Acc@5: 87.5000 (86.3324)  time: 0.6571  data: 0.0025  max mem: 2374\n",
            "Train: Epoch[2/5]  [100/162]  eta: 0:00:40  Lr: 0.001875  Loss: 1.0430  Acc@1: 62.5000 (60.3342)  Acc@5: 87.5000 (86.7574)  time: 0.6584  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[2/5]  [110/162]  eta: 0:00:33  Lr: 0.001875  Loss: 1.0128  Acc@1: 62.5000 (60.2477)  Acc@5: 87.5000 (86.8806)  time: 0.6585  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[2/5]  [120/162]  eta: 0:00:27  Lr: 0.001875  Loss: 1.3258  Acc@1: 62.5000 (60.7438)  Acc@5: 87.5000 (87.1901)  time: 0.6589  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[2/5]  [130/162]  eta: 0:00:20  Lr: 0.001875  Loss: 1.2078  Acc@1: 62.5000 (60.7347)  Acc@5: 87.5000 (86.8321)  time: 0.6594  data: 0.0025  max mem: 2374\n",
            "Train: Epoch[2/5]  [140/162]  eta: 0:00:14  Lr: 0.001875  Loss: 1.4398  Acc@1: 62.5000 (61.1259)  Acc@5: 87.5000 (86.9681)  time: 0.6615  data: 0.0020  max mem: 2374\n",
            "Train: Epoch[2/5]  [150/162]  eta: 0:00:07  Lr: 0.001875  Loss: 1.5331  Acc@1: 68.7500 (61.6722)  Acc@5: 87.5000 (87.1689)  time: 0.6619  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[2/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.1098  Acc@1: 68.7500 (61.9953)  Acc@5: 87.5000 (87.3835)  time: 0.6572  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[2/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 1.9190  Acc@1: 62.5000 (61.9527)  Acc@5: 87.5000 (87.3305)  time: 0.6344  data: 0.0009  max mem: 2374\n",
            "Train: Epoch[2/5] Total time: 0:01:45 (0.6518 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.9190  Acc@1: 62.5000 (61.9527)  Acc@5: 87.5000 (87.3305)\n",
            "Train: Epoch[3/5]  [  0/162]  eta: 0:03:06  Lr: 0.001875  Loss: 1.8111  Acc@1: 37.5000 (37.5000)  Acc@5: 75.0000 (75.0000)  time: 1.1543  data: 0.5291  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 10/162]  eta: 0:01:46  Lr: 0.001875  Loss: 0.9699  Acc@1: 68.7500 (63.6364)  Acc@5: 81.2500 (84.6591)  time: 0.7022  data: 0.0504  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 20/162]  eta: 0:01:36  Lr: 0.001875  Loss: 1.1219  Acc@1: 68.7500 (64.8810)  Acc@5: 87.5000 (86.3095)  time: 0.6574  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 30/162]  eta: 0:01:28  Lr: 0.001875  Loss: 1.3365  Acc@1: 62.5000 (64.9194)  Acc@5: 87.5000 (86.6935)  time: 0.6584  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 40/162]  eta: 0:01:21  Lr: 0.001875  Loss: 1.0953  Acc@1: 68.7500 (65.3963)  Acc@5: 87.5000 (88.4146)  time: 0.6604  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 50/162]  eta: 0:01:15  Lr: 0.001875  Loss: 1.2841  Acc@1: 68.7500 (66.2990)  Acc@5: 93.7500 (89.2157)  time: 0.6631  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 60/162]  eta: 0:01:08  Lr: 0.001875  Loss: 0.9196  Acc@1: 68.7500 (66.5984)  Acc@5: 93.7500 (89.1393)  time: 0.6630  data: 0.0025  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 70/162]  eta: 0:01:01  Lr: 0.001875  Loss: 1.2551  Acc@1: 62.5000 (66.1972)  Acc@5: 87.5000 (88.7324)  time: 0.6622  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 80/162]  eta: 0:00:54  Lr: 0.001875  Loss: 1.0674  Acc@1: 68.7500 (66.5123)  Acc@5: 93.7500 (89.3519)  time: 0.6628  data: 0.0019  max mem: 2374\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 3182 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 650, in format\n",
            "    def format(self, record):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 3182 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    log.warning(f\"Received {e.sigval} death signal, shutting down workers\")\n",
            "Message: 'Received 2 death signal, shutting down workers'\n",
            "Arguments: ()\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3195 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/l2p-pytorch/main.py\", line 175, in <module>\n",
            "    main(args)\n",
            "  File \"/content/l2p-pytorch/main.py\", line 135, in main\n",
            "    train_and_evaluate(model, model_without_ddp, original_model,\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 232, in train_and_evaluate\n",
            "    train_stats = train_one_epoch(model=model, original_model=original_model, criterion=criterion, \n",
            "  File \"/content/l2p-pytorch/engine.py\", line 63, in train_one_epoch\n",
            "    not_mask = torch.tensor(not_mask, dtype=torch.int64).to(device)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 3182 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py cifar100_l2p --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbiUfIHyQ02t",
        "outputId": "77031b1c-eb51-49a0-aa61-3381abe60b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='cifar100_l2p', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=True, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "Loading checkpoint from: ./output/checkpoint/task1_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:01:35  Loss: 0.4389 (0.4389)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 1.5233  data: 0.3560  max mem: 1156\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:25  Loss: 0.4181 (0.4133)  Acc@1: 100.0000 (97.1591)  Acc@5: 100.0000 (99.4318)  time: 0.4889  data: 0.0344  max mem: 1157\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4181 (0.4811)  Acc@1: 93.7500 (96.4286)  Acc@5: 100.0000 (99.7024)  time: 0.3854  data: 0.0014  max mem: 1157\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3209 (0.4284)  Acc@1: 100.0000 (97.3790)  Acc@5: 100.0000 (99.7984)  time: 0.3891  data: 0.0005  max mem: 1157\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.2965 (0.4261)  Acc@1: 100.0000 (97.5610)  Acc@5: 100.0000 (99.8476)  time: 0.3957  data: 0.0020  max mem: 1157\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3595 (0.4154)  Acc@1: 100.0000 (97.7941)  Acc@5: 100.0000 (99.7549)  time: 0.4019  data: 0.0041  max mem: 1157\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4022 (0.4095)  Acc@1: 100.0000 (98.0533)  Acc@5: 100.0000 (99.7951)  time: 0.4085  data: 0.0025  max mem: 1157\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4030 (0.4104)  Acc@1: 100.0000 (98.1000)  Acc@5: 100.0000 (99.8000)  time: 0.4006  data: 0.0018  max mem: 1157\n",
            "Test: [Task 1] Total time: 0:00:26 (0.4133 s / it)\n",
            "* Acc@1 98.100 Acc@5 99.800 loss 0.410\n",
            "[Average accuracy till task1]\tAcc@1: 98.1000\tAcc@5: 99.8000\tLoss: 0.4104\n",
            "Loading checkpoint from: ./output/checkpoint/task2_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:48  Loss: 0.4949 (0.4949)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7649  data: 0.3692  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.4524 (0.4691)  Acc@1: 93.7500 (93.1818)  Acc@5: 100.0000 (99.4318)  time: 0.4494  data: 0.0365  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4427 (0.5171)  Acc@1: 93.7500 (92.2619)  Acc@5: 100.0000 (99.7024)  time: 0.4181  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3931 (0.4850)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (99.7984)  time: 0.4209  data: 0.0011  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.4260 (0.4748)  Acc@1: 93.7500 (92.8354)  Acc@5: 100.0000 (99.8476)  time: 0.4268  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3854 (0.4554)  Acc@1: 93.7500 (93.6275)  Acc@5: 100.0000 (99.7549)  time: 0.4343  data: 0.0025  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3854 (0.4521)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.7951)  time: 0.4427  data: 0.0008  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3854 (0.4503)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.8000)  time: 0.4332  data: 0.0004  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4332 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.800 loss 0.450\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:48  Loss: 0.5508 (0.5508)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7630  data: 0.3423  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:25  Loss: 0.5508 (0.5424)  Acc@1: 93.7500 (95.4545)  Acc@5: 100.0000 (99.4318)  time: 0.4818  data: 0.0327  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:20  Loss: 0.5631 (0.6228)  Acc@1: 93.7500 (94.0476)  Acc@5: 100.0000 (99.4048)  time: 0.4555  data: 0.0011  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:15  Loss: 0.6539 (0.6270)  Acc@1: 93.7500 (93.1452)  Acc@5: 100.0000 (98.9919)  time: 0.4562  data: 0.0018  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6045 (0.6057)  Acc@1: 93.7500 (93.5976)  Acc@5: 100.0000 (99.0854)  time: 0.4503  data: 0.0018  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5034 (0.5978)  Acc@1: 93.7500 (93.2598)  Acc@5: 100.0000 (99.1422)  time: 0.4418  data: 0.0006  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.4732 (0.5745)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.2828)  time: 0.4357  data: 0.0017  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4657 (0.5666)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.3000)  time: 0.4236  data: 0.0017  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:28 (0.4497 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.300 loss 0.567\n",
            "[Average accuracy till task2]\tAcc@1: 93.8000\tAcc@5: 99.5500\tLoss: 0.5085\tForgetting: 4.3000\tBackward: -4.3000\n",
            "Loading checkpoint from: ./output/checkpoint/task3_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:45  Loss: 0.5178 (0.5178)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7250  data: 0.3305  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:24  Loss: 0.5002 (0.4788)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4529  data: 0.0307  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4875 (0.5342)  Acc@1: 93.7500 (89.8810)  Acc@5: 100.0000 (99.7024)  time: 0.4255  data: 0.0005  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.4442 (0.5114)  Acc@1: 93.7500 (90.3226)  Acc@5: 100.0000 (99.7984)  time: 0.4243  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.4416 (0.4966)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (99.8476)  time: 0.4226  data: 0.0026  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3982 (0.4744)  Acc@1: 93.7500 (91.6667)  Acc@5: 100.0000 (99.7549)  time: 0.4217  data: 0.0010  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3946 (0.4658)  Acc@1: 93.7500 (91.7008)  Acc@5: 100.0000 (99.7951)  time: 0.4226  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3946 (0.4639)  Acc@1: 93.7500 (91.9000)  Acc@5: 100.0000 (99.8000)  time: 0.4122  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:26 (0.4265 s / it)\n",
            "* Acc@1 91.900 Acc@5 99.800 loss 0.464\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:43  Loss: 0.6183 (0.6183)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6958  data: 0.3065  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.5662 (0.6001)  Acc@1: 100.0000 (95.4545)  Acc@5: 100.0000 (98.2955)  time: 0.4476  data: 0.0290  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.6122 (0.6841)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4248  data: 0.0008  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6694 (0.6667)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (98.3871)  time: 0.4280  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:09  Loss: 0.6524 (0.6528)  Acc@1: 93.7500 (92.5305)  Acc@5: 100.0000 (98.6280)  time: 0.4308  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5743 (0.6429)  Acc@1: 93.7500 (92.4020)  Acc@5: 100.0000 (98.7745)  time: 0.4338  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5022 (0.6258)  Acc@1: 93.7500 (92.4180)  Acc@5: 100.0000 (98.9754)  time: 0.4363  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4910 (0.6172)  Acc@1: 93.7500 (92.5000)  Acc@5: 100.0000 (99.0000)  time: 0.4258  data: 0.0003  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4327 s / it)\n",
            "* Acc@1 92.500 Acc@5 99.000 loss 0.617\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:45  Loss: 0.2563 (0.2563)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7235  data: 0.3205  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:24  Loss: 0.4239 (0.4755)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4632  data: 0.0313  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.4874 (0.5017)  Acc@1: 87.5000 (90.1786)  Acc@5: 100.0000 (98.8095)  time: 0.4387  data: 0.0016  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4954 (0.4892)  Acc@1: 93.7500 (90.9274)  Acc@5: 100.0000 (98.9919)  time: 0.4399  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.3972 (0.4820)  Acc@1: 93.7500 (91.4634)  Acc@5: 100.0000 (99.0854)  time: 0.4398  data: 0.0023  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4298 (0.4836)  Acc@1: 93.7500 (91.2990)  Acc@5: 100.0000 (98.8971)  time: 0.4395  data: 0.0006  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.4516 (0.4939)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (99.0779)  time: 0.4387  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5528 (0.4960)  Acc@1: 87.5000 (91.2000)  Acc@5: 100.0000 (99.0000)  time: 0.4275  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4412 s / it)\n",
            "* Acc@1 91.200 Acc@5 99.000 loss 0.496\n",
            "[Average accuracy till task3]\tAcc@1: 91.8667\tAcc@5: 99.2667\tLoss: 0.5257\tForgetting: 3.7500\tBackward: -3.7500\n",
            "Loading checkpoint from: ./output/checkpoint/task4_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:39  Loss: 0.5862 (0.5862)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6191  data: 0.2208  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5155 (0.4946)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4503  data: 0.0209  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.5155 (0.5422)  Acc@1: 93.7500 (89.2857)  Acc@5: 100.0000 (99.4048)  time: 0.4335  data: 0.0010  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5011 (0.5325)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (99.3952)  time: 0.4323  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4808 (0.5196)  Acc@1: 87.5000 (89.1768)  Acc@5: 100.0000 (99.5427)  time: 0.4314  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4410 (0.5057)  Acc@1: 93.7500 (89.8284)  Acc@5: 100.0000 (99.5098)  time: 0.4304  data: 0.0007  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4410 (0.4955)  Acc@1: 93.7500 (90.1639)  Acc@5: 100.0000 (99.3852)  time: 0.4292  data: 0.0023  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4410 (0.4961)  Acc@1: 93.7500 (90.4000)  Acc@5: 100.0000 (99.4000)  time: 0.4183  data: 0.0023  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4319 s / it)\n",
            "* Acc@1 90.400 Acc@5 99.400 loss 0.496\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:37  Loss: 0.7167 (0.7167)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6018  data: 0.2092  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.6513 (0.6737)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (98.8636)  time: 0.4448  data: 0.0194  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.6781 (0.7447)  Acc@1: 87.5000 (87.7976)  Acc@5: 100.0000 (98.5119)  time: 0.4296  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6909 (0.7304)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (97.7823)  time: 0.4302  data: 0.0033  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:09  Loss: 0.6816 (0.7167)  Acc@1: 87.5000 (88.8720)  Acc@5: 100.0000 (98.1707)  time: 0.4309  data: 0.0014  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5950 (0.7098)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (98.4069)  time: 0.4321  data: 0.0012  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5703 (0.6861)  Acc@1: 87.5000 (88.8320)  Acc@5: 100.0000 (98.5656)  time: 0.4333  data: 0.0024  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5563 (0.6762)  Acc@1: 87.5000 (88.9000)  Acc@5: 100.0000 (98.6000)  time: 0.4225  data: 0.0024  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4317 s / it)\n",
            "* Acc@1 88.900 Acc@5 98.600 loss 0.676\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:36  Loss: 0.3755 (0.3755)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5740  data: 0.1795  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5362 (0.5234)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4480  data: 0.0169  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.5362 (0.5304)  Acc@1: 87.5000 (88.9881)  Acc@5: 100.0000 (98.2143)  time: 0.4356  data: 0.0019  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4622 (0.5213)  Acc@1: 87.5000 (89.3145)  Acc@5: 100.0000 (98.5887)  time: 0.4359  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4136 (0.5136)  Acc@1: 87.5000 (89.4817)  Acc@5: 100.0000 (98.9329)  time: 0.4368  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4773 (0.5184)  Acc@1: 93.7500 (89.7059)  Acc@5: 100.0000 (98.6520)  time: 0.4364  data: 0.0013  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5373 (0.5309)  Acc@1: 87.5000 (89.3443)  Acc@5: 100.0000 (98.7705)  time: 0.4355  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5929 (0.5299)  Acc@1: 87.5000 (89.2000)  Acc@5: 100.0000 (98.7000)  time: 0.4246  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4357 s / it)\n",
            "* Acc@1 89.200 Acc@5 98.700 loss 0.530\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:34  Loss: 0.7318 (0.7318)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5507  data: 0.1541  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5270 (0.5498)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4455  data: 0.0157  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.5078 (0.5582)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.2143)  time: 0.4342  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4446 (0.5321)  Acc@1: 87.5000 (88.7097)  Acc@5: 100.0000 (98.3871)  time: 0.4334  data: 0.0039  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3201 (0.4888)  Acc@1: 93.7500 (89.7866)  Acc@5: 100.0000 (98.6280)  time: 0.4336  data: 0.0030  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.3877 (0.5000)  Acc@1: 93.7500 (89.9510)  Acc@5: 100.0000 (98.6520)  time: 0.4337  data: 0.0014  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.4762 (0.5150)  Acc@1: 87.5000 (89.2418)  Acc@5: 100.0000 (98.4631)  time: 0.4336  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.4607 (0.5098)  Acc@1: 87.5000 (89.4000)  Acc@5: 100.0000 (98.5000)  time: 0.4230  data: 0.0020  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4335 s / it)\n",
            "* Acc@1 89.400 Acc@5 98.500 loss 0.510\n",
            "[Average accuracy till task4]\tAcc@1: 89.4750\tAcc@5: 98.8000\tLoss: 0.5530\tForgetting: 4.8667\tBackward: -4.8667\n",
            "Loading checkpoint from: ./output/checkpoint/task5_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:35  Loss: 0.6559 (0.6559)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5563  data: 0.1535  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5713 (0.5606)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.4318)  time: 0.4428  data: 0.0144  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.5713 (0.6167)  Acc@1: 87.5000 (85.7143)  Acc@5: 100.0000 (99.1071)  time: 0.4324  data: 0.0014  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5675 (0.5979)  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (99.1935)  time: 0.4327  data: 0.0013  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5467 (0.5848)  Acc@1: 87.5000 (86.4329)  Acc@5: 100.0000 (99.2378)  time: 0.4318  data: 0.0004  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4638 (0.5604)  Acc@1: 87.5000 (87.2549)  Acc@5: 100.0000 (99.1422)  time: 0.4313  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4566 (0.5459)  Acc@1: 87.5000 (87.7049)  Acc@5: 100.0000 (99.0779)  time: 0.4318  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4566 (0.5441)  Acc@1: 93.7500 (87.9000)  Acc@5: 100.0000 (99.1000)  time: 0.4213  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4316 s / it)\n",
            "* Acc@1 87.900 Acc@5 99.100 loss 0.544\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:35  Loss: 0.7879 (0.7879)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5643  data: 0.1623  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.6982 (0.7158)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (97.7273)  time: 0.4427  data: 0.0152  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.7017 (0.7687)  Acc@1: 87.5000 (87.2024)  Acc@5: 100.0000 (97.0238)  time: 0.4314  data: 0.0015  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7485 (0.7506)  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (96.5726)  time: 0.4327  data: 0.0015  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6587 (0.7283)  Acc@1: 87.5000 (88.2622)  Acc@5: 100.0000 (96.9512)  time: 0.4327  data: 0.0022  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6215 (0.7243)  Acc@1: 87.5000 (87.6225)  Acc@5: 100.0000 (97.1814)  time: 0.4319  data: 0.0048  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5902 (0.6996)  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (97.4385)  time: 0.4325  data: 0.0029  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5863 (0.6921)  Acc@1: 87.5000 (87.9000)  Acc@5: 100.0000 (97.5000)  time: 0.4216  data: 0.0028  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4319 s / it)\n",
            "* Acc@1 87.900 Acc@5 97.500 loss 0.692\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:40  Loss: 0.4664 (0.4664)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6422  data: 0.2450  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5764 (0.5878)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (98.8636)  time: 0.4514  data: 0.0226  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.5947 (0.5967)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (97.9167)  time: 0.4327  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.5763 (0.5853)  Acc@1: 87.5000 (84.6774)  Acc@5: 100.0000 (98.3871)  time: 0.4330  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4424 (0.5776)  Acc@1: 87.5000 (85.2134)  Acc@5: 100.0000 (98.6280)  time: 0.4334  data: 0.0009  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.5344 (0.5813)  Acc@1: 87.5000 (85.6618)  Acc@5: 100.0000 (98.4069)  time: 0.4340  data: 0.0030  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5605 (0.5866)  Acc@1: 87.5000 (85.5533)  Acc@5: 100.0000 (98.3607)  time: 0.4337  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5692 (0.5848)  Acc@1: 81.2500 (85.5000)  Acc@5: 100.0000 (98.4000)  time: 0.4229  data: 0.0017  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4342 s / it)\n",
            "* Acc@1 85.500 Acc@5 98.400 loss 0.585\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:34  Loss: 0.7242 (0.7242)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5513  data: 0.1487  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5598 (0.5694)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4444  data: 0.0141  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.5505 (0.5830)  Acc@1: 87.5000 (84.8214)  Acc@5: 100.0000 (97.9167)  time: 0.4345  data: 0.0012  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4813 (0.5666)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (97.5806)  time: 0.4350  data: 0.0012  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3869 (0.5228)  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (98.0183)  time: 0.4346  data: 0.0013  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4259 (0.5375)  Acc@1: 87.5000 (87.7451)  Acc@5: 100.0000 (98.0392)  time: 0.4345  data: 0.0045  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5104 (0.5581)  Acc@1: 87.5000 (87.3975)  Acc@5: 100.0000 (97.7459)  time: 0.4345  data: 0.0042  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5064 (0.5543)  Acc@1: 87.5000 (87.6000)  Acc@5: 100.0000 (97.8000)  time: 0.4238  data: 0.0033  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4339 s / it)\n",
            "* Acc@1 87.600 Acc@5 97.800 loss 0.554\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:35  Loss: 0.2359 (0.2359)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5569  data: 0.1499  max mem: 1323\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:23  Loss: 0.4583 (0.5871)  Acc@1: 93.7500 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4460  data: 0.0167  max mem: 1323\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:18  Loss: 0.4583 (0.5282)  Acc@1: 93.7500 (90.7738)  Acc@5: 100.0000 (98.5119)  time: 0.4354  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.3980 (0.5103)  Acc@1: 93.7500 (90.1210)  Acc@5: 100.0000 (98.7903)  time: 0.4353  data: 0.0013  max mem: 1323\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.3946 (0.4858)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (98.4756)  time: 0.4346  data: 0.0018  max mem: 1323\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4199 (0.4817)  Acc@1: 93.7500 (91.5441)  Acc@5: 100.0000 (98.6520)  time: 0.4351  data: 0.0028  max mem: 1323\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4222 (0.4839)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (98.6680)  time: 0.4359  data: 0.0014  max mem: 1323\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4345 (0.5008)  Acc@1: 93.7500 (90.8000)  Acc@5: 100.0000 (98.6000)  time: 0.4249  data: 0.0010  max mem: 1323\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4348 s / it)\n",
            "* Acc@1 90.800 Acc@5 98.600 loss 0.501\n",
            "[Average accuracy till task5]\tAcc@1: 87.9400\tAcc@5: 98.2800\tLoss: 0.5752\tForgetting: 5.9000\tBackward: -5.9000\n",
            "Loading checkpoint from: ./output/checkpoint/task6_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:34  Loss: 0.8557 (0.8557)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5509  data: 0.1553  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.6478 (0.5963)  Acc@1: 81.2500 (85.7955)  Acc@5: 100.0000 (99.4318)  time: 0.4440  data: 0.0196  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.6478 (0.6494)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (98.8095)  time: 0.4344  data: 0.0032  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5810 (0.6218)  Acc@1: 81.2500 (84.2742)  Acc@5: 100.0000 (99.1935)  time: 0.4350  data: 0.0005  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4946 (0.5961)  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (99.2378)  time: 0.4351  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4587 (0.5723)  Acc@1: 87.5000 (86.5196)  Acc@5: 100.0000 (99.2647)  time: 0.4358  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4569 (0.5571)  Acc@1: 93.7500 (87.2951)  Acc@5: 100.0000 (99.1803)  time: 0.4363  data: 0.0009  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4524 (0.5526)  Acc@1: 93.7500 (87.6000)  Acc@5: 100.0000 (99.2000)  time: 0.4253  data: 0.0009  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4351 s / it)\n",
            "* Acc@1 87.600 Acc@5 99.200 loss 0.553\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:52  Loss: 0.8404 (0.8404)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.8274  data: 0.4218  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:24  Loss: 0.7207 (0.7189)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (97.1591)  time: 0.4694  data: 0.0415  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7110 (0.7700)  Acc@1: 87.5000 (84.2262)  Acc@5: 100.0000 (97.3214)  time: 0.4348  data: 0.0020  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7110 (0.7498)  Acc@1: 81.2500 (85.0806)  Acc@5: 100.0000 (96.7742)  time: 0.4358  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6228 (0.7282)  Acc@1: 87.5000 (85.3659)  Acc@5: 100.0000 (96.9512)  time: 0.4353  data: 0.0026  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6228 (0.7280)  Acc@1: 81.2500 (84.1912)  Acc@5: 100.0000 (97.0588)  time: 0.4358  data: 0.0026  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.6032 (0.7084)  Acc@1: 81.2500 (85.1434)  Acc@5: 100.0000 (97.3361)  time: 0.4365  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5957 (0.7002)  Acc@1: 87.5000 (85.2000)  Acc@5: 100.0000 (97.4000)  time: 0.4255  data: 0.0003  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4402 s / it)\n",
            "* Acc@1 85.200 Acc@5 97.400 loss 0.700\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:46  Loss: 0.3739 (0.3739)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7366  data: 0.3371  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:24  Loss: 0.5623 (0.6029)  Acc@1: 87.5000 (83.5227)  Acc@5: 100.0000 (97.1591)  time: 0.4626  data: 0.0322  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.6745 (0.6095)  Acc@1: 87.5000 (84.2262)  Acc@5: 93.7500 (97.0238)  time: 0.4360  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.6438 (0.6101)  Acc@1: 81.2500 (83.8710)  Acc@5: 100.0000 (97.7823)  time: 0.4358  data: 0.0013  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.5547 (0.6059)  Acc@1: 81.2500 (84.2988)  Acc@5: 100.0000 (98.1707)  time: 0.4350  data: 0.0018  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.6387 (0.6123)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4353  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.6387 (0.6199)  Acc@1: 87.5000 (85.1434)  Acc@5: 100.0000 (97.9508)  time: 0.4355  data: 0.0006  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.6869 (0.6203)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.9000)  time: 0.4248  data: 0.0005  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4386 s / it)\n",
            "* Acc@1 84.900 Acc@5 97.900 loss 0.620\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:45  Loss: 0.7500 (0.7500)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7204  data: 0.3159  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:24  Loss: 0.6926 (0.6195)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (97.7273)  time: 0.4611  data: 0.0321  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:19  Loss: 0.5853 (0.6083)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (96.7262)  time: 0.4355  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4736 (0.5783)  Acc@1: 87.5000 (86.8952)  Acc@5: 100.0000 (97.1774)  time: 0.4365  data: 0.0022  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3626 (0.5328)  Acc@1: 93.7500 (88.1098)  Acc@5: 100.0000 (97.7134)  time: 0.4364  data: 0.0028  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4589 (0.5475)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (97.7941)  time: 0.4360  data: 0.0013  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5247 (0.5634)  Acc@1: 87.5000 (87.6025)  Acc@5: 100.0000 (97.3361)  time: 0.4363  data: 0.0007  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5247 (0.5618)  Acc@1: 87.5000 (87.7000)  Acc@5: 100.0000 (97.4000)  time: 0.4255  data: 0.0003  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4385 s / it)\n",
            "* Acc@1 87.700 Acc@5 97.400 loss 0.562\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:44  Loss: 0.2667 (0.2667)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6985  data: 0.2977  max mem: 1323\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:24  Loss: 0.4531 (0.5351)  Acc@1: 93.7500 (91.4773)  Acc@5: 100.0000 (98.2955)  time: 0.4596  data: 0.0276  max mem: 1323\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:19  Loss: 0.4531 (0.4820)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4358  data: 0.0005  max mem: 1323\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.4307 (0.4756)  Acc@1: 93.7500 (92.1371)  Acc@5: 100.0000 (98.5887)  time: 0.4355  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.4049 (0.4612)  Acc@1: 93.7500 (92.6829)  Acc@5: 100.0000 (98.4756)  time: 0.4354  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4049 (0.4702)  Acc@1: 93.7500 (93.0147)  Acc@5: 100.0000 (98.4069)  time: 0.4353  data: 0.0005  max mem: 1323\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4179 (0.4735)  Acc@1: 93.7500 (92.7254)  Acc@5: 100.0000 (98.3607)  time: 0.4353  data: 0.0008  max mem: 1323\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4667 (0.4906)  Acc@1: 93.7500 (92.3000)  Acc@5: 100.0000 (98.3000)  time: 0.4242  data: 0.0008  max mem: 1323\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4371 s / it)\n",
            "* Acc@1 92.300 Acc@5 98.300 loss 0.491\n",
            "Test: [Task 6]  [ 0/63]  eta: 0:00:34  Loss: 0.4410 (0.4410)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5481  data: 0.1478  max mem: 1323\n",
            "Test: [Task 6]  [10/63]  eta: 0:00:23  Loss: 0.6429 (0.5867)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (99.4318)  time: 0.4451  data: 0.0139  max mem: 1323\n",
            "Test: [Task 6]  [20/63]  eta: 0:00:18  Loss: 0.6429 (0.6360)  Acc@1: 81.2500 (83.0357)  Acc@5: 100.0000 (99.1071)  time: 0.4352  data: 0.0019  max mem: 1323\n",
            "Test: [Task 6]  [30/63]  eta: 0:00:14  Loss: 0.5972 (0.6252)  Acc@1: 81.2500 (82.2581)  Acc@5: 100.0000 (99.3952)  time: 0.4348  data: 0.0048  max mem: 1323\n",
            "Test: [Task 6]  [40/63]  eta: 0:00:10  Loss: 0.6066 (0.6568)  Acc@1: 75.0000 (81.0976)  Acc@5: 100.0000 (98.9329)  time: 0.4342  data: 0.0034  max mem: 1323\n",
            "Test: [Task 6]  [50/63]  eta: 0:00:05  Loss: 0.6548 (0.6397)  Acc@1: 81.2500 (81.6176)  Acc@5: 100.0000 (99.1422)  time: 0.4345  data: 0.0006  max mem: 1323\n",
            "Test: [Task 6]  [60/63]  eta: 0:00:01  Loss: 0.5738 (0.6575)  Acc@1: 81.2500 (81.7623)  Acc@5: 100.0000 (98.9754)  time: 0.4340  data: 0.0009  max mem: 1323\n",
            "Test: [Task 6]  [62/63]  eta: 0:00:00  Loss: 0.5738 (0.6536)  Acc@1: 81.2500 (82.1000)  Acc@5: 100.0000 (99.0000)  time: 0.4232  data: 0.0009  max mem: 1323\n",
            "Test: [Task 6] Total time: 0:00:27 (0.4339 s / it)\n",
            "* Acc@1 82.100 Acc@5 99.000 loss 0.654\n",
            "[Average accuracy till task6]\tAcc@1: 86.6333\tAcc@5: 98.2000\tLoss: 0.5965\tForgetting: 5.4200\tBackward: -5.1200\n",
            "Loading checkpoint from: ./output/checkpoint/task7_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:37  Loss: 0.8063 (0.8063)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.6021  data: 0.2076  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.6840 (0.6192)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4493  data: 0.0193  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.6964 (0.6710)  Acc@1: 81.2500 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4337  data: 0.0007  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5941 (0.6455)  Acc@1: 81.2500 (85.6855)  Acc@5: 100.0000 (98.1855)  time: 0.4329  data: 0.0027  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5753 (0.6239)  Acc@1: 87.5000 (85.9756)  Acc@5: 100.0000 (98.4756)  time: 0.4327  data: 0.0025  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4881 (0.6033)  Acc@1: 87.5000 (86.2745)  Acc@5: 100.0000 (98.4069)  time: 0.4333  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4766 (0.5865)  Acc@1: 87.5000 (87.0902)  Acc@5: 100.0000 (98.3607)  time: 0.4331  data: 0.0016  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4712 (0.5856)  Acc@1: 93.7500 (87.3000)  Acc@5: 100.0000 (98.4000)  time: 0.4223  data: 0.0016  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4334 s / it)\n",
            "* Acc@1 87.300 Acc@5 98.400 loss 0.586\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:38  Loss: 0.8421 (0.8421)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6113  data: 0.2143  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.7135 (0.7431)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.1591)  time: 0.4499  data: 0.0200  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7636 (0.8059)  Acc@1: 87.5000 (83.0357)  Acc@5: 100.0000 (96.4286)  time: 0.4336  data: 0.0008  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.8425 (0.7955)  Acc@1: 81.2500 (83.6694)  Acc@5: 93.7500 (95.9677)  time: 0.4327  data: 0.0031  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.7209 (0.7759)  Acc@1: 81.2500 (83.6890)  Acc@5: 100.0000 (96.3415)  time: 0.4319  data: 0.0039  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6508 (0.7737)  Acc@1: 81.2500 (83.4559)  Acc@5: 100.0000 (96.4461)  time: 0.4321  data: 0.0023  max mem: 1323\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1086, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    log.warning(f\"Received {e.sigval} death signal, shutting down workers\")\n",
            "Message: 'Received 2 death signal, shutting down workers'\n",
            "Arguments: ()\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 40111 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/l2p-pytorch/main.py\", line 165, in <module>\n",
            "    main(args)\n",
            "  File \"/content/l2p-pytorch/main.py\", line 104, in main\n",
            "    _ = evaluate_till_now(model, original_model, data_loader, device, \n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 151, in evaluate_till_now\n",
            "    test_stats = evaluate(model=model, original_model=original_model, data_loader=data_loader[i]['val'], \n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 118, in evaluate\n",
            "    output = model(input, task_id=task_id, cls_features=cls_features)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 514, in forward\n",
            "    res = self.forward_features(x, task_id=task_id, cls_features=cls_features, train=train)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 470, in forward_features\n",
            "    res = self.prompt(x, prompt_mask=prompt_mask, cls_features=cls_features)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 64, in forward\n",
            "    prompt_norm = self.l2_normalize(self.prompt_key, dim=1) # Pool_size, C\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 44, in l2_normalize\n",
            "    x_inv_norm = torch.rsqrt(torch.maximum(square_sum, torch.tensor(epsilon, device=x.device)))\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py TinyImagenet --eval"
      ],
      "metadata": {
        "id": "O6gAp7JXsW7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py Imagenet-R --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_s5jV_xsGF7",
        "outputId": "e949fbce-b6ce-4b2d-c39f-3ed0ea20f00b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='Imagenet-R', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='Imagenet-R', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=True, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=200)\n",
            "No checkpoint found at: ./output/checkpoint/task1_checkpoint.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ilXx4rassMTb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}