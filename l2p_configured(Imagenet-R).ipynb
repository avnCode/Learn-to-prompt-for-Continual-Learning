{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Imagenet-R"
      ],
      "metadata": {
        "id": "pWj1W0FjuwOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90TRrimFstfu",
        "outputId": "7c2f63a0-668e-4141-baa0-62982dcf96db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'l2p-pytorch'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 172 (delta 91), reused 105 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (172/172), 67.80 KiB | 789.00 KiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JH-LEE-KR/l2p-pytorch\n",
        "!cd l2p-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/l2p-pytorch/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "vfW9EfPptjAf",
        "outputId": "b66c5f3d-8952-4697-a22e-3cccc39085d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.3\n",
            "  Downloading matplotlib-3.5.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchprofile==0.0.4\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (0.15.1+cu118)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.12.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: pillow, matplotlib, torchprofile, timm\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed matplotlib-3.5.3 pillow-9.2.0 timm-0.6.7 torchprofile-0.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xv1qg6OpQvM",
        "outputId": "b719debd-0145-48d9-c12a-03754b586334"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/Shareddrives/Colab/AIP_PROJECT/l2p_configs_files.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vm3QGoxWpc_H",
        "outputId": "4fa0723c-7f8c-4e3f-87f4-43fbb5c72b4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/Shareddrives/Colab/AIP_PROJECT/l2p_configs_files.zip\n",
            "  inflating: cifar100_l2p.py         \n",
            "  inflating: datasets.py             \n",
            "  inflating: Imagenet_R.py           \n",
            "  inflating: main.py                 \n",
            "  inflating: TinyImagenet.py         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/l2p-pytorch/main.py\n",
        "!rm /content/l2p-pytorch/datasets.py\n",
        "!rm /content/l2p-pytorch/configs/cifar100_l2p.py\n",
        "!cp /content/cifar100_l2p.py /content/l2p-pytorch/configs/cifar100_l2p.py\n",
        "!cp /content/TinyImagenet.py /content/l2p-pytorch/configs/TinyImagenet.py\n",
        "!cp /content/Imagenet_R.py /content/l2p-pytorch/configs/Imagenet_R.py\n",
        "!cp /content/datasets.py /content/l2p-pytorch/datasets.py\n",
        "!cp /content/main.py /content/l2p-pytorch/main.py"
      ],
      "metadata": {
        "id": "S8X6GXjIovvt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        cifar100_l2p \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "id": "YB7KPZw1tpvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        TinyImagenet \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "id": "45kqXo8eqRXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        Imagenet-R \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5aBGWfUqUZI",
        "outputId": "36e5918f-a067-4eba-e773-f860f4641f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Downloading https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar to /local_datasets/imagenet-r.tar\n",
            "100% 2191079936/2191079936 [01:49<00:00, 19938848.16it/s]\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='Imagenet-R', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='Imagenet-R', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=200)\n",
            "number of params: 199880\n",
            "Start training for 5 epochs\n",
            "Train: Epoch[1/5]  [  0/162]  eta: 0:09:42  Lr: 0.001875  Loss: 3.0009  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  time: 3.5943  data: 0.6442  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 10/162]  eta: 0:02:14  Lr: 0.001875  Loss: 2.8325  Acc@1: 18.7500 (20.4545)  Acc@5: 43.7500 (42.6136)  time: 0.8837  data: 0.0597  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 20/162]  eta: 0:01:48  Lr: 0.001875  Loss: 2.7956  Acc@1: 18.7500 (17.8571)  Acc@5: 43.7500 (41.0714)  time: 0.6219  data: 0.0009  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 30/162]  eta: 0:01:35  Lr: 0.001875  Loss: 2.5015  Acc@1: 18.7500 (24.5968)  Acc@5: 50.0000 (48.5887)  time: 0.6387  data: 0.0007  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 40/162]  eta: 0:01:26  Lr: 0.001875  Loss: 2.3596  Acc@1: 43.7500 (29.8780)  Acc@5: 68.7500 (53.6585)  time: 0.6492  data: 0.0006  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 50/162]  eta: 0:01:18  Lr: 0.001875  Loss: 2.5327  Acc@1: 43.7500 (31.9853)  Acc@5: 68.7500 (57.3529)  time: 0.6602  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 60/162]  eta: 0:01:11  Lr: 0.001875  Loss: 2.4281  Acc@1: 43.7500 (33.5041)  Acc@5: 68.7500 (59.3238)  time: 0.6773  data: 0.0023  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 70/162]  eta: 0:01:04  Lr: 0.001875  Loss: 2.6011  Acc@1: 43.7500 (35.0352)  Acc@5: 68.7500 (61.0035)  time: 0.6952  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 80/162]  eta: 0:00:57  Lr: 0.001875  Loss: 2.1350  Acc@1: 37.5000 (35.9568)  Acc@5: 68.7500 (62.5772)  time: 0.7108  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 90/162]  eta: 0:00:50  Lr: 0.001875  Loss: 2.1097  Acc@1: 43.7500 (37.6374)  Acc@5: 75.0000 (64.2170)  time: 0.7125  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[1/5]  [100/162]  eta: 0:00:43  Lr: 0.001875  Loss: 2.0415  Acc@1: 50.0000 (38.7995)  Acc@5: 81.2500 (65.7797)  time: 0.6991  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[1/5]  [110/162]  eta: 0:00:36  Lr: 0.001875  Loss: 1.9966  Acc@1: 50.0000 (39.8086)  Acc@5: 81.2500 (67.5676)  time: 0.6857  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[1/5]  [120/162]  eta: 0:00:29  Lr: 0.001875  Loss: 1.9253  Acc@1: 50.0000 (40.5475)  Acc@5: 87.5000 (68.6983)  time: 0.6765  data: 0.0018  max mem: 2374\n",
            "Train: Epoch[1/5]  [130/162]  eta: 0:00:22  Lr: 0.001875  Loss: 1.6153  Acc@1: 56.2500 (42.4141)  Acc@5: 87.5000 (69.7996)  time: 0.6704  data: 0.0022  max mem: 2374\n",
            "Train: Epoch[1/5]  [140/162]  eta: 0:00:15  Lr: 0.001875  Loss: 1.8425  Acc@1: 56.2500 (42.9965)  Acc@5: 81.2500 (70.6117)  time: 0.6675  data: 0.0025  max mem: 2374\n",
            "Train: Epoch[1/5]  [150/162]  eta: 0:00:08  Lr: 0.001875  Loss: 1.6304  Acc@1: 56.2500 (44.2053)  Acc@5: 81.2500 (71.7301)  time: 0.6677  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[1/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.7043  Acc@1: 50.0000 (44.5264)  Acc@5: 87.5000 (72.3602)  time: 0.6705  data: 0.0020  max mem: 2374\n",
            "Train: Epoch[1/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 2.7187  Acc@1: 50.0000 (44.4789)  Acc@5: 87.5000 (72.2976)  time: 0.6501  data: 0.0020  max mem: 2374\n",
            "Train: Epoch[1/5] Total time: 0:01:51 (0.6875 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 2.7187  Acc@1: 50.0000 (44.4789)  Acc@5: 87.5000 (72.2976)\n",
            "Train: Epoch[2/5]  [  0/162]  eta: 0:03:02  Lr: 0.001875  Loss: 1.5074  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 1.1286  data: 0.5013  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 10/162]  eta: 0:01:49  Lr: 0.001875  Loss: 1.5733  Acc@1: 62.5000 (61.9318)  Acc@5: 93.7500 (90.3409)  time: 0.7185  data: 0.0475  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 20/162]  eta: 0:01:39  Lr: 0.001875  Loss: 1.8015  Acc@1: 62.5000 (58.9286)  Acc@5: 87.5000 (87.7976)  time: 0.6803  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 30/162]  eta: 0:01:31  Lr: 0.001875  Loss: 1.5458  Acc@1: 56.2500 (58.4677)  Acc@5: 87.5000 (87.0968)  time: 0.6851  data: 0.0020  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 40/162]  eta: 0:01:24  Lr: 0.001875  Loss: 1.3075  Acc@1: 56.2500 (59.2988)  Acc@5: 87.5000 (87.0427)  time: 0.6878  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 50/162]  eta: 0:01:17  Lr: 0.001875  Loss: 1.2734  Acc@1: 56.2500 (59.1912)  Acc@5: 87.5000 (86.8873)  time: 0.6886  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 60/162]  eta: 0:01:10  Lr: 0.001875  Loss: 1.8823  Acc@1: 56.2500 (58.9139)  Acc@5: 81.2500 (86.3730)  time: 0.6867  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 70/162]  eta: 0:01:03  Lr: 0.001875  Loss: 1.5845  Acc@1: 56.2500 (59.1549)  Acc@5: 81.2500 (86.1796)  time: 0.6839  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 80/162]  eta: 0:00:56  Lr: 0.001875  Loss: 1.0828  Acc@1: 56.2500 (59.1049)  Acc@5: 87.5000 (86.4198)  time: 0.6817  data: 0.0027  max mem: 2374\n",
            "Train: Epoch[2/5]  [ 90/162]  eta: 0:00:49  Lr: 0.001875  Loss: 1.2943  Acc@1: 62.5000 (59.6841)  Acc@5: 87.5000 (86.3324)  time: 0.6796  data: 0.0022  max mem: 2374\n",
            "Train: Epoch[2/5]  [100/162]  eta: 0:00:42  Lr: 0.001875  Loss: 1.0430  Acc@1: 62.5000 (60.3342)  Acc@5: 87.5000 (86.7574)  time: 0.6779  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[2/5]  [110/162]  eta: 0:00:35  Lr: 0.001875  Loss: 1.0128  Acc@1: 62.5000 (60.2477)  Acc@5: 87.5000 (86.8806)  time: 0.6773  data: 0.0024  max mem: 2374\n",
            "Train: Epoch[2/5]  [120/162]  eta: 0:00:28  Lr: 0.001875  Loss: 1.3258  Acc@1: 62.5000 (60.7438)  Acc@5: 87.5000 (87.1901)  time: 0.6768  data: 0.0020  max mem: 2374\n",
            "Train: Epoch[2/5]  [130/162]  eta: 0:00:21  Lr: 0.001875  Loss: 1.2078  Acc@1: 62.5000 (60.7347)  Acc@5: 87.5000 (86.8321)  time: 0.6772  data: 0.0028  max mem: 2374\n",
            "Train: Epoch[2/5]  [140/162]  eta: 0:00:15  Lr: 0.001875  Loss: 1.4398  Acc@1: 62.5000 (61.1259)  Acc@5: 87.5000 (86.9681)  time: 0.6788  data: 0.0035  max mem: 2374\n",
            "Train: Epoch[2/5]  [150/162]  eta: 0:00:08  Lr: 0.001875  Loss: 1.5331  Acc@1: 68.7500 (61.6722)  Acc@5: 87.5000 (87.1689)  time: 0.6801  data: 0.0026  max mem: 2374\n",
            "Train: Epoch[2/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.1098  Acc@1: 68.7500 (61.9953)  Acc@5: 87.5000 (87.3835)  time: 0.6813  data: 0.0002  max mem: 2374\n",
            "Train: Epoch[2/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 1.9190  Acc@1: 62.5000 (61.9527)  Acc@5: 87.5000 (87.3305)  time: 0.6574  data: 0.0002  max mem: 2374\n",
            "Train: Epoch[2/5] Total time: 0:01:50 (0.6819 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.9190  Acc@1: 62.5000 (61.9527)  Acc@5: 87.5000 (87.3305)\n",
            "Train: Epoch[3/5]  [  0/162]  eta: 0:02:54  Lr: 0.001875  Loss: 1.8111  Acc@1: 37.5000 (37.5000)  Acc@5: 75.0000 (75.0000)  time: 1.0789  data: 0.4402  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 10/162]  eta: 0:01:49  Lr: 0.001875  Loss: 0.9699  Acc@1: 68.7500 (63.6364)  Acc@5: 81.2500 (84.6591)  time: 0.7179  data: 0.0403  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 20/162]  eta: 0:01:39  Lr: 0.001875  Loss: 1.1219  Acc@1: 68.7500 (64.8810)  Acc@5: 87.5000 (86.3095)  time: 0.6821  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 30/162]  eta: 0:01:31  Lr: 0.001875  Loss: 1.3365  Acc@1: 62.5000 (64.9194)  Acc@5: 87.5000 (86.6935)  time: 0.6829  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 40/162]  eta: 0:01:24  Lr: 0.001875  Loss: 1.0953  Acc@1: 68.7500 (65.3963)  Acc@5: 87.5000 (88.4146)  time: 0.6828  data: 0.0016  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 50/162]  eta: 0:01:17  Lr: 0.001875  Loss: 1.2841  Acc@1: 68.7500 (66.2990)  Acc@5: 93.7500 (89.2157)  time: 0.6832  data: 0.0025  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 60/162]  eta: 0:01:10  Lr: 0.001875  Loss: 0.9196  Acc@1: 68.7500 (66.5984)  Acc@5: 93.7500 (89.1393)  time: 0.6838  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 70/162]  eta: 0:01:03  Lr: 0.001875  Loss: 1.2551  Acc@1: 62.5000 (66.1972)  Acc@5: 87.5000 (88.7324)  time: 0.6840  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 80/162]  eta: 0:00:56  Lr: 0.001875  Loss: 1.0674  Acc@1: 68.7500 (66.5123)  Acc@5: 93.7500 (89.3519)  time: 0.6852  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[3/5]  [ 90/162]  eta: 0:00:49  Lr: 0.001875  Loss: 1.0286  Acc@1: 68.7500 (66.2088)  Acc@5: 93.7500 (89.3544)  time: 0.6852  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[3/5]  [100/162]  eta: 0:00:42  Lr: 0.001875  Loss: 1.2144  Acc@1: 68.7500 (66.8317)  Acc@5: 93.7500 (89.7277)  time: 0.6845  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[3/5]  [110/162]  eta: 0:00:35  Lr: 0.001875  Loss: 1.0104  Acc@1: 68.7500 (66.5541)  Acc@5: 93.7500 (89.7523)  time: 0.6842  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[3/5]  [120/162]  eta: 0:00:28  Lr: 0.001875  Loss: 0.8018  Acc@1: 62.5000 (66.5289)  Acc@5: 93.7500 (89.7211)  time: 0.6849  data: 0.0008  max mem: 2374\n",
            "Train: Epoch[3/5]  [130/162]  eta: 0:00:21  Lr: 0.001875  Loss: 1.2833  Acc@1: 68.7500 (66.3645)  Acc@5: 93.7500 (89.6469)  time: 0.6850  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[3/5]  [140/162]  eta: 0:00:15  Lr: 0.001875  Loss: 1.1950  Acc@1: 68.7500 (66.3564)  Acc@5: 87.5000 (89.4947)  time: 0.6851  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[3/5]  [150/162]  eta: 0:00:08  Lr: 0.001875  Loss: 1.2450  Acc@1: 68.7500 (66.3493)  Acc@5: 87.5000 (89.3212)  time: 0.6852  data: 0.0027  max mem: 2374\n",
            "Train: Epoch[3/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 0.8448  Acc@1: 68.7500 (66.3820)  Acc@5: 87.5000 (89.3634)  time: 0.6842  data: 0.0026  max mem: 2374\n",
            "Train: Epoch[3/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3468  Acc@1: 68.7500 (66.3696)  Acc@5: 87.5000 (89.3840)  time: 0.6607  data: 0.0026  max mem: 2374\n",
            "Train: Epoch[3/5] Total time: 0:01:50 (0.6840 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3468  Acc@1: 68.7500 (66.3696)  Acc@5: 87.5000 (89.3840)\n",
            "Train: Epoch[4/5]  [  0/162]  eta: 0:02:30  Lr: 0.001875  Loss: 1.0479  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.9278  data: 0.2811  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 10/162]  eta: 0:01:47  Lr: 0.001875  Loss: 0.6857  Acc@1: 68.7500 (71.0227)  Acc@5: 93.7500 (92.0455)  time: 0.7058  data: 0.0288  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 20/162]  eta: 0:01:38  Lr: 0.001875  Loss: 1.3173  Acc@1: 68.7500 (69.9405)  Acc@5: 93.7500 (91.6667)  time: 0.6835  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 30/162]  eta: 0:01:31  Lr: 0.001875  Loss: 1.0441  Acc@1: 68.7500 (71.1694)  Acc@5: 93.7500 (92.3387)  time: 0.6833  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 40/162]  eta: 0:01:24  Lr: 0.001875  Loss: 0.9811  Acc@1: 68.7500 (69.8171)  Acc@5: 93.7500 (91.9207)  time: 0.6839  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 50/162]  eta: 0:01:17  Lr: 0.001875  Loss: 1.3946  Acc@1: 68.7500 (69.1176)  Acc@5: 87.5000 (91.6667)  time: 0.6844  data: 0.0011  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 60/162]  eta: 0:01:10  Lr: 0.001875  Loss: 1.5027  Acc@1: 68.7500 (69.6721)  Acc@5: 93.7500 (91.9057)  time: 0.6832  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 70/162]  eta: 0:01:03  Lr: 0.001875  Loss: 0.8054  Acc@1: 68.7500 (69.6303)  Acc@5: 93.7500 (91.8134)  time: 0.6837  data: 0.0022  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 80/162]  eta: 0:00:56  Lr: 0.001875  Loss: 0.9556  Acc@1: 68.7500 (69.9846)  Acc@5: 93.7500 (91.8981)  time: 0.6852  data: 0.0021  max mem: 2374\n",
            "Train: Epoch[4/5]  [ 90/162]  eta: 0:00:49  Lr: 0.001875  Loss: 0.7985  Acc@1: 68.7500 (69.9176)  Acc@5: 93.7500 (91.6896)  time: 0.6846  data: 0.0015  max mem: 2374\n",
            "Train: Epoch[4/5]  [100/162]  eta: 0:00:42  Lr: 0.001875  Loss: 1.2298  Acc@1: 68.7500 (69.6782)  Acc@5: 93.7500 (91.6460)  time: 0.6842  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[4/5]  [110/162]  eta: 0:00:35  Lr: 0.001875  Loss: 1.2029  Acc@1: 62.5000 (69.5383)  Acc@5: 93.7500 (91.4977)  time: 0.6850  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[4/5]  [120/162]  eta: 0:00:28  Lr: 0.001875  Loss: 0.5905  Acc@1: 75.0000 (69.8864)  Acc@5: 93.7500 (91.6839)  time: 0.6849  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[4/5]  [130/162]  eta: 0:00:21  Lr: 0.001875  Loss: 1.3132  Acc@1: 68.7500 (69.5134)  Acc@5: 93.7500 (91.9370)  time: 0.6844  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[4/5]  [140/162]  eta: 0:00:15  Lr: 0.001875  Loss: 0.8042  Acc@1: 62.5000 (69.1046)  Acc@5: 93.7500 (91.7553)  time: 0.6832  data: 0.0023  max mem: 2374\n",
            "Train: Epoch[4/5]  [150/162]  eta: 0:00:08  Lr: 0.001875  Loss: 0.8678  Acc@1: 62.5000 (69.0397)  Acc@5: 93.7500 (91.8046)  time: 0.6821  data: 0.0021  max mem: 2374\n",
            "Train: Epoch[4/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.4786  Acc@1: 68.7500 (69.2547)  Acc@5: 93.7500 (91.8478)  time: 0.6827  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[4/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 0.2314  Acc@1: 68.7500 (69.2755)  Acc@5: 93.7500 (91.8636)  time: 0.6596  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[4/5] Total time: 0:01:50 (0.6829 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.2314  Acc@1: 68.7500 (69.2755)  Acc@5: 93.7500 (91.8636)\n",
            "Train: Epoch[5/5]  [  0/162]  eta: 0:02:29  Lr: 0.001875  Loss: 0.8040  Acc@1: 75.0000 (75.0000)  Acc@5: 81.2500 (81.2500)  time: 0.9253  data: 0.2867  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 10/162]  eta: 0:01:46  Lr: 0.001875  Loss: 1.1479  Acc@1: 81.2500 (78.4091)  Acc@5: 93.7500 (92.6136)  time: 0.7034  data: 0.0295  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 20/162]  eta: 0:01:38  Lr: 0.001875  Loss: 0.4409  Acc@1: 75.0000 (75.5952)  Acc@5: 93.7500 (93.7500)  time: 0.6811  data: 0.0034  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 30/162]  eta: 0:01:30  Lr: 0.001875  Loss: 1.4849  Acc@1: 68.7500 (74.1935)  Acc@5: 93.7500 (93.1452)  time: 0.6806  data: 0.0017  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 40/162]  eta: 0:01:23  Lr: 0.001875  Loss: 0.6670  Acc@1: 68.7500 (73.9329)  Acc@5: 93.7500 (92.6829)  time: 0.6804  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 50/162]  eta: 0:01:16  Lr: 0.001875  Loss: 1.4113  Acc@1: 68.7500 (72.6716)  Acc@5: 87.5000 (91.7892)  time: 0.6802  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 60/162]  eta: 0:01:09  Lr: 0.001875  Loss: 0.8401  Acc@1: 68.7500 (71.4139)  Acc@5: 87.5000 (90.9836)  time: 0.6794  data: 0.0008  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 70/162]  eta: 0:01:02  Lr: 0.001875  Loss: 0.8481  Acc@1: 68.7500 (71.3908)  Acc@5: 87.5000 (90.9331)  time: 0.6791  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 80/162]  eta: 0:00:56  Lr: 0.001875  Loss: 0.5450  Acc@1: 75.0000 (71.4506)  Acc@5: 93.7500 (90.8179)  time: 0.6798  data: 0.0062  max mem: 2374\n",
            "Train: Epoch[5/5]  [ 90/162]  eta: 0:00:49  Lr: 0.001875  Loss: 1.1700  Acc@1: 68.7500 (71.5659)  Acc@5: 93.7500 (91.2088)  time: 0.6797  data: 0.0061  max mem: 2374\n",
            "Train: Epoch[5/5]  [100/162]  eta: 0:00:42  Lr: 0.001875  Loss: 0.9434  Acc@1: 75.0000 (71.5347)  Acc@5: 93.7500 (91.3985)  time: 0.6797  data: 0.0018  max mem: 2374\n",
            "Train: Epoch[5/5]  [110/162]  eta: 0:00:35  Lr: 0.001875  Loss: 0.7904  Acc@1: 68.7500 (71.3401)  Acc@5: 93.7500 (91.5541)  time: 0.6787  data: 0.0018  max mem: 2374\n",
            "Train: Epoch[5/5]  [120/162]  eta: 0:00:28  Lr: 0.001875  Loss: 0.8044  Acc@1: 68.7500 (71.2293)  Acc@5: 93.7500 (91.5289)  time: 0.6786  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[5/5]  [130/162]  eta: 0:00:21  Lr: 0.001875  Loss: 0.8937  Acc@1: 68.7500 (70.6584)  Acc@5: 93.7500 (91.6508)  time: 0.6789  data: 0.0019  max mem: 2374\n",
            "Train: Epoch[5/5]  [140/162]  eta: 0:00:14  Lr: 0.001875  Loss: 1.1430  Acc@1: 68.7500 (70.4787)  Acc@5: 93.7500 (91.6223)  time: 0.6785  data: 0.0013  max mem: 2374\n",
            "Train: Epoch[5/5]  [150/162]  eta: 0:00:08  Lr: 0.001875  Loss: 0.6586  Acc@1: 75.0000 (70.6540)  Acc@5: 93.7500 (91.8874)  time: 0.6794  data: 0.0014  max mem: 2374\n",
            "Train: Epoch[5/5]  [160/162]  eta: 0:00:01  Lr: 0.001875  Loss: 1.0746  Acc@1: 75.0000 (70.8463)  Acc@5: 93.7500 (91.8478)  time: 0.6801  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[5/5]  [161/162]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5336  Acc@1: 75.0000 (70.9028)  Acc@5: 93.7500 (91.8636)  time: 0.6571  data: 0.0012  max mem: 2374\n",
            "Train: Epoch[5/5] Total time: 0:01:50 (0.6791 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5336  Acc@1: 75.0000 (70.9028)  Acc@5: 93.7500 (91.8636)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:47  Loss: 2.8129 (2.8129)  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 1.1630  data: 0.7517  max mem: 2374\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.5032 (2.4082)  Acc@1: 62.5000 (68.1818)  Acc@5: 93.7500 (92.6136)  time: 0.5041  data: 0.0717  max mem: 2374\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.8888 (2.0743)  Acc@1: 75.0000 (72.9167)  Acc@5: 93.7500 (94.0476)  time: 0.4399  data: 0.0020  max mem: 2374\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 2.0549 (2.1880)  Acc@1: 68.7500 (70.5645)  Acc@5: 93.7500 (92.9435)  time: 0.4416  data: 0.0009  max mem: 2374\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2250 (2.1905)  Acc@1: 68.7500 (71.8462)  Acc@5: 93.7500 (92.3077)  time: 0.4365  data: 0.0021  max mem: 2374\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4577 s / it)\n",
            "* Acc@1 71.846 Acc@5 92.308 loss 2.190\n",
            "[Average accuracy till task1]\tAcc@1: 71.8462\tAcc@5: 92.3077\tLoss: 2.1905\n",
            "Train: Epoch[1/5]  [  0/179]  eta: 0:03:31  Lr: 0.001875  Loss: 2.8438  Acc@1: 12.5000 (12.5000)  Acc@5: 31.2500 (31.2500)  time: 1.1818  data: 0.5511  max mem: 2374\n",
            "Train: Epoch[1/5]  [ 10/179]  eta: 0:02:02  Lr: 0.001875  Loss: 2.7514  Acc@1: 18.7500 (18.7500)  Acc@5: 43.7500 (42.6136)  time: 0.7250  data: 0.0503  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 20/179]  eta: 0:01:51  Lr: 0.001875  Loss: 2.5527  Acc@1: 25.0000 (25.2976)  Acc@5: 56.2500 (53.8690)  time: 0.6792  data: 0.0024  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 30/179]  eta: 0:01:43  Lr: 0.001875  Loss: 2.2441  Acc@1: 43.7500 (31.0484)  Acc@5: 68.7500 (60.0806)  time: 0.6796  data: 0.0024  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 40/179]  eta: 0:01:36  Lr: 0.001875  Loss: 2.1623  Acc@1: 43.7500 (34.6037)  Acc@5: 68.7500 (62.0427)  time: 0.6808  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 50/179]  eta: 0:01:29  Lr: 0.001875  Loss: 2.0786  Acc@1: 43.7500 (36.7647)  Acc@5: 68.7500 (64.5833)  time: 0.6820  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 60/179]  eta: 0:01:22  Lr: 0.001875  Loss: 2.2019  Acc@1: 43.7500 (38.6270)  Acc@5: 75.0000 (65.3689)  time: 0.6834  data: 0.0025  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 70/179]  eta: 0:01:15  Lr: 0.001875  Loss: 1.8301  Acc@1: 50.0000 (40.4930)  Acc@5: 75.0000 (66.9014)  time: 0.6847  data: 0.0025  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 80/179]  eta: 0:01:08  Lr: 0.001875  Loss: 1.7597  Acc@1: 56.2500 (41.0494)  Acc@5: 75.0000 (67.9012)  time: 0.6851  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[1/5]  [ 90/179]  eta: 0:01:01  Lr: 0.001875  Loss: 1.7601  Acc@1: 50.0000 (42.1016)  Acc@5: 75.0000 (68.8874)  time: 0.6842  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[1/5]  [100/179]  eta: 0:00:54  Lr: 0.001875  Loss: 1.2699  Acc@1: 50.0000 (43.4406)  Acc@5: 81.2500 (69.9257)  time: 0.6837  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[1/5]  [110/179]  eta: 0:00:47  Lr: 0.001875  Loss: 1.5499  Acc@1: 56.2500 (44.7072)  Acc@5: 81.2500 (70.7770)  time: 0.6834  data: 0.0017  max mem: 2375\n",
            "Train: Epoch[1/5]  [120/179]  eta: 0:00:40  Lr: 0.001875  Loss: 1.6182  Acc@1: 56.2500 (45.2996)  Acc@5: 81.2500 (71.6426)  time: 0.6822  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[1/5]  [130/179]  eta: 0:00:33  Lr: 0.001875  Loss: 1.3983  Acc@1: 56.2500 (46.8034)  Acc@5: 81.2500 (72.6622)  time: 0.6816  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[1/5]  [140/179]  eta: 0:00:26  Lr: 0.001875  Loss: 1.1932  Acc@1: 62.5000 (47.5177)  Acc@5: 81.2500 (73.4929)  time: 0.6804  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[1/5]  [150/179]  eta: 0:00:19  Lr: 0.001875  Loss: 1.5185  Acc@1: 62.5000 (48.6341)  Acc@5: 87.5000 (74.2136)  time: 0.6793  data: 0.0013  max mem: 2375\n",
            "Train: Epoch[1/5]  [160/179]  eta: 0:00:13  Lr: 0.001875  Loss: 1.3895  Acc@1: 62.5000 (49.1071)  Acc@5: 81.2500 (74.5730)  time: 0.6789  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[1/5]  [170/179]  eta: 0:00:06  Lr: 0.001875  Loss: 1.1554  Acc@1: 62.5000 (50.1827)  Acc@5: 87.5000 (75.6213)  time: 0.6792  data: 0.0021  max mem: 2375\n",
            "Train: Epoch[1/5]  [178/179]  eta: 0:00:00  Lr: 0.001875  Loss: 1.2228  Acc@1: 62.5000 (50.4202)  Acc@5: 87.5000 (75.9104)  time: 0.6648  data: 0.0020  max mem: 2375\n",
            "Train: Epoch[1/5] Total time: 0:02:02 (0.6831 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.2228  Acc@1: 62.5000 (50.4202)  Acc@5: 87.5000 (75.9104)\n",
            "Train: Epoch[2/5]  [  0/179]  eta: 0:02:43  Lr: 0.001875  Loss: 1.2559  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.9161  data: 0.2807  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 10/179]  eta: 0:01:58  Lr: 0.001875  Loss: 1.5633  Acc@1: 62.5000 (64.7727)  Acc@5: 87.5000 (87.5000)  time: 0.6989  data: 0.0299  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 20/179]  eta: 0:01:49  Lr: 0.001875  Loss: 1.2005  Acc@1: 62.5000 (61.6071)  Acc@5: 87.5000 (86.0119)  time: 0.6770  data: 0.0026  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 30/179]  eta: 0:01:42  Lr: 0.001875  Loss: 1.2391  Acc@1: 62.5000 (62.2984)  Acc@5: 81.2500 (85.6855)  time: 0.6774  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 40/179]  eta: 0:01:34  Lr: 0.001875  Loss: 1.3725  Acc@1: 62.5000 (62.0427)  Acc@5: 87.5000 (86.4329)  time: 0.6774  data: 0.0017  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 50/179]  eta: 0:01:27  Lr: 0.001875  Loss: 0.7873  Acc@1: 62.5000 (63.3578)  Acc@5: 87.5000 (87.2549)  time: 0.6770  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 60/179]  eta: 0:01:21  Lr: 0.001875  Loss: 1.0829  Acc@1: 68.7500 (64.2418)  Acc@5: 87.5000 (87.7049)  time: 0.6773  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 70/179]  eta: 0:01:14  Lr: 0.001875  Loss: 0.7426  Acc@1: 68.7500 (64.5246)  Acc@5: 87.5000 (87.9401)  time: 0.6768  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 80/179]  eta: 0:01:07  Lr: 0.001875  Loss: 1.3843  Acc@1: 62.5000 (63.7346)  Acc@5: 87.5000 (88.1944)  time: 0.6769  data: 0.0020  max mem: 2375\n",
            "Train: Epoch[2/5]  [ 90/179]  eta: 0:01:00  Lr: 0.001875  Loss: 1.5152  Acc@1: 56.2500 (63.5302)  Acc@5: 87.5000 (87.8434)  time: 0.6775  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[2/5]  [100/179]  eta: 0:00:53  Lr: 0.001875  Loss: 1.1604  Acc@1: 62.5000 (63.6757)  Acc@5: 87.5000 (88.3663)  time: 0.6772  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[2/5]  [110/179]  eta: 0:00:46  Lr: 0.001875  Loss: 1.2262  Acc@1: 62.5000 (63.7387)  Acc@5: 93.7500 (88.6261)  time: 0.6765  data: 0.0007  max mem: 2375\n",
            "Train: Epoch[2/5]  [120/179]  eta: 0:00:40  Lr: 0.001875  Loss: 1.4524  Acc@1: 56.2500 (63.4814)  Acc@5: 87.5000 (88.1198)  time: 0.6771  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[2/5]  [130/179]  eta: 0:00:33  Lr: 0.001875  Loss: 0.9203  Acc@1: 62.5000 (63.4065)  Acc@5: 87.5000 (88.2156)  time: 0.6780  data: 0.0022  max mem: 2375\n",
            "Train: Epoch[2/5]  [140/179]  eta: 0:00:26  Lr: 0.001875  Loss: 0.5673  Acc@1: 62.5000 (63.8298)  Acc@5: 87.5000 (88.5638)  time: 0.6785  data: 0.0040  max mem: 2375\n",
            "Train: Epoch[2/5]  [150/179]  eta: 0:00:19  Lr: 0.001875  Loss: 1.0430  Acc@1: 62.5000 (64.0315)  Acc@5: 87.5000 (88.6175)  time: 0.6796  data: 0.0036  max mem: 2375\n",
            "Train: Epoch[2/5]  [160/179]  eta: 0:00:12  Lr: 0.001875  Loss: 0.9058  Acc@1: 68.7500 (64.4022)  Acc@5: 87.5000 (88.4705)  time: 0.6801  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[2/5]  [170/179]  eta: 0:00:06  Lr: 0.001875  Loss: 1.1484  Acc@1: 62.5000 (64.1447)  Acc@5: 87.5000 (88.2675)  time: 0.6798  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[2/5]  [178/179]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3042  Acc@1: 56.2500 (64.0756)  Acc@5: 87.5000 (88.2353)  time: 0.6632  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[2/5] Total time: 0:02:01 (0.6780 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3042  Acc@1: 56.2500 (64.0756)  Acc@5: 87.5000 (88.2353)\n",
            "Train: Epoch[3/5]  [  0/179]  eta: 0:03:55  Lr: 0.001875  Loss: 1.3129  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 1.3159  data: 0.6703  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 10/179]  eta: 0:02:04  Lr: 0.001875  Loss: 1.5410  Acc@1: 75.0000 (73.8636)  Acc@5: 93.7500 (89.2045)  time: 0.7351  data: 0.0618  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 20/179]  eta: 0:01:52  Lr: 0.001875  Loss: 1.2092  Acc@1: 62.5000 (68.4524)  Acc@5: 93.7500 (88.9881)  time: 0.6780  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 30/179]  eta: 0:01:44  Lr: 0.001875  Loss: 1.0289  Acc@1: 62.5000 (69.9597)  Acc@5: 93.7500 (90.3226)  time: 0.6797  data: 0.0013  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 40/179]  eta: 0:01:36  Lr: 0.001875  Loss: 0.8346  Acc@1: 75.0000 (70.4268)  Acc@5: 93.7500 (91.3110)  time: 0.6803  data: 0.0017  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 50/179]  eta: 0:01:29  Lr: 0.001875  Loss: 0.8935  Acc@1: 68.7500 (70.5882)  Acc@5: 93.7500 (90.9314)  time: 0.6809  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 60/179]  eta: 0:01:22  Lr: 0.001875  Loss: 1.2454  Acc@1: 68.7500 (69.7746)  Acc@5: 87.5000 (90.5738)  time: 0.6815  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 70/179]  eta: 0:01:15  Lr: 0.001875  Loss: 0.6919  Acc@1: 68.7500 (70.1585)  Acc@5: 87.5000 (90.4049)  time: 0.6812  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 80/179]  eta: 0:01:08  Lr: 0.001875  Loss: 1.0160  Acc@1: 68.7500 (69.5216)  Acc@5: 93.7500 (90.8951)  time: 0.6813  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[3/5]  [ 90/179]  eta: 0:01:01  Lr: 0.001875  Loss: 1.1013  Acc@1: 68.7500 (70.0549)  Acc@5: 93.7500 (90.5907)  time: 0.6814  data: 0.0013  max mem: 2375\n",
            "Train: Epoch[3/5]  [100/179]  eta: 0:00:54  Lr: 0.001875  Loss: 1.1116  Acc@1: 68.7500 (70.2970)  Acc@5: 93.7500 (90.5941)  time: 0.6817  data: 0.0013  max mem: 2375\n",
            "Train: Epoch[3/5]  [110/179]  eta: 0:00:47  Lr: 0.001875  Loss: 0.6524  Acc@1: 75.0000 (70.4955)  Acc@5: 93.7500 (90.7095)  time: 0.6828  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[3/5]  [120/179]  eta: 0:00:40  Lr: 0.001875  Loss: 0.9107  Acc@1: 75.0000 (70.6612)  Acc@5: 93.7500 (90.9607)  time: 0.6825  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[3/5]  [130/179]  eta: 0:00:33  Lr: 0.001875  Loss: 0.8370  Acc@1: 68.7500 (70.5153)  Acc@5: 93.7500 (91.0305)  time: 0.6817  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[3/5]  [140/179]  eta: 0:00:26  Lr: 0.001875  Loss: 0.7256  Acc@1: 68.7500 (70.0798)  Acc@5: 87.5000 (91.0018)  time: 0.6827  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[3/5]  [150/179]  eta: 0:00:19  Lr: 0.001875  Loss: 0.7293  Acc@1: 62.5000 (70.0331)  Acc@5: 87.5000 (90.8526)  time: 0.6835  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[3/5]  [160/179]  eta: 0:00:13  Lr: 0.001875  Loss: 0.4914  Acc@1: 68.7500 (70.0699)  Acc@5: 87.5000 (90.8773)  time: 0.6844  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[3/5]  [170/179]  eta: 0:00:06  Lr: 0.001875  Loss: 0.9634  Acc@1: 68.7500 (69.9196)  Acc@5: 93.7500 (90.8260)  time: 0.6850  data: 0.0019  max mem: 2375\n",
            "Train: Epoch[3/5]  [178/179]  eta: 0:00:00  Lr: 0.001875  Loss: 0.8811  Acc@1: 68.7500 (69.9230)  Acc@5: 93.7500 (90.8613)  time: 0.6669  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[3/5] Total time: 0:02:02 (0.6838 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.8811  Acc@1: 68.7500 (69.9230)  Acc@5: 93.7500 (90.8613)\n",
            "Train: Epoch[4/5]  [  0/179]  eta: 0:02:57  Lr: 0.001875  Loss: 0.3109  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.9937  data: 0.3548  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 10/179]  eta: 0:02:00  Lr: 0.001875  Loss: 0.1773  Acc@1: 68.7500 (71.5909)  Acc@5: 93.7500 (93.1818)  time: 0.7122  data: 0.0336  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 20/179]  eta: 0:01:50  Lr: 0.001875  Loss: 0.9830  Acc@1: 68.7500 (69.9405)  Acc@5: 93.7500 (91.3690)  time: 0.6833  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 30/179]  eta: 0:01:43  Lr: 0.001875  Loss: 1.1233  Acc@1: 68.7500 (70.5645)  Acc@5: 93.7500 (92.3387)  time: 0.6824  data: 0.0014  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 40/179]  eta: 0:01:36  Lr: 0.001875  Loss: 0.6144  Acc@1: 68.7500 (69.9695)  Acc@5: 93.7500 (92.3780)  time: 0.6836  data: 0.0008  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 50/179]  eta: 0:01:28  Lr: 0.001875  Loss: 1.1294  Acc@1: 75.0000 (70.4657)  Acc@5: 93.7500 (91.9118)  time: 0.6840  data: 0.0003  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 60/179]  eta: 0:01:21  Lr: 0.001875  Loss: 0.6854  Acc@1: 75.0000 (70.3893)  Acc@5: 93.7500 (91.5984)  time: 0.6838  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 70/179]  eta: 0:01:14  Lr: 0.001875  Loss: 1.0439  Acc@1: 75.0000 (71.0387)  Acc@5: 93.7500 (91.3732)  time: 0.6841  data: 0.0016  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 80/179]  eta: 0:01:08  Lr: 0.001875  Loss: 1.1342  Acc@1: 81.2500 (71.6821)  Acc@5: 93.7500 (91.7438)  time: 0.6841  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[4/5]  [ 90/179]  eta: 0:01:01  Lr: 0.001875  Loss: 0.8844  Acc@1: 75.0000 (72.3214)  Acc@5: 93.7500 (91.7582)  time: 0.6841  data: 0.0009  max mem: 2375\n",
            "Train: Epoch[4/5]  [100/179]  eta: 0:00:54  Lr: 0.001875  Loss: 0.6936  Acc@1: 75.0000 (72.7104)  Acc@5: 93.7500 (91.8317)  time: 0.6842  data: 0.0020  max mem: 2375\n",
            "Train: Epoch[4/5]  [110/179]  eta: 0:00:47  Lr: 0.001875  Loss: 0.5622  Acc@1: 75.0000 (72.4099)  Acc@5: 93.7500 (91.6667)  time: 0.6845  data: 0.0020  max mem: 2375\n",
            "Train: Epoch[4/5]  [120/179]  eta: 0:00:40  Lr: 0.001875  Loss: 0.6229  Acc@1: 75.0000 (72.3657)  Acc@5: 93.7500 (91.8905)  time: 0.6842  data: 0.0009  max mem: 2375\n",
            "Train: Epoch[4/5]  [130/179]  eta: 0:00:33  Lr: 0.001875  Loss: 0.8633  Acc@1: 75.0000 (72.0420)  Acc@5: 93.7500 (91.8893)  time: 0.6852  data: 0.0010  max mem: 2375\n",
            "Train: Epoch[4/5]  [140/179]  eta: 0:00:26  Lr: 0.001875  Loss: 0.9474  Acc@1: 68.7500 (71.7199)  Acc@5: 93.7500 (91.8440)  time: 0.6854  data: 0.0021  max mem: 2375\n",
            "Train: Epoch[4/5]  [150/179]  eta: 0:00:19  Lr: 0.001875  Loss: 1.5803  Acc@1: 68.7500 (71.3576)  Acc@5: 87.5000 (91.7632)  time: 0.6841  data: 0.0023  max mem: 2375\n",
            "Train: Epoch[4/5]  [160/179]  eta: 0:00:13  Lr: 0.001875  Loss: 0.8696  Acc@1: 75.0000 (71.5450)  Acc@5: 87.5000 (91.7702)  time: 0.6843  data: 0.0021  max mem: 2375\n",
            "Train: Epoch[4/5]  [170/179]  eta: 0:00:06  Lr: 0.001875  Loss: 0.7360  Acc@1: 75.0000 (71.4547)  Acc@5: 93.7500 (91.8860)  time: 0.6851  data: 0.0020  max mem: 2375\n",
            "Train: Epoch[4/5]  [178/179]  eta: 0:00:00  Lr: 0.001875  Loss: 2.4339  Acc@1: 68.7500 (71.3585)  Acc@5: 93.7500 (91.8417)  time: 0.6685  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[4/5] Total time: 0:02:02 (0.6846 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 2.4339  Acc@1: 68.7500 (71.3585)  Acc@5: 93.7500 (91.8417)\n",
            "Train: Epoch[5/5]  [  0/179]  eta: 0:03:39  Lr: 0.001875  Loss: 0.7153  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 1.2273  data: 0.5747  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 10/179]  eta: 0:02:03  Lr: 0.001875  Loss: 0.7282  Acc@1: 81.2500 (75.0000)  Acc@5: 93.7500 (93.1818)  time: 0.7327  data: 0.0526  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 20/179]  eta: 0:01:52  Lr: 0.001875  Loss: 1.2626  Acc@1: 75.0000 (73.2143)  Acc@5: 93.7500 (93.7500)  time: 0.6831  data: 0.0007  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 30/179]  eta: 0:01:44  Lr: 0.001875  Loss: 0.6791  Acc@1: 75.0000 (72.7823)  Acc@5: 93.7500 (92.5403)  time: 0.6833  data: 0.0042  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 40/179]  eta: 0:01:36  Lr: 0.001875  Loss: 1.0520  Acc@1: 75.0000 (73.1707)  Acc@5: 87.5000 (91.7683)  time: 0.6841  data: 0.0039  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 50/179]  eta: 0:01:29  Lr: 0.001875  Loss: 0.7786  Acc@1: 75.0000 (72.9167)  Acc@5: 93.7500 (91.9118)  time: 0.6840  data: 0.0021  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 60/179]  eta: 0:01:22  Lr: 0.001875  Loss: 0.6920  Acc@1: 75.0000 (73.8730)  Acc@5: 93.7500 (92.0082)  time: 0.6834  data: 0.0022  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 70/179]  eta: 0:01:15  Lr: 0.001875  Loss: 0.6635  Acc@1: 75.0000 (73.9437)  Acc@5: 93.7500 (92.1655)  time: 0.6835  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 80/179]  eta: 0:01:08  Lr: 0.001875  Loss: 0.3918  Acc@1: 75.0000 (74.1512)  Acc@5: 93.7500 (92.2840)  time: 0.6841  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[5/5]  [ 90/179]  eta: 0:01:01  Lr: 0.001875  Loss: 0.7792  Acc@1: 68.7500 (73.4890)  Acc@5: 93.7500 (92.3764)  time: 0.6837  data: 0.0024  max mem: 2375\n",
            "Train: Epoch[5/5]  [100/179]  eta: 0:00:54  Lr: 0.001875  Loss: 1.0324  Acc@1: 68.7500 (73.3292)  Acc@5: 93.7500 (92.5124)  time: 0.6829  data: 0.0029  max mem: 2375\n",
            "Train: Epoch[5/5]  [110/179]  eta: 0:00:47  Lr: 0.001875  Loss: 0.8644  Acc@1: 68.7500 (72.8604)  Acc@5: 93.7500 (92.7928)  time: 0.6835  data: 0.0023  max mem: 2375\n",
            "Train: Epoch[5/5]  [120/179]  eta: 0:00:40  Lr: 0.001875  Loss: 0.5591  Acc@1: 68.7500 (72.8822)  Acc@5: 93.7500 (93.1302)  time: 0.6837  data: 0.0018  max mem: 2375\n",
            "Train: Epoch[5/5]  [130/179]  eta: 0:00:33  Lr: 0.001875  Loss: 0.4746  Acc@1: 75.0000 (73.1870)  Acc@5: 100.0000 (93.3206)  time: 0.6831  data: 0.0015  max mem: 2375\n",
            "Train: Epoch[5/5]  [140/179]  eta: 0:00:26  Lr: 0.001875  Loss: 0.8575  Acc@1: 75.0000 (73.4929)  Acc@5: 93.7500 (93.1738)  time: 0.6830  data: 0.0017  max mem: 2375\n",
            "Train: Epoch[5/5]  [150/179]  eta: 0:00:19  Lr: 0.001875  Loss: 1.0378  Acc@1: 75.0000 (73.3444)  Acc@5: 93.7500 (92.9636)  time: 0.6830  data: 0.0007  max mem: 2375\n",
            "Train: Epoch[5/5]  [160/179]  eta: 0:00:13  Lr: 0.001875  Loss: 0.6186  Acc@1: 75.0000 (73.4860)  Acc@5: 93.7500 (92.9348)  time: 0.6818  data: 0.0012  max mem: 2375\n",
            "Train: Epoch[5/5]  [170/179]  eta: 0:00:06  Lr: 0.001875  Loss: 1.3537  Acc@1: 75.0000 (73.3553)  Acc@5: 93.7500 (92.8728)  time: 0.6821  data: 0.0011  max mem: 2375\n",
            "Train: Epoch[5/5]  [178/179]  eta: 0:00:00  Lr: 0.001875  Loss: 0.7112  Acc@1: 75.0000 (73.7395)  Acc@5: 93.7500 (92.9272)  time: 0.6660  data: 0.0003  max mem: 2375\n",
            "Train: Epoch[5/5] Total time: 0:02:02 (0.6848 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.7112  Acc@1: 75.0000 (73.7395)  Acc@5: 93.7500 (92.9272)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:37  Loss: 2.8991 (2.8991)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 0.9054  data: 0.4864  max mem: 2375\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.5544 (2.4487)  Acc@1: 62.5000 (60.2273)  Acc@5: 87.5000 (87.5000)  time: 0.4835  data: 0.0447  max mem: 2375\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.8473 (2.0227)  Acc@1: 75.0000 (68.4524)  Acc@5: 93.7500 (90.4762)  time: 0.4425  data: 0.0009  max mem: 2375\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.8519 (2.1232)  Acc@1: 68.7500 (65.1210)  Acc@5: 87.5000 (88.5081)  time: 0.4429  data: 0.0027  max mem: 2375\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2136 (2.1528)  Acc@1: 62.5000 (64.7692)  Acc@5: 87.5000 (87.8462)  time: 0.4352  data: 0.0021  max mem: 2375\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4516 s / it)\n",
            "* Acc@1 64.769 Acc@5 87.846 loss 2.153\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:33  Loss: 1.0352 (1.0352)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.7118  data: 0.2994  max mem: 2375\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1268 (1.3580)  Acc@1: 81.2500 (79.5455)  Acc@5: 93.7500 (93.1818)  time: 0.4664  data: 0.0288  max mem: 2375\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 2.0074 (1.8725)  Acc@1: 75.0000 (72.3214)  Acc@5: 93.7500 (87.7976)  time: 0.4422  data: 0.0020  max mem: 2375\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.1648 (1.9389)  Acc@1: 75.0000 (73.1855)  Acc@5: 87.5000 (87.7016)  time: 0.4426  data: 0.0014  max mem: 2375\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1175 (2.0206)  Acc@1: 68.7500 (70.7317)  Acc@5: 87.5000 (87.8049)  time: 0.4428  data: 0.0009  max mem: 2375\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0422 (1.9505)  Acc@1: 62.5000 (71.6000)  Acc@5: 87.5000 (88.0000)  time: 0.4444  data: 0.0008  max mem: 2375\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4508 s / it)\n",
            "* Acc@1 71.600 Acc@5 88.000 loss 1.951\n",
            "[Average accuracy till task2]\tAcc@1: 68.1846\tAcc@5: 87.9231\tLoss: 2.0517\tForgetting: 7.0769\tBackward: -7.0769\n",
            "Train: Epoch[1/5]  [  0/110]  eta: 0:02:30  Lr: 0.001875  Loss: 2.8194  Acc@1: 6.2500 (6.2500)  Acc@5: 37.5000 (37.5000)  time: 1.3646  data: 0.6807  max mem: 2376\n",
            "Train: Epoch[1/5]  [ 10/110]  eta: 0:01:13  Lr: 0.001875  Loss: 2.6324  Acc@1: 25.0000 (22.7273)  Acc@5: 43.7500 (48.2955)  time: 0.7393  data: 0.0624  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/110]  eta: 0:01:03  Lr: 0.001875  Loss: 2.4031  Acc@1: 25.0000 (27.0833)  Acc@5: 56.2500 (56.5476)  time: 0.6780  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/110]  eta: 0:00:56  Lr: 0.001875  Loss: 2.0048  Acc@1: 37.5000 (31.0484)  Acc@5: 68.7500 (61.6935)  time: 0.6797  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/110]  eta: 0:00:48  Lr: 0.001875  Loss: 2.0422  Acc@1: 43.7500 (33.9939)  Acc@5: 81.2500 (66.1585)  time: 0.6805  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/110]  eta: 0:00:41  Lr: 0.001875  Loss: 2.2733  Acc@1: 43.7500 (36.3971)  Acc@5: 81.2500 (69.7304)  time: 0.6818  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/110]  eta: 0:00:34  Lr: 0.001875  Loss: 1.4089  Acc@1: 50.0000 (39.2418)  Acc@5: 81.2500 (71.7213)  time: 0.6823  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/110]  eta: 0:00:27  Lr: 0.001875  Loss: 1.4540  Acc@1: 56.2500 (41.4613)  Acc@5: 81.2500 (72.8873)  time: 0.6831  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/110]  eta: 0:00:20  Lr: 0.001875  Loss: 1.1982  Acc@1: 56.2500 (43.1327)  Acc@5: 81.2500 (74.3056)  time: 0.6848  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/110]  eta: 0:00:13  Lr: 0.001875  Loss: 1.4989  Acc@1: 62.5000 (45.4670)  Acc@5: 87.5000 (76.3049)  time: 0.6852  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/110]  eta: 0:00:06  Lr: 0.001875  Loss: 1.1668  Acc@1: 62.5000 (47.0297)  Acc@5: 93.7500 (77.3515)  time: 0.6847  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[1/5]  [109/110]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1353  Acc@1: 62.5000 (47.8261)  Acc@5: 81.2500 (77.5744)  time: 0.6614  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:15 (0.6852 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1353  Acc@1: 62.5000 (47.8261)  Acc@5: 81.2500 (77.5744)\n",
            "Train: Epoch[2/5]  [  0/110]  eta: 0:01:55  Lr: 0.001875  Loss: 1.3413  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 1.0460  data: 0.4110  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/110]  eta: 0:01:11  Lr: 0.001875  Loss: 1.6078  Acc@1: 62.5000 (64.7727)  Acc@5: 87.5000 (86.3636)  time: 0.7160  data: 0.0381  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/110]  eta: 0:01:03  Lr: 0.001875  Loss: 1.0020  Acc@1: 62.5000 (64.2857)  Acc@5: 87.5000 (87.5000)  time: 0.6832  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/110]  eta: 0:00:55  Lr: 0.001875  Loss: 1.0890  Acc@1: 62.5000 (62.7016)  Acc@5: 87.5000 (87.5000)  time: 0.6829  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/110]  eta: 0:00:48  Lr: 0.001875  Loss: 1.3182  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (86.8902)  time: 0.6822  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/110]  eta: 0:00:41  Lr: 0.001875  Loss: 1.2793  Acc@1: 62.5000 (63.4804)  Acc@5: 87.5000 (88.2353)  time: 0.6821  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/110]  eta: 0:00:34  Lr: 0.001875  Loss: 0.9794  Acc@1: 68.7500 (64.2418)  Acc@5: 93.7500 (88.3197)  time: 0.6820  data: 0.0033  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/110]  eta: 0:00:27  Lr: 0.001875  Loss: 1.1321  Acc@1: 68.7500 (64.1725)  Acc@5: 87.5000 (87.8521)  time: 0.6821  data: 0.0032  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/110]  eta: 0:00:20  Lr: 0.001875  Loss: 0.7137  Acc@1: 62.5000 (63.9660)  Acc@5: 87.5000 (87.9630)  time: 0.6823  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/110]  eta: 0:00:13  Lr: 0.001875  Loss: 1.2925  Acc@1: 62.5000 (63.8049)  Acc@5: 93.7500 (87.8434)  time: 0.6822  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/110]  eta: 0:00:06  Lr: 0.001875  Loss: 1.0013  Acc@1: 62.5000 (63.6757)  Acc@5: 87.5000 (88.1188)  time: 0.6823  data: 0.0025  max mem: 2378\n",
            "Train: Epoch[2/5]  [109/110]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1517  Acc@1: 62.5000 (63.2723)  Acc@5: 87.5000 (88.1579)  time: 0.6565  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:14 (0.6816 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1517  Acc@1: 62.5000 (63.2723)  Acc@5: 87.5000 (88.1579)\n",
            "Train: Epoch[3/5]  [  0/110]  eta: 0:01:45  Lr: 0.001875  Loss: 1.1132  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.9573  data: 0.3228  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/110]  eta: 0:01:10  Lr: 0.001875  Loss: 0.6950  Acc@1: 68.7500 (67.0455)  Acc@5: 93.7500 (93.7500)  time: 0.7059  data: 0.0310  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/110]  eta: 0:01:02  Lr: 0.001875  Loss: 0.9034  Acc@1: 68.7500 (66.3690)  Acc@5: 93.7500 (92.2619)  time: 0.6813  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/110]  eta: 0:00:55  Lr: 0.001875  Loss: 0.8752  Acc@1: 62.5000 (65.9274)  Acc@5: 87.5000 (91.9355)  time: 0.6816  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/110]  eta: 0:00:48  Lr: 0.001875  Loss: 0.7499  Acc@1: 68.7500 (66.4634)  Acc@5: 93.7500 (92.3780)  time: 0.6811  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/110]  eta: 0:00:41  Lr: 0.001875  Loss: 1.1521  Acc@1: 68.7500 (67.2794)  Acc@5: 93.7500 (92.0343)  time: 0.6806  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/110]  eta: 0:00:34  Lr: 0.001875  Loss: 0.6146  Acc@1: 62.5000 (67.3156)  Acc@5: 93.7500 (92.2131)  time: 0.6808  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/110]  eta: 0:00:27  Lr: 0.001875  Loss: 1.0157  Acc@1: 75.0000 (67.9577)  Acc@5: 93.7500 (92.4296)  time: 0.6801  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/110]  eta: 0:00:20  Lr: 0.001875  Loss: 1.1911  Acc@1: 62.5000 (66.9753)  Acc@5: 93.7500 (92.0525)  time: 0.6796  data: 0.0027  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/110]  eta: 0:00:13  Lr: 0.001875  Loss: 1.0540  Acc@1: 62.5000 (66.8269)  Acc@5: 93.7500 (92.3077)  time: 0.6809  data: 0.0027  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/110]  eta: 0:00:06  Lr: 0.001875  Loss: 1.0121  Acc@1: 62.5000 (66.9554)  Acc@5: 93.7500 (92.3886)  time: 0.6808  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [109/110]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6783  Acc@1: 68.7500 (67.1053)  Acc@5: 93.7500 (92.5629)  time: 0.6559  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:14 (0.6793 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6783  Acc@1: 68.7500 (67.1053)  Acc@5: 93.7500 (92.5629)\n",
            "Train: Epoch[4/5]  [  0/110]  eta: 0:01:43  Lr: 0.001875  Loss: 0.8505  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 0.9379  data: 0.2969  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/110]  eta: 0:01:10  Lr: 0.001875  Loss: 0.9091  Acc@1: 68.7500 (69.8864)  Acc@5: 93.7500 (93.1818)  time: 0.7044  data: 0.0288  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/110]  eta: 0:01:02  Lr: 0.001875  Loss: 1.2120  Acc@1: 68.7500 (69.6429)  Acc@5: 93.7500 (91.3690)  time: 0.6806  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/110]  eta: 0:00:55  Lr: 0.001875  Loss: 1.1997  Acc@1: 68.7500 (69.1532)  Acc@5: 93.7500 (91.3306)  time: 0.6795  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/110]  eta: 0:00:48  Lr: 0.001875  Loss: 0.7450  Acc@1: 68.7500 (69.8171)  Acc@5: 93.7500 (91.9207)  time: 0.6794  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/110]  eta: 0:00:41  Lr: 0.001875  Loss: 0.9127  Acc@1: 75.0000 (70.7108)  Acc@5: 93.7500 (92.7696)  time: 0.6801  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/110]  eta: 0:00:34  Lr: 0.001875  Loss: 1.1357  Acc@1: 68.7500 (70.6967)  Acc@5: 93.7500 (92.6230)  time: 0.6804  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/110]  eta: 0:00:27  Lr: 0.001875  Loss: 0.8999  Acc@1: 68.7500 (70.6866)  Acc@5: 93.7500 (92.4296)  time: 0.6802  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/110]  eta: 0:00:20  Lr: 0.001875  Loss: 0.5635  Acc@1: 75.0000 (71.4506)  Acc@5: 93.7500 (92.6698)  time: 0.6797  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/110]  eta: 0:00:13  Lr: 0.001875  Loss: 0.5713  Acc@1: 68.7500 (70.8104)  Acc@5: 93.7500 (92.7198)  time: 0.6795  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/110]  eta: 0:00:06  Lr: 0.001875  Loss: 0.7540  Acc@1: 68.7500 (70.6064)  Acc@5: 93.7500 (93.0074)  time: 0.6800  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[4/5]  [109/110]  eta: 0:00:00  Lr: 0.001875  Loss: 0.0687  Acc@1: 68.7500 (71.1098)  Acc@5: 93.7500 (93.1922)  time: 0.6544  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:14 (0.6783 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.0687  Acc@1: 68.7500 (71.1098)  Acc@5: 93.7500 (93.1922)\n",
            "Train: Epoch[5/5]  [  0/110]  eta: 0:01:51  Lr: 0.001875  Loss: 1.1632  Acc@1: 56.2500 (56.2500)  Acc@5: 93.7500 (93.7500)  time: 1.0108  data: 0.3800  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/110]  eta: 0:01:10  Lr: 0.001875  Loss: 0.9972  Acc@1: 81.2500 (75.5682)  Acc@5: 93.7500 (93.1818)  time: 0.7092  data: 0.0364  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/110]  eta: 0:01:02  Lr: 0.001875  Loss: 1.3685  Acc@1: 75.0000 (75.5952)  Acc@5: 93.7500 (91.9643)  time: 0.6789  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/110]  eta: 0:00:55  Lr: 0.001875  Loss: 0.7694  Acc@1: 75.0000 (73.9919)  Acc@5: 87.5000 (91.9355)  time: 0.6783  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/110]  eta: 0:00:48  Lr: 0.001875  Loss: 0.9248  Acc@1: 68.7500 (73.0183)  Acc@5: 93.7500 (93.2927)  time: 0.6781  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/110]  eta: 0:00:41  Lr: 0.001875  Loss: 0.8547  Acc@1: 68.7500 (73.5294)  Acc@5: 93.7500 (93.2598)  time: 0.6783  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/110]  eta: 0:00:34  Lr: 0.001875  Loss: 0.2274  Acc@1: 81.2500 (74.1803)  Acc@5: 93.7500 (93.9549)  time: 0.6785  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/110]  eta: 0:00:27  Lr: 0.001875  Loss: 0.7954  Acc@1: 75.0000 (73.8556)  Acc@5: 100.0000 (94.2782)  time: 0.6780  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/110]  eta: 0:00:20  Lr: 0.001875  Loss: 0.4640  Acc@1: 75.0000 (74.4599)  Acc@5: 93.7500 (94.3673)  time: 0.6783  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/110]  eta: 0:00:13  Lr: 0.001875  Loss: 0.6732  Acc@1: 75.0000 (73.4890)  Acc@5: 93.7500 (94.3681)  time: 0.6794  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/110]  eta: 0:00:06  Lr: 0.001875  Loss: 1.1505  Acc@1: 68.7500 (72.5866)  Acc@5: 87.5000 (93.6881)  time: 0.6792  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [109/110]  eta: 0:00:00  Lr: 0.001875  Loss: 0.2497  Acc@1: 68.7500 (72.4828)  Acc@5: 87.5000 (93.6499)  time: 0.6536  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:14 (0.6781 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.2497  Acc@1: 68.7500 (72.4828)  Acc@5: 87.5000 (93.6499)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:44  Loss: 3.0207 (3.0207)  Acc@1: 62.5000 (62.5000)  Acc@5: 81.2500 (81.2500)  time: 1.0964  data: 0.6832  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.7513 (2.6899)  Acc@1: 56.2500 (55.6818)  Acc@5: 81.2500 (78.4091)  time: 0.4967  data: 0.0663  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0140 (2.2036)  Acc@1: 68.7500 (65.7738)  Acc@5: 87.5000 (83.9286)  time: 0.4395  data: 0.0026  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.8679 (2.2373)  Acc@1: 68.7500 (63.7097)  Acc@5: 87.5000 (83.2661)  time: 0.4418  data: 0.0004  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2206 (2.2442)  Acc@1: 62.5000 (63.8462)  Acc@5: 81.2500 (82.6154)  time: 0.4335  data: 0.0003  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4542 s / it)\n",
            "* Acc@1 63.846 Acc@5 82.615 loss 2.244\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:37  Loss: 0.9311 (0.9311)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.8080  data: 0.4058  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.3228 (1.4388)  Acc@1: 81.2500 (79.5455)  Acc@5: 93.7500 (90.3409)  time: 0.4739  data: 0.0377  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.9351 (1.9605)  Acc@1: 68.7500 (70.8333)  Acc@5: 87.5000 (84.2262)  time: 0.4416  data: 0.0007  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.2262 (2.0376)  Acc@1: 68.7500 (70.9677)  Acc@5: 87.5000 (85.2823)  time: 0.4420  data: 0.0019  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2365 (2.1530)  Acc@1: 62.5000 (67.6829)  Acc@5: 87.5000 (83.9939)  time: 0.4416  data: 0.0022  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.2262 (2.0714)  Acc@1: 56.2500 (68.1333)  Acc@5: 81.2500 (84.2667)  time: 0.4391  data: 0.0013  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4497 s / it)\n",
            "* Acc@1 68.133 Acc@5 84.267 loss 2.071\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:22  Loss: 2.7883 (2.7883)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (87.5000)  time: 0.8489  data: 0.4458  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.6382 (1.7531)  Acc@1: 62.5000 (61.3636)  Acc@5: 93.7500 (90.3409)  time: 0.4789  data: 0.0433  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.6711 (1.8206)  Acc@1: 62.5000 (60.4167)  Acc@5: 93.7500 (88.9881)  time: 0.4418  data: 0.0020  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.7242 (1.8649)  Acc@1: 62.5000 (58.7822)  Acc@5: 93.7500 (89.6956)  time: 0.4385  data: 0.0016  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4568 s / it)\n",
            "* Acc@1 58.782 Acc@5 89.696 loss 1.865\n",
            "[Average accuracy till task3]\tAcc@1: 63.5872\tAcc@5: 85.5259\tLoss: 2.0602\tForgetting: 5.7333\tBackward: -5.7333\n",
            "Train: Epoch[1/5]  [  0/148]  eta: 0:02:18  Lr: 0.001875  Loss: 2.8232  Acc@1: 6.2500 (6.2500)  Acc@5: 37.5000 (37.5000)  time: 0.9376  data: 0.3021  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/148]  eta: 0:01:36  Lr: 0.001875  Loss: 2.6592  Acc@1: 12.5000 (16.4773)  Acc@5: 50.0000 (51.1364)  time: 0.7022  data: 0.0277  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/148]  eta: 0:01:28  Lr: 0.001875  Loss: 2.0994  Acc@1: 25.0000 (27.0833)  Acc@5: 62.5000 (61.0119)  time: 0.6797  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/148]  eta: 0:01:21  Lr: 0.001875  Loss: 2.3264  Acc@1: 43.7500 (31.4516)  Acc@5: 75.0000 (66.7339)  time: 0.6813  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/148]  eta: 0:01:14  Lr: 0.001875  Loss: 2.1571  Acc@1: 43.7500 (33.8415)  Acc@5: 75.0000 (68.4451)  time: 0.6821  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/148]  eta: 0:01:07  Lr: 0.001875  Loss: 2.1555  Acc@1: 43.7500 (37.1324)  Acc@5: 75.0000 (70.3431)  time: 0.6832  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/148]  eta: 0:01:00  Lr: 0.001875  Loss: 1.8201  Acc@1: 50.0000 (40.0615)  Acc@5: 81.2500 (72.8484)  time: 0.6840  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/148]  eta: 0:00:53  Lr: 0.001875  Loss: 1.5271  Acc@1: 56.2500 (42.4296)  Acc@5: 87.5000 (73.9437)  time: 0.6831  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/148]  eta: 0:00:46  Lr: 0.001875  Loss: 1.6050  Acc@1: 56.2500 (43.2099)  Acc@5: 81.2500 (74.8457)  time: 0.6807  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/148]  eta: 0:00:39  Lr: 0.001875  Loss: 1.5061  Acc@1: 50.0000 (44.0247)  Acc@5: 81.2500 (75.4808)  time: 0.6793  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/148]  eta: 0:00:32  Lr: 0.001875  Loss: 1.5940  Acc@1: 50.0000 (44.9257)  Acc@5: 81.2500 (75.9901)  time: 0.6796  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/148]  eta: 0:00:25  Lr: 0.001875  Loss: 1.5554  Acc@1: 56.2500 (46.2275)  Acc@5: 87.5000 (77.0270)  time: 0.6792  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/148]  eta: 0:00:19  Lr: 0.001875  Loss: 1.8632  Acc@1: 56.2500 (46.9008)  Acc@5: 87.5000 (77.3244)  time: 0.6789  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/148]  eta: 0:00:12  Lr: 0.001875  Loss: 1.2837  Acc@1: 50.0000 (47.6622)  Acc@5: 81.2500 (77.8149)  time: 0.6788  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[1/5]  [140/148]  eta: 0:00:05  Lr: 0.001875  Loss: 1.4942  Acc@1: 56.2500 (48.5372)  Acc@5: 87.5000 (78.5018)  time: 0.6770  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [147/148]  eta: 0:00:00  Lr: 0.001875  Loss: 0.9326  Acc@1: 62.5000 (49.2151)  Acc@5: 87.5000 (78.7442)  time: 0.6531  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:40 (0.6793 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.9326  Acc@1: 62.5000 (49.2151)  Acc@5: 87.5000 (78.7442)\n",
            "Train: Epoch[2/5]  [  0/148]  eta: 0:02:41  Lr: 0.001875  Loss: 1.2118  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 1.0907  data: 0.4634  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/148]  eta: 0:01:38  Lr: 0.001875  Loss: 1.2542  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (92.0455)  time: 0.7123  data: 0.0432  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/148]  eta: 0:01:28  Lr: 0.001875  Loss: 1.3526  Acc@1: 62.5000 (65.4762)  Acc@5: 87.5000 (90.7738)  time: 0.6751  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/148]  eta: 0:01:21  Lr: 0.001875  Loss: 0.4791  Acc@1: 62.5000 (65.1210)  Acc@5: 87.5000 (91.1290)  time: 0.6749  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/148]  eta: 0:01:13  Lr: 0.001875  Loss: 1.4485  Acc@1: 62.5000 (64.7866)  Acc@5: 87.5000 (89.9390)  time: 0.6738  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/148]  eta: 0:01:06  Lr: 0.001875  Loss: 1.2473  Acc@1: 62.5000 (65.4412)  Acc@5: 87.5000 (89.5833)  time: 0.6748  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/148]  eta: 0:01:00  Lr: 0.001875  Loss: 1.1203  Acc@1: 62.5000 (65.5738)  Acc@5: 93.7500 (90.0615)  time: 0.6764  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/148]  eta: 0:00:53  Lr: 0.001875  Loss: 1.2633  Acc@1: 62.5000 (65.2289)  Acc@5: 93.7500 (89.8768)  time: 0.6776  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/148]  eta: 0:00:46  Lr: 0.001875  Loss: 0.6945  Acc@1: 68.7500 (65.9722)  Acc@5: 87.5000 (89.8148)  time: 0.6799  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/148]  eta: 0:00:39  Lr: 0.001875  Loss: 1.0510  Acc@1: 62.5000 (65.3159)  Acc@5: 87.5000 (89.3544)  time: 0.6811  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/148]  eta: 0:00:32  Lr: 0.001875  Loss: 0.7751  Acc@1: 62.5000 (65.5322)  Acc@5: 87.5000 (89.7896)  time: 0.6816  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/148]  eta: 0:00:25  Lr: 0.001875  Loss: 0.8333  Acc@1: 68.7500 (66.0473)  Acc@5: 93.7500 (89.9775)  time: 0.6821  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/148]  eta: 0:00:19  Lr: 0.001875  Loss: 1.0044  Acc@1: 68.7500 (66.1157)  Acc@5: 87.5000 (90.0310)  time: 0.6827  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/148]  eta: 0:00:12  Lr: 0.001875  Loss: 0.7901  Acc@1: 62.5000 (65.6489)  Acc@5: 93.7500 (90.1718)  time: 0.6839  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [140/148]  eta: 0:00:05  Lr: 0.001875  Loss: 0.8260  Acc@1: 68.7500 (66.0018)  Acc@5: 93.7500 (89.9379)  time: 0.6838  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [147/148]  eta: 0:00:00  Lr: 0.001875  Loss: 2.0821  Acc@1: 68.7500 (65.9313)  Acc@5: 87.5000 (89.8600)  time: 0.6603  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:40 (0.6794 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 2.0821  Acc@1: 68.7500 (65.9313)  Acc@5: 87.5000 (89.8600)\n",
            "Train: Epoch[3/5]  [  0/148]  eta: 0:02:25  Lr: 0.001875  Loss: 0.9774  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.9849  data: 0.3513  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/148]  eta: 0:01:38  Lr: 0.001875  Loss: 0.9468  Acc@1: 62.5000 (66.4773)  Acc@5: 93.7500 (93.1818)  time: 0.7107  data: 0.0326  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/148]  eta: 0:01:29  Lr: 0.001875  Loss: 0.8220  Acc@1: 62.5000 (64.2857)  Acc@5: 93.7500 (91.6667)  time: 0.6828  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/148]  eta: 0:01:21  Lr: 0.001875  Loss: 1.1679  Acc@1: 62.5000 (66.7339)  Acc@5: 93.7500 (90.9274)  time: 0.6825  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/148]  eta: 0:01:14  Lr: 0.001875  Loss: 1.1121  Acc@1: 62.5000 (65.5488)  Acc@5: 87.5000 (90.3963)  time: 0.6828  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/148]  eta: 0:01:07  Lr: 0.001875  Loss: 0.5335  Acc@1: 62.5000 (65.9314)  Acc@5: 93.7500 (91.5441)  time: 0.6835  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/148]  eta: 0:01:00  Lr: 0.001875  Loss: 0.1070  Acc@1: 68.7500 (67.6230)  Acc@5: 100.0000 (92.2131)  time: 0.6844  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/148]  eta: 0:00:53  Lr: 0.001875  Loss: 1.3463  Acc@1: 68.7500 (67.6056)  Acc@5: 93.7500 (92.1655)  time: 0.6836  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/148]  eta: 0:00:46  Lr: 0.001875  Loss: 0.7735  Acc@1: 75.0000 (68.2099)  Acc@5: 93.7500 (92.0525)  time: 0.6810  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/148]  eta: 0:00:39  Lr: 0.001875  Loss: 1.1857  Acc@1: 75.0000 (68.8874)  Acc@5: 93.7500 (92.1016)  time: 0.6807  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/148]  eta: 0:00:32  Lr: 0.001875  Loss: 0.9457  Acc@1: 75.0000 (69.1832)  Acc@5: 93.7500 (92.1411)  time: 0.6811  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/148]  eta: 0:00:26  Lr: 0.001875  Loss: 0.7879  Acc@1: 75.0000 (69.2568)  Acc@5: 93.7500 (92.1734)  time: 0.6803  data: 0.0027  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/148]  eta: 0:00:19  Lr: 0.001875  Loss: 1.2486  Acc@1: 75.0000 (69.4731)  Acc@5: 93.7500 (92.0971)  time: 0.6798  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/148]  eta: 0:00:12  Lr: 0.001875  Loss: 0.9840  Acc@1: 68.7500 (69.4179)  Acc@5: 93.7500 (92.4141)  time: 0.6797  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [140/148]  eta: 0:00:05  Lr: 0.001875  Loss: 0.7417  Acc@1: 68.7500 (69.6809)  Acc@5: 93.7500 (92.3759)  time: 0.6794  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [147/148]  eta: 0:00:00  Lr: 0.001875  Loss: 0.4219  Acc@1: 68.7500 (69.5800)  Acc@5: 93.7500 (92.2359)  time: 0.6557  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:40 (0.6809 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.4219  Acc@1: 68.7500 (69.5800)  Acc@5: 93.7500 (92.2359)\n",
            "Train: Epoch[4/5]  [  0/148]  eta: 0:03:01  Lr: 0.001875  Loss: 1.1115  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (87.5000)  time: 1.2256  data: 0.5821  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/148]  eta: 0:01:40  Lr: 0.001875  Loss: 1.0802  Acc@1: 68.7500 (67.0455)  Acc@5: 93.7500 (93.1818)  time: 0.7271  data: 0.0572  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/148]  eta: 0:01:30  Lr: 0.001875  Loss: 0.8388  Acc@1: 68.7500 (70.5357)  Acc@5: 93.7500 (94.3452)  time: 0.6772  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/148]  eta: 0:01:22  Lr: 0.001875  Loss: 0.7932  Acc@1: 68.7500 (70.1613)  Acc@5: 87.5000 (92.7419)  time: 0.6772  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/148]  eta: 0:01:14  Lr: 0.001875  Loss: 0.8733  Acc@1: 75.0000 (70.5793)  Acc@5: 87.5000 (92.0732)  time: 0.6788  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/148]  eta: 0:01:07  Lr: 0.001875  Loss: 1.0297  Acc@1: 75.0000 (70.3431)  Acc@5: 87.5000 (92.0343)  time: 0.6794  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/148]  eta: 0:01:00  Lr: 0.001875  Loss: 0.5819  Acc@1: 68.7500 (70.7992)  Acc@5: 93.7500 (92.7254)  time: 0.6789  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/148]  eta: 0:00:53  Lr: 0.001875  Loss: 0.8669  Acc@1: 68.7500 (70.6866)  Acc@5: 93.7500 (92.6937)  time: 0.6794  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/148]  eta: 0:00:46  Lr: 0.001875  Loss: 0.6945  Acc@1: 68.7500 (69.9074)  Acc@5: 93.7500 (92.2068)  time: 0.6792  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/148]  eta: 0:00:39  Lr: 0.001875  Loss: 0.7132  Acc@1: 68.7500 (70.3984)  Acc@5: 93.7500 (92.4451)  time: 0.6796  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/148]  eta: 0:00:32  Lr: 0.001875  Loss: 0.6576  Acc@1: 75.0000 (70.6064)  Acc@5: 93.7500 (92.4505)  time: 0.6793  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/148]  eta: 0:00:25  Lr: 0.001875  Loss: 0.3700  Acc@1: 75.0000 (70.7207)  Acc@5: 93.7500 (92.5676)  time: 0.6783  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/148]  eta: 0:00:19  Lr: 0.001875  Loss: 0.7585  Acc@1: 75.0000 (71.0744)  Acc@5: 93.7500 (92.6653)  time: 0.6791  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/148]  eta: 0:00:12  Lr: 0.001875  Loss: 1.5531  Acc@1: 68.7500 (70.4198)  Acc@5: 93.7500 (92.6527)  time: 0.6786  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[4/5]  [140/148]  eta: 0:00:05  Lr: 0.001875  Loss: 0.7326  Acc@1: 68.7500 (70.7447)  Acc@5: 93.7500 (92.6862)  time: 0.6780  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[4/5]  [147/148]  eta: 0:00:00  Lr: 0.001875  Loss: 0.7811  Acc@1: 68.7500 (70.7679)  Acc@5: 93.7500 (92.6602)  time: 0.6550  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:40 (0.6797 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.7811  Acc@1: 68.7500 (70.7679)  Acc@5: 93.7500 (92.6602)\n",
            "Train: Epoch[5/5]  [  0/148]  eta: 0:02:11  Lr: 0.001875  Loss: 0.9318  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.8885  data: 0.2521  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/148]  eta: 0:01:36  Lr: 0.001875  Loss: 1.2261  Acc@1: 75.0000 (73.8636)  Acc@5: 93.7500 (94.8864)  time: 0.6998  data: 0.0232  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/148]  eta: 0:01:28  Lr: 0.001875  Loss: 1.1304  Acc@1: 75.0000 (72.3214)  Acc@5: 93.7500 (94.0476)  time: 0.6801  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/148]  eta: 0:01:21  Lr: 0.001875  Loss: 0.9042  Acc@1: 75.0000 (73.5887)  Acc@5: 93.7500 (93.5484)  time: 0.6797  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/148]  eta: 0:01:14  Lr: 0.001875  Loss: 0.7638  Acc@1: 68.7500 (71.4939)  Acc@5: 93.7500 (93.1402)  time: 0.6803  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/148]  eta: 0:01:07  Lr: 0.001875  Loss: 0.3385  Acc@1: 68.7500 (71.8137)  Acc@5: 93.7500 (93.0147)  time: 0.6807  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/148]  eta: 0:01:00  Lr: 0.001875  Loss: 0.5701  Acc@1: 75.0000 (71.9262)  Acc@5: 93.7500 (92.7254)  time: 0.6810  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/148]  eta: 0:00:53  Lr: 0.001875  Loss: 1.1869  Acc@1: 75.0000 (71.7430)  Acc@5: 93.7500 (92.7817)  time: 0.6813  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/148]  eta: 0:00:46  Lr: 0.001875  Loss: 0.4499  Acc@1: 68.7500 (71.9136)  Acc@5: 93.7500 (93.1327)  time: 0.6813  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/148]  eta: 0:00:39  Lr: 0.001875  Loss: 0.4489  Acc@1: 75.0000 (72.3214)  Acc@5: 93.7500 (93.2005)  time: 0.6812  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/148]  eta: 0:00:32  Lr: 0.001875  Loss: 0.6332  Acc@1: 75.0000 (72.8342)  Acc@5: 93.7500 (93.5025)  time: 0.6813  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/148]  eta: 0:00:25  Lr: 0.001875  Loss: 0.9340  Acc@1: 75.0000 (72.8604)  Acc@5: 100.0000 (93.3559)  time: 0.6819  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/148]  eta: 0:00:19  Lr: 0.001875  Loss: 0.5257  Acc@1: 75.0000 (72.5723)  Acc@5: 93.7500 (93.2851)  time: 0.6812  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/148]  eta: 0:00:12  Lr: 0.001875  Loss: 0.5864  Acc@1: 75.0000 (72.7099)  Acc@5: 93.7500 (93.2252)  time: 0.6806  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[5/5]  [140/148]  eta: 0:00:05  Lr: 0.001875  Loss: 0.3735  Acc@1: 68.7500 (72.9610)  Acc@5: 93.7500 (93.3511)  time: 0.6815  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[5/5]  [147/148]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1870  Acc@1: 68.7500 (72.8044)  Acc@5: 93.7500 (93.3814)  time: 0.6578  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:40 (0.6797 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1870  Acc@1: 68.7500 (72.8044)  Acc@5: 93.7500 (93.3814)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:34  Loss: 2.9212 (2.9212)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 0.8360  data: 0.4314  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.6791 (2.6872)  Acc@1: 43.7500 (48.2955)  Acc@5: 81.2500 (74.4318)  time: 0.4765  data: 0.0410  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0007 (2.2151)  Acc@1: 56.2500 (58.6310)  Acc@5: 87.5000 (81.8452)  time: 0.4416  data: 0.0014  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.8298 (2.2529)  Acc@1: 62.5000 (57.6613)  Acc@5: 87.5000 (80.4435)  time: 0.4411  data: 0.0032  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2340 (2.2691)  Acc@1: 56.2500 (58.1538)  Acc@5: 75.0000 (79.3846)  time: 0.4330  data: 0.0029  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4486 s / it)\n",
            "* Acc@1 58.154 Acc@5 79.385 loss 2.269\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:39  Loss: 1.0037 (1.0037)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.8428  data: 0.4393  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.2320 (1.4801)  Acc@1: 87.5000 (78.4091)  Acc@5: 93.7500 (89.2045)  time: 0.4779  data: 0.0417  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8390 (1.9986)  Acc@1: 68.7500 (66.6667)  Acc@5: 87.5000 (82.1429)  time: 0.4408  data: 0.0037  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.3694 (2.1176)  Acc@1: 56.2500 (66.3306)  Acc@5: 81.2500 (81.6532)  time: 0.4408  data: 0.0032  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.3694 (2.2281)  Acc@1: 56.2500 (62.9573)  Acc@5: 81.2500 (80.1829)  time: 0.4413  data: 0.0008  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.3441 (2.1411)  Acc@1: 56.2500 (63.8667)  Acc@5: 81.2500 (80.6667)  time: 0.4394  data: 0.0005  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4512 s / it)\n",
            "* Acc@1 63.867 Acc@5 80.667 loss 2.141\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:29  Loss: 2.9388 (2.9388)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 1.0859  data: 0.6685  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.7004 (1.8509)  Acc@1: 62.5000 (59.0909)  Acc@5: 93.7500 (88.6364)  time: 0.5384  data: 0.1048  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7180 (1.8874)  Acc@1: 62.5000 (59.2262)  Acc@5: 93.7500 (86.9048)  time: 0.4621  data: 0.0245  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.7180 (1.9356)  Acc@1: 56.2500 (57.3770)  Acc@5: 93.7500 (87.3536)  time: 0.4347  data: 0.0004  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4789 s / it)\n",
            "* Acc@1 57.377 Acc@5 87.354 loss 1.936\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:39  Loss: 2.3813 (2.3813)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 1.1108  data: 0.6936  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:13  Loss: 2.1479 (1.8624)  Acc@1: 62.5000 (58.5227)  Acc@5: 87.5000 (86.3636)  time: 0.5067  data: 0.0722  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1259 (2.0558)  Acc@1: 62.5000 (57.1429)  Acc@5: 81.2500 (82.1429)  time: 0.4445  data: 0.0058  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1259 (2.0700)  Acc@1: 62.5000 (58.4677)  Acc@5: 81.2500 (84.0726)  time: 0.4420  data: 0.0009  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1323 (2.0768)  Acc@1: 56.2500 (58.6806)  Acc@5: 87.5000 (84.0278)  time: 0.4420  data: 0.0004  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4644 s / it)\n",
            "* Acc@1 58.681 Acc@5 84.028 loss 2.077\n",
            "[Average accuracy till task4]\tAcc@1: 59.5195\tAcc@5: 82.8582\tLoss: 2.1056\tForgetting: 7.6103\tBackward: -7.6103\n",
            "Train: Epoch[1/5]  [  0/176]  eta: 0:04:19  Lr: 0.001875  Loss: 2.8782  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.5000)  time: 1.4725  data: 0.7866  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/176]  eta: 0:02:04  Lr: 0.001875  Loss: 2.6571  Acc@1: 18.7500 (22.7273)  Acc@5: 43.7500 (43.1818)  time: 0.7494  data: 0.0726  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/176]  eta: 0:01:51  Lr: 0.001875  Loss: 2.5643  Acc@1: 25.0000 (28.5714)  Acc@5: 56.2500 (54.4643)  time: 0.6798  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/176]  eta: 0:01:43  Lr: 0.001875  Loss: 2.2276  Acc@1: 37.5000 (33.0645)  Acc@5: 68.7500 (59.8790)  time: 0.6833  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/176]  eta: 0:01:35  Lr: 0.001875  Loss: 2.0178  Acc@1: 43.7500 (35.9756)  Acc@5: 68.7500 (63.4146)  time: 0.6854  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/176]  eta: 0:01:27  Lr: 0.001875  Loss: 2.2475  Acc@1: 43.7500 (38.3578)  Acc@5: 75.0000 (66.7892)  time: 0.6851  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/176]  eta: 0:01:20  Lr: 0.001875  Loss: 1.8080  Acc@1: 43.7500 (40.1639)  Acc@5: 81.2500 (69.2623)  time: 0.6832  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/176]  eta: 0:01:13  Lr: 0.001875  Loss: 1.8989  Acc@1: 43.7500 (41.5493)  Acc@5: 81.2500 (70.9507)  time: 0.6829  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/176]  eta: 0:01:06  Lr: 0.001875  Loss: 1.7278  Acc@1: 50.0000 (42.9784)  Acc@5: 81.2500 (71.7593)  time: 0.6828  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/176]  eta: 0:00:59  Lr: 0.001875  Loss: 1.6264  Acc@1: 56.2500 (44.0934)  Acc@5: 81.2500 (72.6648)  time: 0.6827  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/176]  eta: 0:00:52  Lr: 0.001875  Loss: 1.7852  Acc@1: 56.2500 (45.7921)  Acc@5: 87.5000 (74.1955)  time: 0.6827  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/176]  eta: 0:00:45  Lr: 0.001875  Loss: 1.8088  Acc@1: 62.5000 (47.0721)  Acc@5: 87.5000 (74.9437)  time: 0.6824  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/176]  eta: 0:00:38  Lr: 0.001875  Loss: 1.7114  Acc@1: 56.2500 (47.9339)  Acc@5: 87.5000 (75.7231)  time: 0.6815  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/176]  eta: 0:00:31  Lr: 0.001875  Loss: 1.3810  Acc@1: 56.2500 (48.7118)  Acc@5: 87.5000 (76.6698)  time: 0.6816  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[1/5]  [140/176]  eta: 0:00:24  Lr: 0.001875  Loss: 1.0739  Acc@1: 62.5000 (49.8227)  Acc@5: 87.5000 (77.4379)  time: 0.6829  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[1/5]  [150/176]  eta: 0:00:17  Lr: 0.001875  Loss: 1.2744  Acc@1: 62.5000 (50.6623)  Acc@5: 87.5000 (77.8560)  time: 0.6829  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [160/176]  eta: 0:00:11  Lr: 0.001875  Loss: 1.7005  Acc@1: 56.2500 (51.2422)  Acc@5: 87.5000 (78.2997)  time: 0.6829  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[1/5]  [170/176]  eta: 0:00:04  Lr: 0.001875  Loss: 1.4004  Acc@1: 56.2500 (51.8275)  Acc@5: 87.5000 (78.6915)  time: 0.6825  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[1/5]  [175/176]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3776  Acc@1: 62.5000 (52.1227)  Acc@5: 87.5000 (78.8798)  time: 0.6569  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:02:00 (0.6846 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3776  Acc@1: 62.5000 (52.1227)  Acc@5: 87.5000 (78.8798)\n",
            "Train: Epoch[2/5]  [  0/176]  eta: 0:03:02  Lr: 0.001875  Loss: 1.9443  Acc@1: 37.5000 (37.5000)  Acc@5: 62.5000 (62.5000)  time: 1.0351  data: 0.4043  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/176]  eta: 0:01:58  Lr: 0.001875  Loss: 1.0629  Acc@1: 56.2500 (60.7955)  Acc@5: 87.5000 (85.7955)  time: 0.7137  data: 0.0370  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/176]  eta: 0:01:48  Lr: 0.001875  Loss: 1.4033  Acc@1: 56.2500 (61.0119)  Acc@5: 87.5000 (86.0119)  time: 0.6809  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/176]  eta: 0:01:40  Lr: 0.001875  Loss: 1.3440  Acc@1: 56.2500 (61.4919)  Acc@5: 87.5000 (87.0968)  time: 0.6792  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/176]  eta: 0:01:33  Lr: 0.001875  Loss: 1.3509  Acc@1: 62.5000 (62.6524)  Acc@5: 93.7500 (88.8720)  time: 0.6786  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/176]  eta: 0:01:26  Lr: 0.001875  Loss: 1.0785  Acc@1: 68.7500 (62.3775)  Acc@5: 93.7500 (88.2353)  time: 0.6792  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/176]  eta: 0:01:19  Lr: 0.001875  Loss: 1.5036  Acc@1: 68.7500 (63.4221)  Acc@5: 87.5000 (88.9344)  time: 0.6789  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/176]  eta: 0:01:12  Lr: 0.001875  Loss: 1.0932  Acc@1: 68.7500 (64.4366)  Acc@5: 93.7500 (89.2606)  time: 0.6788  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/176]  eta: 0:01:05  Lr: 0.001875  Loss: 1.1877  Acc@1: 68.7500 (64.8148)  Acc@5: 93.7500 (89.4290)  time: 0.6787  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/176]  eta: 0:00:58  Lr: 0.001875  Loss: 0.7150  Acc@1: 68.7500 (65.1786)  Acc@5: 87.5000 (89.0797)  time: 0.6783  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/176]  eta: 0:00:51  Lr: 0.001875  Loss: 1.1088  Acc@1: 62.5000 (64.7896)  Acc@5: 87.5000 (89.0470)  time: 0.6787  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/176]  eta: 0:00:45  Lr: 0.001875  Loss: 1.2296  Acc@1: 62.5000 (64.4707)  Acc@5: 87.5000 (88.6261)  time: 0.6798  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/176]  eta: 0:00:38  Lr: 0.001875  Loss: 1.5074  Acc@1: 68.7500 (64.8244)  Acc@5: 87.5000 (88.6364)  time: 0.6798  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/176]  eta: 0:00:31  Lr: 0.001875  Loss: 0.8886  Acc@1: 68.7500 (64.9809)  Acc@5: 93.7500 (89.1698)  time: 0.6790  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [140/176]  eta: 0:00:24  Lr: 0.001875  Loss: 0.9953  Acc@1: 68.7500 (65.2926)  Acc@5: 93.7500 (89.4947)  time: 0.6788  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [150/176]  eta: 0:00:17  Lr: 0.001875  Loss: 0.5635  Acc@1: 68.7500 (65.6457)  Acc@5: 87.5000 (89.4868)  time: 0.6784  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[2/5]  [160/176]  eta: 0:00:10  Lr: 0.001875  Loss: 1.6278  Acc@1: 68.7500 (65.9161)  Acc@5: 87.5000 (89.5963)  time: 0.6784  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[2/5]  [170/176]  eta: 0:00:04  Lr: 0.001875  Loss: 0.7432  Acc@1: 68.7500 (65.8626)  Acc@5: 87.5000 (89.5102)  time: 0.6787  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[2/5]  [175/176]  eta: 0:00:00  Lr: 0.001875  Loss: 0.4637  Acc@1: 68.7500 (65.8580)  Acc@5: 87.5000 (89.4399)  time: 0.6515  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:59 (0.6784 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.4637  Acc@1: 68.7500 (65.8580)  Acc@5: 87.5000 (89.4399)\n",
            "Train: Epoch[3/5]  [  0/176]  eta: 0:03:27  Lr: 0.001875  Loss: 1.0369  Acc@1: 68.7500 (68.7500)  Acc@5: 93.7500 (93.7500)  time: 1.1768  data: 0.5498  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/176]  eta: 0:02:00  Lr: 0.001875  Loss: 0.4737  Acc@1: 81.2500 (78.9773)  Acc@5: 93.7500 (95.4545)  time: 0.7239  data: 0.0504  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/176]  eta: 0:01:49  Lr: 0.001875  Loss: 1.3474  Acc@1: 75.0000 (73.8095)  Acc@5: 93.7500 (94.6429)  time: 0.6787  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/176]  eta: 0:01:41  Lr: 0.001875  Loss: 1.3228  Acc@1: 62.5000 (69.5565)  Acc@5: 93.7500 (93.3468)  time: 0.6787  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/176]  eta: 0:01:33  Lr: 0.001875  Loss: 0.9607  Acc@1: 62.5000 (68.7500)  Acc@5: 93.7500 (93.4451)  time: 0.6782  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/176]  eta: 0:01:26  Lr: 0.001875  Loss: 1.2534  Acc@1: 62.5000 (68.5049)  Acc@5: 93.7500 (93.0147)  time: 0.6773  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/176]  eta: 0:01:19  Lr: 0.001875  Loss: 0.2442  Acc@1: 68.7500 (69.2623)  Acc@5: 93.7500 (92.5205)  time: 0.6774  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/176]  eta: 0:01:12  Lr: 0.001875  Loss: 0.9890  Acc@1: 68.7500 (69.5423)  Acc@5: 93.7500 (92.6056)  time: 0.6781  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/176]  eta: 0:01:05  Lr: 0.001875  Loss: 1.0109  Acc@1: 68.7500 (69.4444)  Acc@5: 93.7500 (92.3611)  time: 0.6782  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/176]  eta: 0:00:58  Lr: 0.001875  Loss: 1.0139  Acc@1: 68.7500 (69.1621)  Acc@5: 93.7500 (92.4451)  time: 0.6798  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/176]  eta: 0:00:51  Lr: 0.001875  Loss: 0.8638  Acc@1: 62.5000 (69.0594)  Acc@5: 93.7500 (91.8317)  time: 0.6821  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/176]  eta: 0:00:45  Lr: 0.001875  Loss: 0.7760  Acc@1: 68.7500 (69.4257)  Acc@5: 93.7500 (91.8356)  time: 0.6834  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/176]  eta: 0:00:38  Lr: 0.001875  Loss: 1.4565  Acc@1: 75.0000 (69.1632)  Acc@5: 93.7500 (91.8905)  time: 0.6845  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/176]  eta: 0:00:31  Lr: 0.001875  Loss: 1.0322  Acc@1: 68.7500 (69.5134)  Acc@5: 87.5000 (91.7939)  time: 0.6846  data: 0.0038  max mem: 2378\n",
            "Train: Epoch[3/5]  [140/176]  eta: 0:00:24  Lr: 0.001875  Loss: 1.4692  Acc@1: 75.0000 (69.3262)  Acc@5: 87.5000 (91.7553)  time: 0.6827  data: 0.0037  max mem: 2378\n",
            "Train: Epoch[3/5]  [150/176]  eta: 0:00:17  Lr: 0.001875  Loss: 1.1676  Acc@1: 68.7500 (69.4536)  Acc@5: 93.7500 (91.7219)  time: 0.6815  data: 0.0032  max mem: 2378\n",
            "Train: Epoch[3/5]  [160/176]  eta: 0:00:10  Lr: 0.001875  Loss: 0.6000  Acc@1: 68.7500 (69.5264)  Acc@5: 93.7500 (91.6537)  time: 0.6814  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [170/176]  eta: 0:00:04  Lr: 0.001875  Loss: 0.3410  Acc@1: 75.0000 (69.6637)  Acc@5: 93.7500 (91.7398)  time: 0.6811  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[3/5]  [175/176]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5077  Acc@1: 68.7500 (69.6397)  Acc@5: 93.7500 (91.7232)  time: 0.6535  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:59 (0.6806 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5077  Acc@1: 68.7500 (69.6397)  Acc@5: 93.7500 (91.7232)\n",
            "Train: Epoch[4/5]  [  0/176]  eta: 0:02:30  Lr: 0.001875  Loss: 1.3118  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.8523  data: 0.2186  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/176]  eta: 0:01:55  Lr: 0.001875  Loss: 0.6871  Acc@1: 75.0000 (76.1364)  Acc@5: 93.7500 (92.6136)  time: 0.6955  data: 0.0218  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/176]  eta: 0:01:47  Lr: 0.001875  Loss: 0.7959  Acc@1: 75.0000 (74.7024)  Acc@5: 93.7500 (93.1548)  time: 0.6792  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/176]  eta: 0:01:39  Lr: 0.001875  Loss: 0.8384  Acc@1: 68.7500 (72.5806)  Acc@5: 93.7500 (92.3387)  time: 0.6782  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/176]  eta: 0:01:32  Lr: 0.001875  Loss: 0.9688  Acc@1: 68.7500 (71.0366)  Acc@5: 93.7500 (92.2256)  time: 0.6770  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/176]  eta: 0:01:25  Lr: 0.001875  Loss: 1.4520  Acc@1: 68.7500 (71.2010)  Acc@5: 93.7500 (92.6471)  time: 0.6762  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/176]  eta: 0:01:18  Lr: 0.001875  Loss: 0.7591  Acc@1: 75.0000 (71.5164)  Acc@5: 93.7500 (92.8279)  time: 0.6759  data: 0.0025  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/176]  eta: 0:01:12  Lr: 0.001875  Loss: 0.5232  Acc@1: 68.7500 (71.2148)  Acc@5: 93.7500 (92.5176)  time: 0.6764  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/176]  eta: 0:01:05  Lr: 0.001875  Loss: 0.8921  Acc@1: 68.7500 (70.5247)  Acc@5: 93.7500 (92.9012)  time: 0.6772  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/176]  eta: 0:00:58  Lr: 0.001875  Loss: 1.0724  Acc@1: 68.7500 (70.0549)  Acc@5: 93.7500 (92.6511)  time: 0.6779  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/176]  eta: 0:00:51  Lr: 0.001875  Loss: 0.7138  Acc@1: 75.0000 (70.6683)  Acc@5: 93.7500 (92.7599)  time: 0.6781  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/176]  eta: 0:00:44  Lr: 0.001875  Loss: 0.7259  Acc@1: 75.0000 (70.6644)  Acc@5: 93.7500 (92.8491)  time: 0.6786  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/176]  eta: 0:00:38  Lr: 0.001875  Loss: 1.2788  Acc@1: 68.7500 (70.4029)  Acc@5: 93.7500 (92.6653)  time: 0.6793  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/176]  eta: 0:00:31  Lr: 0.001875  Loss: 0.8147  Acc@1: 68.7500 (70.1336)  Acc@5: 87.5000 (92.3664)  time: 0.6797  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [140/176]  eta: 0:00:24  Lr: 0.001875  Loss: 0.9705  Acc@1: 68.7500 (70.2128)  Acc@5: 87.5000 (92.3759)  time: 0.6799  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [150/176]  eta: 0:00:17  Lr: 0.001875  Loss: 0.7300  Acc@1: 68.7500 (69.9089)  Acc@5: 87.5000 (92.2599)  time: 0.6798  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [160/176]  eta: 0:00:10  Lr: 0.001875  Loss: 1.1569  Acc@1: 68.7500 (70.2252)  Acc@5: 93.7500 (92.2748)  time: 0.6803  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[4/5]  [170/176]  eta: 0:00:04  Lr: 0.001875  Loss: 0.8064  Acc@1: 75.0000 (70.3947)  Acc@5: 93.7500 (92.2880)  time: 0.6800  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[4/5]  [175/176]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6999  Acc@1: 68.7500 (70.1748)  Acc@5: 93.7500 (92.2940)  time: 0.6531  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:59 (0.6770 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6999  Acc@1: 68.7500 (70.1748)  Acc@5: 93.7500 (92.2940)\n",
            "Train: Epoch[5/5]  [  0/176]  eta: 0:03:47  Lr: 0.001875  Loss: 0.7777  Acc@1: 68.7500 (68.7500)  Acc@5: 100.0000 (100.0000)  time: 1.2902  data: 0.6356  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/176]  eta: 0:02:02  Lr: 0.001875  Loss: 0.9006  Acc@1: 68.7500 (73.8636)  Acc@5: 100.0000 (96.5909)  time: 0.7353  data: 0.0609  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/176]  eta: 0:01:50  Lr: 0.001875  Loss: 0.7014  Acc@1: 68.7500 (72.6190)  Acc@5: 100.0000 (95.5357)  time: 0.6809  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/176]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3196  Acc@1: 75.0000 (74.1935)  Acc@5: 93.7500 (95.1613)  time: 0.6811  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/176]  eta: 0:01:34  Lr: 0.001875  Loss: 0.8450  Acc@1: 75.0000 (74.3902)  Acc@5: 93.7500 (95.2744)  time: 0.6815  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/176]  eta: 0:01:27  Lr: 0.001875  Loss: 0.7423  Acc@1: 68.7500 (72.9167)  Acc@5: 93.7500 (94.6078)  time: 0.6832  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/176]  eta: 0:01:20  Lr: 0.001875  Loss: 0.7085  Acc@1: 75.0000 (73.7705)  Acc@5: 93.7500 (94.3648)  time: 0.6830  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/176]  eta: 0:01:13  Lr: 0.001875  Loss: 0.4514  Acc@1: 75.0000 (73.7676)  Acc@5: 93.7500 (94.3662)  time: 0.6836  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/176]  eta: 0:01:06  Lr: 0.001875  Loss: 0.6614  Acc@1: 75.0000 (73.4568)  Acc@5: 93.7500 (94.0586)  time: 0.6853  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/176]  eta: 0:00:59  Lr: 0.001875  Loss: 1.4233  Acc@1: 75.0000 (73.5577)  Acc@5: 93.7500 (94.1621)  time: 0.6847  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/176]  eta: 0:00:52  Lr: 0.001875  Loss: 0.3965  Acc@1: 75.0000 (73.6386)  Acc@5: 93.7500 (94.1832)  time: 0.6833  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/176]  eta: 0:00:45  Lr: 0.001875  Loss: 0.9015  Acc@1: 68.7500 (73.2545)  Acc@5: 93.7500 (94.3131)  time: 0.6837  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/176]  eta: 0:00:38  Lr: 0.001875  Loss: 0.4120  Acc@1: 68.7500 (73.0372)  Acc@5: 93.7500 (94.1116)  time: 0.6836  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/176]  eta: 0:00:31  Lr: 0.001875  Loss: 0.9719  Acc@1: 68.7500 (72.6145)  Acc@5: 93.7500 (94.0363)  time: 0.6833  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[5/5]  [140/176]  eta: 0:00:24  Lr: 0.001875  Loss: 0.5861  Acc@1: 68.7500 (72.2074)  Acc@5: 93.7500 (93.7943)  time: 0.6843  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[5/5]  [150/176]  eta: 0:00:17  Lr: 0.001875  Loss: 0.5482  Acc@1: 68.7500 (72.3924)  Acc@5: 93.7500 (93.8328)  time: 0.6839  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[5/5]  [160/176]  eta: 0:00:10  Lr: 0.001875  Loss: 0.6628  Acc@1: 75.0000 (72.5932)  Acc@5: 93.7500 (93.9829)  time: 0.6830  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[5/5]  [170/176]  eta: 0:00:04  Lr: 0.001875  Loss: 0.6369  Acc@1: 75.0000 (72.7339)  Acc@5: 93.7500 (94.0424)  time: 0.6830  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [175/176]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3859  Acc@1: 75.0000 (72.8505)  Acc@5: 93.7500 (94.0064)  time: 0.6560  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:02:00 (0.6840 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3859  Acc@1: 75.0000 (72.8505)  Acc@5: 93.7500 (94.0064)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:34  Loss: 2.5753 (2.5753)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.8440  data: 0.4350  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.4745 (2.4950)  Acc@1: 50.0000 (51.1364)  Acc@5: 75.0000 (76.1364)  time: 0.4796  data: 0.0399  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9920 (2.1204)  Acc@1: 56.2500 (59.2262)  Acc@5: 81.2500 (81.2500)  time: 0.4436  data: 0.0008  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7868 (2.1373)  Acc@1: 62.5000 (58.4677)  Acc@5: 87.5000 (80.4435)  time: 0.4432  data: 0.0021  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0508 (2.1310)  Acc@1: 56.2500 (59.3846)  Acc@5: 81.2500 (80.1538)  time: 0.4358  data: 0.0016  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4509 s / it)\n",
            "* Acc@1 59.385 Acc@5 80.154 loss 2.131\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:35  Loss: 1.1189 (1.1189)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.7501  data: 0.3405  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1189 (1.3813)  Acc@1: 81.2500 (76.7045)  Acc@5: 93.7500 (87.5000)  time: 0.4730  data: 0.0363  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.7779 (1.9240)  Acc@1: 68.7500 (64.8810)  Acc@5: 87.5000 (81.2500)  time: 0.4442  data: 0.0048  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0319 (1.9470)  Acc@1: 62.5000 (66.9355)  Acc@5: 87.5000 (82.6613)  time: 0.4434  data: 0.0028  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1133 (2.0751)  Acc@1: 62.5000 (63.5671)  Acc@5: 87.5000 (80.7927)  time: 0.4433  data: 0.0012  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1742 (2.0058)  Acc@1: 56.2500 (64.5333)  Acc@5: 75.0000 (80.9333)  time: 0.4411  data: 0.0004  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4514 s / it)\n",
            "* Acc@1 64.533 Acc@5 80.933 loss 2.006\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:29  Loss: 2.6615 (2.6615)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 1.0916  data: 0.6807  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.7814 (1.8744)  Acc@1: 62.5000 (59.6591)  Acc@5: 87.5000 (88.6364)  time: 0.5451  data: 0.1123  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7874 (1.9149)  Acc@1: 62.5000 (58.9286)  Acc@5: 87.5000 (86.0119)  time: 0.4658  data: 0.0279  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8718 (2.0098)  Acc@1: 56.2500 (55.5035)  Acc@5: 87.5000 (84.7775)  time: 0.4354  data: 0.0004  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4809 s / it)\n",
            "* Acc@1 55.504 Acc@5 84.778 loss 2.010\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:33  Loss: 2.7613 (2.7613)  Acc@1: 37.5000 (37.5000)  Acc@5: 62.5000 (62.5000)  time: 0.9374  data: 0.5065  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.3557 (2.0714)  Acc@1: 56.2500 (56.2500)  Acc@5: 81.2500 (78.4091)  time: 0.4870  data: 0.0513  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1041 (2.1907)  Acc@1: 50.0000 (52.9762)  Acc@5: 81.2500 (77.6786)  time: 0.4426  data: 0.0038  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1041 (2.1482)  Acc@1: 56.2500 (55.4435)  Acc@5: 81.2500 (80.8468)  time: 0.4431  data: 0.0014  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1041 (2.1509)  Acc@1: 56.2500 (55.9028)  Acc@5: 87.5000 (81.0764)  time: 0.4429  data: 0.0006  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4591 s / it)\n",
            "* Acc@1 55.903 Acc@5 81.076 loss 2.151\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:50  Loss: 1.3606 (1.3606)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 1.1311  data: 0.7157  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:17  Loss: 1.8693 (2.0054)  Acc@1: 75.0000 (70.4545)  Acc@5: 81.2500 (86.3636)  time: 0.5036  data: 0.0677  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 1.9922 (1.9592)  Acc@1: 68.7500 (67.8571)  Acc@5: 81.2500 (85.1190)  time: 0.4422  data: 0.0019  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.0383 (2.0809)  Acc@1: 62.5000 (63.9113)  Acc@5: 75.0000 (80.8468)  time: 0.4436  data: 0.0007  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4369 (2.0745)  Acc@1: 56.2500 (62.8049)  Acc@5: 75.0000 (80.7927)  time: 0.4434  data: 0.0016  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.0333 (2.0381)  Acc@1: 62.5000 (63.6872)  Acc@5: 75.0000 (80.8659)  time: 0.4416  data: 0.0016  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4586 s / it)\n",
            "* Acc@1 63.687 Acc@5 80.866 loss 2.038\n",
            "[Average accuracy till task5]\tAcc@1: 59.8023\tAcc@5: 81.5614\tLoss: 2.0671\tForgetting: 6.3962\tBackward: -6.3962\n",
            "Train: Epoch[1/5]  [  0/139]  eta: 0:02:29  Lr: 0.001875  Loss: 2.8609  Acc@1: 0.0000 (0.0000)  Acc@5: 25.0000 (25.0000)  time: 1.0779  data: 0.4464  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/139]  eta: 0:01:32  Lr: 0.001875  Loss: 2.5385  Acc@1: 18.7500 (22.7273)  Acc@5: 62.5000 (58.5227)  time: 0.7186  data: 0.0408  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/139]  eta: 0:01:23  Lr: 0.001875  Loss: 2.5206  Acc@1: 25.0000 (25.0000)  Acc@5: 56.2500 (56.5476)  time: 0.6830  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/139]  eta: 0:01:15  Lr: 0.001875  Loss: 2.2200  Acc@1: 37.5000 (29.6371)  Acc@5: 56.2500 (60.8871)  time: 0.6845  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/139]  eta: 0:01:08  Lr: 0.001875  Loss: 2.0527  Acc@1: 37.5000 (32.6220)  Acc@5: 75.0000 (64.3293)  time: 0.6843  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/139]  eta: 0:01:01  Lr: 0.001875  Loss: 2.0016  Acc@1: 50.0000 (37.5000)  Acc@5: 75.0000 (67.2794)  time: 0.6839  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/139]  eta: 0:00:54  Lr: 0.001875  Loss: 1.9273  Acc@1: 56.2500 (39.6516)  Acc@5: 81.2500 (68.7500)  time: 0.6837  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/139]  eta: 0:00:47  Lr: 0.001875  Loss: 1.9859  Acc@1: 50.0000 (40.6690)  Acc@5: 75.0000 (70.3345)  time: 0.6819  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/139]  eta: 0:00:40  Lr: 0.001875  Loss: 1.5155  Acc@1: 50.0000 (42.9784)  Acc@5: 81.2500 (71.8364)  time: 0.6810  data: 0.0030  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/139]  eta: 0:00:33  Lr: 0.001875  Loss: 1.3099  Acc@1: 56.2500 (44.8489)  Acc@5: 81.2500 (73.5577)  time: 0.6803  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/139]  eta: 0:00:26  Lr: 0.001875  Loss: 1.6088  Acc@1: 56.2500 (46.1015)  Acc@5: 87.5000 (74.5050)  time: 0.6791  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/139]  eta: 0:00:19  Lr: 0.001875  Loss: 1.4242  Acc@1: 62.5000 (48.0293)  Acc@5: 87.5000 (76.0135)  time: 0.6788  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/139]  eta: 0:00:13  Lr: 0.001875  Loss: 1.5509  Acc@1: 62.5000 (49.2252)  Acc@5: 81.2500 (76.4979)  time: 0.6780  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/139]  eta: 0:00:06  Lr: 0.001875  Loss: 2.0897  Acc@1: 62.5000 (49.8569)  Acc@5: 81.2500 (76.6698)  time: 0.6781  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[1/5]  [138/139]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3459  Acc@1: 62.5000 (50.4973)  Acc@5: 81.2500 (77.1700)  time: 0.6538  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:34 (0.6813 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3459  Acc@1: 62.5000 (50.4973)  Acc@5: 81.2500 (77.1700)\n",
            "Train: Epoch[2/5]  [  0/139]  eta: 0:02:53  Lr: 0.001875  Loss: 1.3419  Acc@1: 68.7500 (68.7500)  Acc@5: 75.0000 (75.0000)  time: 1.2506  data: 0.6063  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/139]  eta: 0:01:33  Lr: 0.001875  Loss: 1.4072  Acc@1: 62.5000 (65.9091)  Acc@5: 87.5000 (86.9318)  time: 0.7280  data: 0.0562  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/139]  eta: 0:01:23  Lr: 0.001875  Loss: 1.4183  Acc@1: 62.5000 (66.6667)  Acc@5: 87.5000 (88.0952)  time: 0.6760  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/139]  eta: 0:01:15  Lr: 0.001875  Loss: 1.3425  Acc@1: 62.5000 (65.5242)  Acc@5: 93.7500 (88.7097)  time: 0.6766  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/139]  eta: 0:01:08  Lr: 0.001875  Loss: 1.8495  Acc@1: 62.5000 (66.0061)  Acc@5: 93.7500 (88.8720)  time: 0.6775  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/139]  eta: 0:01:01  Lr: 0.001875  Loss: 1.3619  Acc@1: 62.5000 (66.2990)  Acc@5: 87.5000 (88.9706)  time: 0.6777  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/139]  eta: 0:00:54  Lr: 0.001875  Loss: 1.3765  Acc@1: 68.7500 (66.3934)  Acc@5: 87.5000 (89.0369)  time: 0.6776  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/139]  eta: 0:00:47  Lr: 0.001875  Loss: 1.7010  Acc@1: 68.7500 (65.4930)  Acc@5: 87.5000 (88.4683)  time: 0.6786  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/139]  eta: 0:00:40  Lr: 0.001875  Loss: 0.9982  Acc@1: 56.2500 (64.5833)  Acc@5: 87.5000 (88.2716)  time: 0.6791  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/139]  eta: 0:00:33  Lr: 0.001875  Loss: 0.6622  Acc@1: 62.5000 (65.1099)  Acc@5: 87.5000 (88.3929)  time: 0.6793  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/139]  eta: 0:00:26  Lr: 0.001875  Loss: 1.1139  Acc@1: 68.7500 (64.9752)  Acc@5: 87.5000 (88.4282)  time: 0.6797  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/139]  eta: 0:00:19  Lr: 0.001875  Loss: 0.8932  Acc@1: 68.7500 (65.7095)  Acc@5: 87.5000 (88.7950)  time: 0.6796  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/139]  eta: 0:00:12  Lr: 0.001875  Loss: 1.3243  Acc@1: 68.7500 (65.4442)  Acc@5: 87.5000 (88.5331)  time: 0.6793  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/139]  eta: 0:00:06  Lr: 0.001875  Loss: 0.9216  Acc@1: 68.7500 (65.6011)  Acc@5: 87.5000 (88.5019)  time: 0.6790  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [138/139]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1871  Acc@1: 68.7500 (65.7324)  Acc@5: 87.5000 (88.4268)  time: 0.6537  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:34 (0.6793 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1871  Acc@1: 68.7500 (65.7324)  Acc@5: 87.5000 (88.4268)\n",
            "Train: Epoch[3/5]  [  0/139]  eta: 0:02:29  Lr: 0.001875  Loss: 1.0806  Acc@1: 68.7500 (68.7500)  Acc@5: 93.7500 (93.7500)  time: 1.0736  data: 0.4438  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/139]  eta: 0:01:32  Lr: 0.001875  Loss: 1.0395  Acc@1: 68.7500 (62.5000)  Acc@5: 87.5000 (89.7727)  time: 0.7167  data: 0.0415  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/139]  eta: 0:01:23  Lr: 0.001875  Loss: 0.7511  Acc@1: 68.7500 (65.7738)  Acc@5: 87.5000 (89.5833)  time: 0.6801  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/139]  eta: 0:01:15  Lr: 0.001875  Loss: 0.8690  Acc@1: 68.7500 (67.9435)  Acc@5: 87.5000 (90.1210)  time: 0.6795  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/139]  eta: 0:01:08  Lr: 0.001875  Loss: 1.2189  Acc@1: 68.7500 (67.5305)  Acc@5: 93.7500 (90.0915)  time: 0.6796  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/139]  eta: 0:01:01  Lr: 0.001875  Loss: 1.1258  Acc@1: 68.7500 (67.7696)  Acc@5: 93.7500 (90.6863)  time: 0.6796  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/139]  eta: 0:00:54  Lr: 0.001875  Loss: 1.1248  Acc@1: 68.7500 (67.4180)  Acc@5: 93.7500 (90.4713)  time: 0.6794  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/139]  eta: 0:00:47  Lr: 0.001875  Loss: 0.3913  Acc@1: 68.7500 (69.0141)  Acc@5: 93.7500 (91.0211)  time: 0.6793  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/139]  eta: 0:00:40  Lr: 0.001875  Loss: 1.3138  Acc@1: 75.0000 (68.9815)  Acc@5: 93.7500 (90.5093)  time: 0.6794  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/139]  eta: 0:00:33  Lr: 0.001875  Loss: 0.6083  Acc@1: 68.7500 (69.2995)  Acc@5: 87.5000 (90.4533)  time: 0.6794  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/139]  eta: 0:00:26  Lr: 0.001875  Loss: 1.1108  Acc@1: 68.7500 (68.8119)  Acc@5: 87.5000 (90.4084)  time: 0.6784  data: 0.0025  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/139]  eta: 0:00:19  Lr: 0.001875  Loss: 0.9093  Acc@1: 68.7500 (69.0315)  Acc@5: 87.5000 (90.4842)  time: 0.6774  data: 0.0042  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/139]  eta: 0:00:12  Lr: 0.001875  Loss: 1.4054  Acc@1: 75.0000 (69.1632)  Acc@5: 87.5000 (90.3926)  time: 0.6789  data: 0.0046  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/139]  eta: 0:00:06  Lr: 0.001875  Loss: 1.1135  Acc@1: 62.5000 (69.0363)  Acc@5: 93.7500 (90.4103)  time: 0.6794  data: 0.0029  max mem: 2378\n",
            "Train: Epoch[3/5]  [138/139]  eta: 0:00:00  Lr: 0.001875  Loss: 2.2411  Acc@1: 62.5000 (68.8517)  Acc@5: 93.7500 (90.3255)  time: 0.6545  data: 0.0002  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:34 (0.6793 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 2.2411  Acc@1: 62.5000 (68.8517)  Acc@5: 93.7500 (90.3255)\n",
            "Train: Epoch[4/5]  [  0/139]  eta: 0:03:07  Lr: 0.001875  Loss: 0.8305  Acc@1: 68.7500 (68.7500)  Acc@5: 100.0000 (100.0000)  time: 1.3456  data: 0.7035  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/139]  eta: 0:01:35  Lr: 0.001875  Loss: 1.4892  Acc@1: 75.0000 (71.5909)  Acc@5: 93.7500 (90.3409)  time: 0.7383  data: 0.0651  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/139]  eta: 0:01:24  Lr: 0.001875  Loss: 1.0380  Acc@1: 75.0000 (72.0238)  Acc@5: 93.7500 (90.7738)  time: 0.6780  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/139]  eta: 0:01:16  Lr: 0.001875  Loss: 0.5159  Acc@1: 68.7500 (70.1613)  Acc@5: 93.7500 (91.9355)  time: 0.6784  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/139]  eta: 0:01:08  Lr: 0.001875  Loss: 0.8150  Acc@1: 68.7500 (69.0549)  Acc@5: 93.7500 (92.2256)  time: 0.6792  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/139]  eta: 0:01:01  Lr: 0.001875  Loss: 0.9701  Acc@1: 68.7500 (68.8725)  Acc@5: 93.7500 (92.4020)  time: 0.6798  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/139]  eta: 0:00:54  Lr: 0.001875  Loss: 1.0019  Acc@1: 68.7500 (69.5697)  Acc@5: 93.7500 (92.9303)  time: 0.6803  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/139]  eta: 0:00:47  Lr: 0.001875  Loss: 1.1328  Acc@1: 75.0000 (69.3662)  Acc@5: 93.7500 (92.8697)  time: 0.6801  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/139]  eta: 0:00:40  Lr: 0.001875  Loss: 0.9584  Acc@1: 68.7500 (69.2130)  Acc@5: 93.7500 (92.9012)  time: 0.6794  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/139]  eta: 0:00:33  Lr: 0.001875  Loss: 1.1587  Acc@1: 62.5000 (68.6813)  Acc@5: 93.7500 (92.4451)  time: 0.6796  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/139]  eta: 0:00:26  Lr: 0.001875  Loss: 0.8446  Acc@1: 68.7500 (68.9356)  Acc@5: 87.5000 (92.0792)  time: 0.6799  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/139]  eta: 0:00:19  Lr: 0.001875  Loss: 0.9500  Acc@1: 68.7500 (68.6374)  Acc@5: 93.7500 (91.8919)  time: 0.6798  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/139]  eta: 0:00:13  Lr: 0.001875  Loss: 1.4422  Acc@1: 68.7500 (68.6983)  Acc@5: 93.7500 (91.6839)  time: 0.6794  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/139]  eta: 0:00:06  Lr: 0.001875  Loss: 0.6863  Acc@1: 75.0000 (69.3225)  Acc@5: 93.7500 (91.8416)  time: 0.6797  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[4/5]  [138/139]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5447  Acc@1: 75.0000 (69.6203)  Acc@5: 93.7500 (91.9530)  time: 0.6554  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:34 (0.6812 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5447  Acc@1: 75.0000 (69.6203)  Acc@5: 93.7500 (91.9530)\n",
            "Train: Epoch[5/5]  [  0/139]  eta: 0:02:32  Lr: 0.001875  Loss: 1.2935  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 1.0940  data: 0.4613  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/139]  eta: 0:01:32  Lr: 0.001875  Loss: 0.5913  Acc@1: 75.0000 (75.5682)  Acc@5: 93.7500 (91.4773)  time: 0.7164  data: 0.0438  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/139]  eta: 0:01:23  Lr: 0.001875  Loss: 0.9278  Acc@1: 75.0000 (75.5952)  Acc@5: 93.7500 (93.4524)  time: 0.6789  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/139]  eta: 0:01:15  Lr: 0.001875  Loss: 1.0181  Acc@1: 68.7500 (74.1935)  Acc@5: 93.7500 (93.5484)  time: 0.6786  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/139]  eta: 0:01:08  Lr: 0.001875  Loss: 0.5759  Acc@1: 68.7500 (73.7805)  Acc@5: 93.7500 (93.9024)  time: 0.6788  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/139]  eta: 0:01:01  Lr: 0.001875  Loss: 0.7100  Acc@1: 68.7500 (73.0392)  Acc@5: 93.7500 (93.2598)  time: 0.6801  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/139]  eta: 0:00:54  Lr: 0.001875  Loss: 0.8311  Acc@1: 75.0000 (73.1557)  Acc@5: 93.7500 (93.2377)  time: 0.6805  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/139]  eta: 0:00:47  Lr: 0.001875  Loss: 1.1232  Acc@1: 68.7500 (72.0951)  Acc@5: 93.7500 (93.1338)  time: 0.6798  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/139]  eta: 0:00:40  Lr: 0.001875  Loss: 0.3880  Acc@1: 68.7500 (72.2994)  Acc@5: 93.7500 (92.9012)  time: 0.6802  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/139]  eta: 0:00:33  Lr: 0.001875  Loss: 1.1242  Acc@1: 75.0000 (71.9780)  Acc@5: 93.7500 (92.8571)  time: 0.6802  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/139]  eta: 0:00:26  Lr: 0.001875  Loss: 0.6762  Acc@1: 75.0000 (72.5866)  Acc@5: 93.7500 (92.8218)  time: 0.6797  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/139]  eta: 0:00:19  Lr: 0.001875  Loss: 0.7778  Acc@1: 75.0000 (72.5225)  Acc@5: 93.7500 (92.5676)  time: 0.6801  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/139]  eta: 0:00:12  Lr: 0.001875  Loss: 1.0251  Acc@1: 68.7500 (72.3657)  Acc@5: 87.5000 (92.5620)  time: 0.6802  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/139]  eta: 0:00:06  Lr: 0.001875  Loss: 0.5390  Acc@1: 68.7500 (72.3760)  Acc@5: 93.7500 (92.5095)  time: 0.6800  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[5/5]  [138/139]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5738  Acc@1: 75.0000 (72.5588)  Acc@5: 93.7500 (92.5407)  time: 0.6549  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:34 (0.6796 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5738  Acc@1: 75.0000 (72.5588)  Acc@5: 93.7500 (92.5407)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:33  Loss: 2.4021 (2.4021)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 0.8065  data: 0.3919  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.4021 (2.4954)  Acc@1: 50.0000 (50.5682)  Acc@5: 75.0000 (74.4318)  time: 0.4733  data: 0.0372  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0439 (2.1049)  Acc@1: 56.2500 (60.1190)  Acc@5: 81.2500 (80.0595)  time: 0.4408  data: 0.0011  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.8216 (2.1127)  Acc@1: 62.5000 (58.6694)  Acc@5: 81.2500 (78.6290)  time: 0.4414  data: 0.0025  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0785 (2.1301)  Acc@1: 56.2500 (58.4615)  Acc@5: 75.0000 (78.4615)  time: 0.4337  data: 0.0029  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4477 s / it)\n",
            "* Acc@1 58.462 Acc@5 78.462 loss 2.130\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:31  Loss: 1.0775 (1.0775)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.6605  data: 0.2558  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.0775 (1.3438)  Acc@1: 81.2500 (77.2727)  Acc@5: 93.7500 (88.0682)  time: 0.4609  data: 0.0250  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.6870 (1.8954)  Acc@1: 68.7500 (65.7738)  Acc@5: 87.5000 (81.2500)  time: 0.4406  data: 0.0026  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.1157 (1.9681)  Acc@1: 62.5000 (65.7258)  Acc@5: 81.2500 (81.4516)  time: 0.4411  data: 0.0025  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2513 (2.0892)  Acc@1: 56.2500 (62.3476)  Acc@5: 81.2500 (79.8780)  time: 0.4414  data: 0.0012  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.2513 (2.0263)  Acc@1: 56.2500 (62.9333)  Acc@5: 75.0000 (79.8667)  time: 0.4387  data: 0.0005  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4469 s / it)\n",
            "* Acc@1 62.933 Acc@5 79.867 loss 2.026\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:29  Loss: 2.6889 (2.6889)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 1.0946  data: 0.6840  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.8613 (1.8697)  Acc@1: 62.5000 (57.3864)  Acc@5: 87.5000 (86.3636)  time: 0.5602  data: 0.1311  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7865 (1.8875)  Acc@1: 62.5000 (57.4405)  Acc@5: 87.5000 (85.4167)  time: 0.4719  data: 0.0382  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8034 (1.9873)  Acc@1: 56.2500 (55.0351)  Acc@5: 87.5000 (84.7775)  time: 0.4312  data: 0.0004  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:13 (0.4851 s / it)\n",
            "* Acc@1 55.035 Acc@5 84.778 loss 1.987\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:29  Loss: 2.5869 (2.5869)  Acc@1: 37.5000 (37.5000)  Acc@5: 68.7500 (68.7500)  time: 0.8226  data: 0.4168  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.2529 (2.0438)  Acc@1: 43.7500 (55.6818)  Acc@5: 81.2500 (79.5455)  time: 0.4740  data: 0.0412  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.0475 (2.1448)  Acc@1: 56.2500 (55.0595)  Acc@5: 81.2500 (77.6786)  time: 0.4403  data: 0.0034  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.0475 (2.1118)  Acc@1: 56.2500 (57.0565)  Acc@5: 81.2500 (80.2419)  time: 0.4416  data: 0.0017  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.0836 (2.1160)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (80.5556)  time: 0.4424  data: 0.0004  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4542 s / it)\n",
            "* Acc@1 56.250 Acc@5 80.556 loss 2.116\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:50  Loss: 1.5374 (1.5374)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 1.1249  data: 0.7228  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:17  Loss: 2.0608 (2.1210)  Acc@1: 68.7500 (63.0682)  Acc@5: 87.5000 (84.0909)  time: 0.5056  data: 0.0696  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.0608 (2.0592)  Acc@1: 62.5000 (63.0952)  Acc@5: 81.2500 (83.6310)  time: 0.4440  data: 0.0024  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.0868 (2.1094)  Acc@1: 62.5000 (60.8871)  Acc@5: 81.2500 (80.4435)  time: 0.4437  data: 0.0005  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4164 (2.1084)  Acc@1: 50.0000 (59.4512)  Acc@5: 75.0000 (80.0305)  time: 0.4446  data: 0.0010  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.0569 (2.0712)  Acc@1: 50.0000 (60.4749)  Acc@5: 75.0000 (80.3073)  time: 0.4402  data: 0.0009  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4603 s / it)\n",
            "* Acc@1 60.475 Acc@5 80.307 loss 2.071\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:42  Loss: 2.3000 (2.3000)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 1.2026  data: 0.7918  max mem: 2378\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:13  Loss: 2.3000 (2.0083)  Acc@1: 56.2500 (60.7955)  Acc@5: 87.5000 (84.0909)  time: 0.5217  data: 0.0856  max mem: 2378\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 1.9122 (2.0149)  Acc@1: 56.2500 (58.9286)  Acc@5: 87.5000 (83.9286)  time: 0.4487  data: 0.0084  max mem: 2378\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 1.8476 (1.8950)  Acc@1: 62.5000 (64.3145)  Acc@5: 87.5000 (84.2742)  time: 0.4437  data: 0.0019  max mem: 2378\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7301 (1.8804)  Acc@1: 75.0000 (65.4122)  Acc@5: 87.5000 (84.7670)  time: 0.4416  data: 0.0012  max mem: 2378\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4694 s / it)\n",
            "* Acc@1 65.412 Acc@5 84.767 loss 1.880\n",
            "[Average accuracy till task6]\tAcc@1: 59.7612\tAcc@5: 81.4559\tLoss: 2.0352\tForgetting: 6.2882\tBackward: -6.2882\n",
            "Train: Epoch[1/5]  [  0/143]  eta: 0:02:59  Lr: 0.001875  Loss: 2.8649  Acc@1: 6.2500 (6.2500)  Acc@5: 31.2500 (31.2500)  time: 1.2554  data: 0.6234  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/143]  eta: 0:01:37  Lr: 0.001875  Loss: 2.6801  Acc@1: 18.7500 (13.6364)  Acc@5: 50.0000 (47.1591)  time: 0.7323  data: 0.0570  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/143]  eta: 0:01:27  Lr: 0.001875  Loss: 2.5059  Acc@1: 18.7500 (20.5357)  Acc@5: 56.2500 (54.1667)  time: 0.6810  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/143]  eta: 0:01:19  Lr: 0.001875  Loss: 2.4511  Acc@1: 25.0000 (24.5968)  Acc@5: 68.7500 (58.0645)  time: 0.6830  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/143]  eta: 0:01:11  Lr: 0.001875  Loss: 2.1718  Acc@1: 37.5000 (27.8963)  Acc@5: 68.7500 (61.5854)  time: 0.6849  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/143]  eta: 0:01:04  Lr: 0.001875  Loss: 2.1083  Acc@1: 37.5000 (30.5147)  Acc@5: 75.0000 (63.8480)  time: 0.6850  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/143]  eta: 0:00:57  Lr: 0.001875  Loss: 2.1515  Acc@1: 37.5000 (32.8893)  Acc@5: 75.0000 (65.4713)  time: 0.6822  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/143]  eta: 0:00:50  Lr: 0.001875  Loss: 1.4228  Acc@1: 50.0000 (35.9155)  Acc@5: 75.0000 (67.8697)  time: 0.6810  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/143]  eta: 0:00:43  Lr: 0.001875  Loss: 2.1200  Acc@1: 50.0000 (37.4228)  Acc@5: 81.2500 (69.2901)  time: 0.6813  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/143]  eta: 0:00:36  Lr: 0.001875  Loss: 1.6812  Acc@1: 50.0000 (39.2170)  Acc@5: 81.2500 (70.8104)  time: 0.6798  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/143]  eta: 0:00:29  Lr: 0.001875  Loss: 2.4276  Acc@1: 50.0000 (40.4703)  Acc@5: 81.2500 (71.3490)  time: 0.6778  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/143]  eta: 0:00:22  Lr: 0.001875  Loss: 1.6162  Acc@1: 56.2500 (41.9482)  Acc@5: 81.2500 (72.0721)  time: 0.6764  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/143]  eta: 0:00:15  Lr: 0.001875  Loss: 1.5187  Acc@1: 56.2500 (43.1302)  Acc@5: 81.2500 (72.7789)  time: 0.6763  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/143]  eta: 0:00:08  Lr: 0.001875  Loss: 1.7922  Acc@1: 56.2500 (44.0840)  Acc@5: 81.2500 (73.4733)  time: 0.6762  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [140/143]  eta: 0:00:02  Lr: 0.001875  Loss: 1.6171  Acc@1: 62.5000 (45.2128)  Acc@5: 81.2500 (74.5124)  time: 0.6761  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5]  [142/143]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3639  Acc@1: 62.5000 (45.3392)  Acc@5: 81.2500 (74.6608)  time: 0.6729  data: 0.0002  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:37 (0.6843 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3639  Acc@1: 62.5000 (45.3392)  Acc@5: 81.2500 (74.6608)\n",
            "Train: Epoch[2/5]  [  0/143]  eta: 0:02:47  Lr: 0.001875  Loss: 1.2955  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 1.1747  data: 0.5291  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/143]  eta: 0:01:35  Lr: 0.001875  Loss: 1.6120  Acc@1: 68.7500 (64.2045)  Acc@5: 87.5000 (89.7727)  time: 0.7200  data: 0.0486  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/143]  eta: 0:01:26  Lr: 0.001875  Loss: 1.7485  Acc@1: 62.5000 (64.2857)  Acc@5: 87.5000 (89.5833)  time: 0.6767  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/143]  eta: 0:01:18  Lr: 0.001875  Loss: 1.5952  Acc@1: 62.5000 (64.1129)  Acc@5: 93.7500 (89.9194)  time: 0.6810  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/143]  eta: 0:01:11  Lr: 0.001875  Loss: 1.2472  Acc@1: 62.5000 (62.0427)  Acc@5: 93.7500 (89.4817)  time: 0.6832  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/143]  eta: 0:01:04  Lr: 0.001875  Loss: 1.5400  Acc@1: 62.5000 (62.7451)  Acc@5: 93.7500 (89.8284)  time: 0.6839  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/143]  eta: 0:00:57  Lr: 0.001875  Loss: 1.4077  Acc@1: 62.5000 (63.3197)  Acc@5: 93.7500 (90.1639)  time: 0.6830  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/143]  eta: 0:00:50  Lr: 0.001875  Loss: 1.1896  Acc@1: 68.7500 (63.2042)  Acc@5: 93.7500 (90.3169)  time: 0.6802  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/143]  eta: 0:00:43  Lr: 0.001875  Loss: 1.4095  Acc@1: 62.5000 (63.2716)  Acc@5: 87.5000 (89.8148)  time: 0.6785  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/143]  eta: 0:00:36  Lr: 0.001875  Loss: 1.5894  Acc@1: 56.2500 (62.9121)  Acc@5: 87.5000 (89.4918)  time: 0.6762  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/143]  eta: 0:00:29  Lr: 0.001875  Loss: 1.5705  Acc@1: 62.5000 (63.3045)  Acc@5: 87.5000 (89.2946)  time: 0.6745  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/143]  eta: 0:00:22  Lr: 0.001875  Loss: 0.6835  Acc@1: 62.5000 (63.2883)  Acc@5: 81.2500 (88.7387)  time: 0.6751  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/143]  eta: 0:00:15  Lr: 0.001875  Loss: 1.5957  Acc@1: 62.5000 (63.2748)  Acc@5: 81.2500 (88.5331)  time: 0.6759  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/143]  eta: 0:00:08  Lr: 0.001875  Loss: 1.0133  Acc@1: 62.5000 (63.2156)  Acc@5: 93.7500 (89.0267)  time: 0.6769  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[2/5]  [140/143]  eta: 0:00:02  Lr: 0.001875  Loss: 1.3310  Acc@1: 62.5000 (62.8989)  Acc@5: 93.7500 (88.9184)  time: 0.6790  data: 0.0028  max mem: 2378\n",
            "Train: Epoch[2/5]  [142/143]  eta: 0:00:00  Lr: 0.001875  Loss: 0.8211  Acc@1: 62.5000 (63.0197)  Acc@5: 93.7500 (88.9716)  time: 0.6742  data: 0.0027  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:37 (0.6819 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.8211  Acc@1: 62.5000 (63.0197)  Acc@5: 93.7500 (88.9716)\n",
            "Train: Epoch[3/5]  [  0/143]  eta: 0:02:39  Lr: 0.001875  Loss: 1.1039  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 1.1170  data: 0.4828  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/143]  eta: 0:01:36  Lr: 0.001875  Loss: 1.2632  Acc@1: 62.5000 (64.2045)  Acc@5: 87.5000 (87.5000)  time: 0.7220  data: 0.0478  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/143]  eta: 0:01:26  Lr: 0.001875  Loss: 1.1993  Acc@1: 62.5000 (65.1786)  Acc@5: 87.5000 (89.5833)  time: 0.6828  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/143]  eta: 0:01:18  Lr: 0.001875  Loss: 1.3331  Acc@1: 62.5000 (65.5242)  Acc@5: 93.7500 (89.1129)  time: 0.6836  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/143]  eta: 0:01:11  Lr: 0.001875  Loss: 0.6883  Acc@1: 68.7500 (66.4634)  Acc@5: 93.7500 (88.8720)  time: 0.6846  data: 0.0005  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/143]  eta: 0:01:04  Lr: 0.001875  Loss: 0.7545  Acc@1: 68.7500 (66.2990)  Acc@5: 93.7500 (89.8284)  time: 0.6835  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/143]  eta: 0:00:57  Lr: 0.001875  Loss: 1.0875  Acc@1: 68.7500 (66.7008)  Acc@5: 93.7500 (90.1639)  time: 0.6815  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/143]  eta: 0:00:50  Lr: 0.001875  Loss: 0.8177  Acc@1: 68.7500 (66.2852)  Acc@5: 93.7500 (90.2289)  time: 0.6814  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/143]  eta: 0:00:43  Lr: 0.001875  Loss: 0.8299  Acc@1: 68.7500 (66.9753)  Acc@5: 93.7500 (90.2778)  time: 0.6810  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/143]  eta: 0:00:36  Lr: 0.001875  Loss: 0.7023  Acc@1: 75.0000 (66.8269)  Acc@5: 93.7500 (90.3846)  time: 0.6797  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/143]  eta: 0:00:29  Lr: 0.001875  Loss: 1.1882  Acc@1: 62.5000 (66.7079)  Acc@5: 93.7500 (90.6559)  time: 0.6782  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/143]  eta: 0:00:22  Lr: 0.001875  Loss: 1.4269  Acc@1: 62.5000 (66.3288)  Acc@5: 87.5000 (90.5968)  time: 0.6772  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/143]  eta: 0:00:15  Lr: 0.001875  Loss: 0.7514  Acc@1: 68.7500 (66.7872)  Acc@5: 87.5000 (90.6508)  time: 0.6763  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/143]  eta: 0:00:08  Lr: 0.001875  Loss: 1.0210  Acc@1: 68.7500 (67.0324)  Acc@5: 93.7500 (90.5534)  time: 0.6768  data: 0.0032  max mem: 2378\n",
            "Train: Epoch[3/5]  [140/143]  eta: 0:00:02  Lr: 0.001875  Loss: 1.0134  Acc@1: 62.5000 (66.5780)  Acc@5: 93.7500 (90.4255)  time: 0.6782  data: 0.0041  max mem: 2378\n",
            "Train: Epoch[3/5]  [142/143]  eta: 0:00:00  Lr: 0.001875  Loss: 0.9981  Acc@1: 62.5000 (66.6521)  Acc@5: 87.5000 (90.3282)  time: 0.6730  data: 0.0033  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:37 (0.6831 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.9981  Acc@1: 62.5000 (66.6521)  Acc@5: 87.5000 (90.3282)\n",
            "Train: Epoch[4/5]  [  0/143]  eta: 0:02:31  Lr: 0.001875  Loss: 0.3571  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 1.0611  data: 0.4309  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/143]  eta: 0:01:34  Lr: 0.001875  Loss: 0.9868  Acc@1: 75.0000 (72.7273)  Acc@5: 93.7500 (94.3182)  time: 0.7129  data: 0.0407  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/143]  eta: 0:01:25  Lr: 0.001875  Loss: 0.8739  Acc@1: 68.7500 (72.3214)  Acc@5: 93.7500 (92.2619)  time: 0.6793  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/143]  eta: 0:01:18  Lr: 0.001875  Loss: 0.9254  Acc@1: 68.7500 (70.7661)  Acc@5: 93.7500 (92.7419)  time: 0.6810  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/143]  eta: 0:01:11  Lr: 0.001875  Loss: 0.9489  Acc@1: 68.7500 (69.9695)  Acc@5: 93.7500 (91.9207)  time: 0.6818  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/143]  eta: 0:01:04  Lr: 0.001875  Loss: 1.5280  Acc@1: 68.7500 (69.6078)  Acc@5: 87.5000 (91.7892)  time: 0.6836  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/143]  eta: 0:00:57  Lr: 0.001875  Loss: 1.1143  Acc@1: 68.7500 (69.6721)  Acc@5: 93.7500 (91.9057)  time: 0.6845  data: 0.0025  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/143]  eta: 0:00:50  Lr: 0.001875  Loss: 0.8452  Acc@1: 75.0000 (69.9824)  Acc@5: 93.7500 (91.9014)  time: 0.6842  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/143]  eta: 0:00:43  Lr: 0.001875  Loss: 1.2449  Acc@1: 68.7500 (69.9846)  Acc@5: 93.7500 (92.3611)  time: 0.6842  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/143]  eta: 0:00:36  Lr: 0.001875  Loss: 0.7135  Acc@1: 68.7500 (70.4670)  Acc@5: 93.7500 (92.5137)  time: 0.6825  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/143]  eta: 0:00:29  Lr: 0.001875  Loss: 0.7731  Acc@1: 75.0000 (70.9777)  Acc@5: 93.7500 (92.3267)  time: 0.6806  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/143]  eta: 0:00:22  Lr: 0.001875  Loss: 0.9065  Acc@1: 75.0000 (70.7770)  Acc@5: 93.7500 (92.3986)  time: 0.6804  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/143]  eta: 0:00:15  Lr: 0.001875  Loss: 0.8763  Acc@1: 68.7500 (70.5579)  Acc@5: 93.7500 (92.4070)  time: 0.6790  data: 0.0026  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/143]  eta: 0:00:08  Lr: 0.001875  Loss: 0.8308  Acc@1: 68.7500 (70.3721)  Acc@5: 87.5000 (92.3187)  time: 0.6772  data: 0.0026  max mem: 2378\n",
            "Train: Epoch[4/5]  [140/143]  eta: 0:00:02  Lr: 0.001875  Loss: 0.8774  Acc@1: 68.7500 (70.1684)  Acc@5: 93.7500 (92.1986)  time: 0.6766  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[4/5]  [142/143]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6374  Acc@1: 68.7500 (70.1969)  Acc@5: 92.3077 (92.1225)  time: 0.6709  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:37 (0.6832 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6374  Acc@1: 68.7500 (70.1969)  Acc@5: 92.3077 (92.1225)\n",
            "Train: Epoch[5/5]  [  0/143]  eta: 0:02:12  Lr: 0.001875  Loss: 1.2005  Acc@1: 43.7500 (43.7500)  Acc@5: 100.0000 (100.0000)  time: 0.9293  data: 0.2981  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/143]  eta: 0:01:33  Lr: 0.001875  Loss: 0.5156  Acc@1: 75.0000 (68.7500)  Acc@5: 93.7500 (93.1818)  time: 0.6994  data: 0.0275  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/143]  eta: 0:01:24  Lr: 0.001875  Loss: 0.6615  Acc@1: 75.0000 (70.2381)  Acc@5: 93.7500 (93.1548)  time: 0.6762  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/143]  eta: 0:01:17  Lr: 0.001875  Loss: 1.0217  Acc@1: 68.7500 (70.5645)  Acc@5: 93.7500 (91.9355)  time: 0.6769  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/143]  eta: 0:01:10  Lr: 0.001875  Loss: 1.2691  Acc@1: 68.7500 (70.2744)  Acc@5: 93.7500 (92.2256)  time: 0.6778  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/143]  eta: 0:01:03  Lr: 0.001875  Loss: 0.8171  Acc@1: 75.0000 (70.7108)  Acc@5: 93.7500 (92.1569)  time: 0.6784  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/143]  eta: 0:00:56  Lr: 0.001875  Loss: 1.3725  Acc@1: 75.0000 (70.5943)  Acc@5: 87.5000 (92.1107)  time: 0.6793  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/143]  eta: 0:00:49  Lr: 0.001875  Loss: 0.3629  Acc@1: 68.7500 (71.2148)  Acc@5: 93.7500 (92.4296)  time: 0.6788  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/143]  eta: 0:00:42  Lr: 0.001875  Loss: 0.5399  Acc@1: 75.0000 (71.4506)  Acc@5: 93.7500 (92.9012)  time: 0.6779  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/143]  eta: 0:00:36  Lr: 0.001875  Loss: 1.1204  Acc@1: 68.7500 (71.7033)  Acc@5: 93.7500 (92.7885)  time: 0.6780  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/143]  eta: 0:00:29  Lr: 0.001875  Loss: 0.5248  Acc@1: 68.7500 (72.0297)  Acc@5: 93.7500 (93.0693)  time: 0.6781  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/143]  eta: 0:00:22  Lr: 0.001875  Loss: 0.9631  Acc@1: 68.7500 (71.2838)  Acc@5: 93.7500 (92.9054)  time: 0.6782  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/143]  eta: 0:00:15  Lr: 0.001875  Loss: 0.7354  Acc@1: 68.7500 (71.2293)  Acc@5: 93.7500 (93.0785)  time: 0.6782  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/143]  eta: 0:00:08  Lr: 0.001875  Loss: 1.2926  Acc@1: 68.7500 (71.5172)  Acc@5: 93.7500 (93.0344)  time: 0.6787  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [140/143]  eta: 0:00:02  Lr: 0.001875  Loss: 1.3344  Acc@1: 75.0000 (71.2323)  Acc@5: 93.7500 (92.7748)  time: 0.6792  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [142/143]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3734  Acc@1: 69.2308 (71.1597)  Acc@5: 93.7500 (92.6477)  time: 0.6730  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:37 (0.6798 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3734  Acc@1: 69.2308 (71.1597)  Acc@5: 93.7500 (92.6477)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:44  Loss: 2.6268 (2.6268)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 1.0878  data: 0.6759  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.4485 (2.4059)  Acc@1: 50.0000 (49.4318)  Acc@5: 75.0000 (75.5682)  time: 0.4989  data: 0.0626  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9147 (2.0507)  Acc@1: 56.2500 (57.7381)  Acc@5: 81.2500 (80.3571)  time: 0.4408  data: 0.0010  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7557 (2.0773)  Acc@1: 68.7500 (57.4597)  Acc@5: 81.2500 (79.4355)  time: 0.4412  data: 0.0021  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0201 (2.0924)  Acc@1: 62.5000 (58.6154)  Acc@5: 81.2500 (79.2308)  time: 0.4340  data: 0.0025  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4547 s / it)\n",
            "* Acc@1 58.615 Acc@5 79.231 loss 2.092\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:40  Loss: 1.0016 (1.0016)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.8523  data: 0.4480  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.0190 (1.3555)  Acc@1: 81.2500 (75.5682)  Acc@5: 87.5000 (87.5000)  time: 0.4778  data: 0.0421  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8335 (1.8390)  Acc@1: 68.7500 (67.5595)  Acc@5: 81.2500 (82.1429)  time: 0.4407  data: 0.0025  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.1344 (1.9022)  Acc@1: 62.5000 (67.9435)  Acc@5: 81.2500 (81.8548)  time: 0.4405  data: 0.0032  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2393 (2.0246)  Acc@1: 62.5000 (64.1768)  Acc@5: 81.2500 (80.1829)  time: 0.4408  data: 0.0017  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1676 (1.9679)  Acc@1: 62.5000 (65.0667)  Acc@5: 75.0000 (80.1333)  time: 0.4396  data: 0.0008  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4503 s / it)\n",
            "* Acc@1 65.067 Acc@5 80.133 loss 1.968\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:22  Loss: 2.5866 (2.5866)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (87.5000)  time: 0.8505  data: 0.4410  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.6910 (1.8417)  Acc@1: 56.2500 (57.9545)  Acc@5: 87.5000 (87.5000)  time: 0.4892  data: 0.0594  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7726 (1.9007)  Acc@1: 50.0000 (57.4405)  Acc@5: 81.2500 (83.6310)  time: 0.4463  data: 0.0111  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8199 (1.9914)  Acc@1: 50.0000 (55.0351)  Acc@5: 81.2500 (83.1382)  time: 0.4335  data: 0.0012  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4571 s / it)\n",
            "* Acc@1 55.035 Acc@5 83.138 loss 1.991\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:29  Loss: 2.8800 (2.8800)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 0.8104  data: 0.4050  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.4860 (2.1216)  Acc@1: 43.7500 (55.1136)  Acc@5: 81.2500 (75.5682)  time: 0.4738  data: 0.0400  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1931 (2.2724)  Acc@1: 50.0000 (52.6786)  Acc@5: 81.2500 (73.8095)  time: 0.4413  data: 0.0048  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1931 (2.2445)  Acc@1: 56.2500 (54.0323)  Acc@5: 81.2500 (77.2177)  time: 0.4429  data: 0.0037  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1931 (2.2467)  Acc@1: 56.2500 (53.6458)  Acc@5: 81.2500 (76.7361)  time: 0.4430  data: 0.0018  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4545 s / it)\n",
            "* Acc@1 53.646 Acc@5 76.736 loss 2.247\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:39  Loss: 1.6073 (1.6073)  Acc@1: 81.2500 (81.2500)  Acc@5: 81.2500 (81.2500)  time: 0.8719  data: 0.4577  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.0510 (2.1353)  Acc@1: 62.5000 (67.0455)  Acc@5: 81.2500 (84.6591)  time: 0.4821  data: 0.0445  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.1318 (2.0852)  Acc@1: 62.5000 (63.6905)  Acc@5: 81.2500 (82.1429)  time: 0.4435  data: 0.0023  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.2009 (2.1809)  Acc@1: 50.0000 (59.8790)  Acc@5: 75.0000 (77.4194)  time: 0.4439  data: 0.0011  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4364 (2.1818)  Acc@1: 50.0000 (58.5366)  Acc@5: 75.0000 (77.4390)  time: 0.4442  data: 0.0005  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.1937 (2.1542)  Acc@1: 50.0000 (58.9385)  Acc@5: 75.0000 (77.5140)  time: 0.4395  data: 0.0003  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4531 s / it)\n",
            "* Acc@1 58.939 Acc@5 77.514 loss 2.154\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:36  Loss: 2.3714 (2.3714)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 1.0392  data: 0.6338  max mem: 2378\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:13  Loss: 2.3714 (2.3698)  Acc@1: 50.0000 (48.2955)  Acc@5: 75.0000 (74.4318)  time: 0.5412  data: 0.1105  max mem: 2378\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 2.2070 (2.3339)  Acc@1: 43.7500 (50.0000)  Acc@5: 75.0000 (75.5952)  time: 0.4666  data: 0.0295  max mem: 2378\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0067 (2.0690)  Acc@1: 75.0000 (59.4758)  Acc@5: 87.5000 (79.0323)  time: 0.4413  data: 0.0024  max mem: 2378\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7181 (2.0069)  Acc@1: 75.0000 (61.6487)  Acc@5: 87.5000 (80.6452)  time: 0.4388  data: 0.0022  max mem: 2378\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4733 s / it)\n",
            "* Acc@1 61.649 Acc@5 80.645 loss 2.007\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:32  Loss: 2.2426 (2.2426)  Acc@1: 75.0000 (75.0000)  Acc@5: 75.0000 (75.0000)  time: 0.9414  data: 0.5334  max mem: 2378\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:13  Loss: 2.4538 (2.3822)  Acc@1: 56.2500 (58.5227)  Acc@5: 75.0000 (75.0000)  time: 0.5267  data: 0.0944  max mem: 2378\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4538 (2.4102)  Acc@1: 56.2500 (59.8214)  Acc@5: 75.0000 (76.1905)  time: 0.4627  data: 0.0258  max mem: 2378\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4225 (2.3242)  Acc@1: 62.5000 (62.5000)  Acc@5: 81.2500 (77.2177)  time: 0.4395  data: 0.0024  max mem: 2378\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.8790 (2.2077)  Acc@1: 68.7500 (64.4524)  Acc@5: 87.5000 (78.8151)  time: 0.4366  data: 0.0024  max mem: 2378\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4669 s / it)\n",
            "* Acc@1 64.452 Acc@5 78.815 loss 2.208\n",
            "[Average accuracy till task7]\tAcc@1: 59.6290\tAcc@5: 79.4589\tLoss: 2.0953\tForgetting: 6.1763\tBackward: -6.1763\n",
            "Train: Epoch[1/5]  [  0/115]  eta: 0:03:10  Lr: 0.001875  Loss: 2.8580  Acc@1: 0.0000 (0.0000)  Acc@5: 25.0000 (25.0000)  time: 1.6565  data: 0.9913  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/115]  eta: 0:01:20  Lr: 0.001875  Loss: 2.7072  Acc@1: 18.7500 (20.4545)  Acc@5: 43.7500 (43.1818)  time: 0.7623  data: 0.0913  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/115]  eta: 0:01:08  Lr: 0.001875  Loss: 2.6070  Acc@1: 31.2500 (27.9762)  Acc@5: 56.2500 (53.5714)  time: 0.6758  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/115]  eta: 0:01:00  Lr: 0.001875  Loss: 2.4108  Acc@1: 37.5000 (31.4516)  Acc@5: 68.7500 (61.0887)  time: 0.6814  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/115]  eta: 0:00:52  Lr: 0.001875  Loss: 2.0667  Acc@1: 43.7500 (35.8232)  Acc@5: 75.0000 (64.7866)  time: 0.6852  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/115]  eta: 0:00:45  Lr: 0.001875  Loss: 1.7269  Acc@1: 50.0000 (39.0931)  Acc@5: 81.2500 (68.2598)  time: 0.6855  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/115]  eta: 0:00:38  Lr: 0.001875  Loss: 1.7295  Acc@1: 50.0000 (40.8811)  Acc@5: 81.2500 (69.8770)  time: 0.6828  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/115]  eta: 0:00:31  Lr: 0.001875  Loss: 1.6173  Acc@1: 43.7500 (41.6373)  Acc@5: 75.0000 (71.1268)  time: 0.6804  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/115]  eta: 0:00:24  Lr: 0.001875  Loss: 1.8344  Acc@1: 56.2500 (44.1358)  Acc@5: 81.2500 (72.9167)  time: 0.6789  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/115]  eta: 0:00:17  Lr: 0.001875  Loss: 2.1609  Acc@1: 56.2500 (44.7115)  Acc@5: 81.2500 (73.7637)  time: 0.6771  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/115]  eta: 0:00:10  Lr: 0.001875  Loss: 1.0484  Acc@1: 50.0000 (46.4728)  Acc@5: 81.2500 (74.8762)  time: 0.6758  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/115]  eta: 0:00:03  Lr: 0.001875  Loss: 1.4306  Acc@1: 56.2500 (47.5788)  Acc@5: 87.5000 (75.6194)  time: 0.6758  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[1/5]  [114/115]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3690  Acc@1: 62.5000 (48.1441)  Acc@5: 87.5000 (76.2555)  time: 0.6594  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:18 (0.6858 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3690  Acc@1: 62.5000 (48.1441)  Acc@5: 87.5000 (76.2555)\n",
            "Train: Epoch[2/5]  [  0/115]  eta: 0:02:23  Lr: 0.001875  Loss: 1.1594  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 1.2518  data: 0.5954  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/115]  eta: 0:01:16  Lr: 0.001875  Loss: 1.1461  Acc@1: 62.5000 (65.3409)  Acc@5: 87.5000 (88.0682)  time: 0.7276  data: 0.0564  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/115]  eta: 0:01:06  Lr: 0.001875  Loss: 1.2335  Acc@1: 68.7500 (67.2619)  Acc@5: 87.5000 (90.1786)  time: 0.6766  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/115]  eta: 0:00:59  Lr: 0.001875  Loss: 1.0732  Acc@1: 68.7500 (65.9274)  Acc@5: 93.7500 (90.5242)  time: 0.6776  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/115]  eta: 0:00:51  Lr: 0.001875  Loss: 1.3261  Acc@1: 62.5000 (65.3963)  Acc@5: 87.5000 (89.1768)  time: 0.6774  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/115]  eta: 0:00:44  Lr: 0.001875  Loss: 1.1846  Acc@1: 62.5000 (65.8088)  Acc@5: 87.5000 (89.3382)  time: 0.6773  data: 0.0031  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/115]  eta: 0:00:37  Lr: 0.001875  Loss: 1.6276  Acc@1: 68.7500 (64.8566)  Acc@5: 87.5000 (88.9344)  time: 0.6775  data: 0.0031  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/115]  eta: 0:00:30  Lr: 0.001875  Loss: 0.7311  Acc@1: 68.7500 (65.7570)  Acc@5: 87.5000 (89.1725)  time: 0.6781  data: 0.0023  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/115]  eta: 0:00:23  Lr: 0.001875  Loss: 1.0082  Acc@1: 62.5000 (65.2778)  Acc@5: 87.5000 (89.0432)  time: 0.6792  data: 0.0025  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/115]  eta: 0:00:17  Lr: 0.001875  Loss: 0.9935  Acc@1: 62.5000 (65.4533)  Acc@5: 87.5000 (89.0110)  time: 0.6810  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/115]  eta: 0:00:10  Lr: 0.001875  Loss: 1.1610  Acc@1: 68.7500 (65.7178)  Acc@5: 87.5000 (88.6139)  time: 0.6820  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/115]  eta: 0:00:03  Lr: 0.001875  Loss: 1.2202  Acc@1: 68.7500 (65.9910)  Acc@5: 87.5000 (88.6261)  time: 0.6832  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [114/115]  eta: 0:00:00  Lr: 0.001875  Loss: 1.2903  Acc@1: 62.5000 (65.6659)  Acc@5: 87.5000 (88.4825)  time: 0.6663  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:18 (0.6819 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.2903  Acc@1: 62.5000 (65.6659)  Acc@5: 87.5000 (88.4825)\n",
            "Train: Epoch[3/5]  [  0/115]  eta: 0:01:54  Lr: 0.001875  Loss: 0.8054  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9915  data: 0.3526  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/115]  eta: 0:01:14  Lr: 0.001875  Loss: 1.0130  Acc@1: 81.2500 (74.4318)  Acc@5: 93.7500 (92.0455)  time: 0.7120  data: 0.0330  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/115]  eta: 0:01:06  Lr: 0.001875  Loss: 1.0283  Acc@1: 68.7500 (70.8333)  Acc@5: 93.7500 (91.0714)  time: 0.6844  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/115]  eta: 0:00:58  Lr: 0.001875  Loss: 1.0145  Acc@1: 62.5000 (70.1613)  Acc@5: 93.7500 (90.9274)  time: 0.6838  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/115]  eta: 0:00:51  Lr: 0.001875  Loss: 0.9881  Acc@1: 75.0000 (71.1890)  Acc@5: 93.7500 (90.8537)  time: 0.6832  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/115]  eta: 0:00:44  Lr: 0.001875  Loss: 1.2588  Acc@1: 68.7500 (70.4657)  Acc@5: 87.5000 (89.9510)  time: 0.6834  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/115]  eta: 0:00:37  Lr: 0.001875  Loss: 0.5389  Acc@1: 75.0000 (71.4139)  Acc@5: 93.7500 (90.3689)  time: 0.6827  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/115]  eta: 0:00:30  Lr: 0.001875  Loss: 1.0058  Acc@1: 75.0000 (71.1268)  Acc@5: 93.7500 (89.9648)  time: 0.6827  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/115]  eta: 0:00:24  Lr: 0.001875  Loss: 0.8251  Acc@1: 75.0000 (71.0648)  Acc@5: 87.5000 (90.2778)  time: 0.6827  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/115]  eta: 0:00:17  Lr: 0.001875  Loss: 0.9114  Acc@1: 75.0000 (70.8791)  Acc@5: 93.7500 (90.5220)  time: 0.6825  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/115]  eta: 0:00:10  Lr: 0.001875  Loss: 0.9829  Acc@1: 68.7500 (71.1634)  Acc@5: 87.5000 (90.5941)  time: 0.6831  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/115]  eta: 0:00:03  Lr: 0.001875  Loss: 0.6973  Acc@1: 68.7500 (71.1149)  Acc@5: 93.7500 (90.7658)  time: 0.6832  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [114/115]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6898  Acc@1: 68.7500 (71.0153)  Acc@5: 93.7500 (90.7205)  time: 0.6659  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:18 (0.6835 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6898  Acc@1: 68.7500 (71.0153)  Acc@5: 93.7500 (90.7205)\n",
            "Train: Epoch[4/5]  [  0/115]  eta: 0:02:01  Lr: 0.001875  Loss: 0.7333  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 1.0561  data: 0.4244  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/115]  eta: 0:01:15  Lr: 0.001875  Loss: 0.8683  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7161  data: 0.0389  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/115]  eta: 0:01:06  Lr: 0.001875  Loss: 0.8008  Acc@1: 75.0000 (75.5952)  Acc@5: 93.7500 (92.8571)  time: 0.6822  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/115]  eta: 0:00:59  Lr: 0.001875  Loss: 0.4215  Acc@1: 75.0000 (72.1774)  Acc@5: 87.5000 (91.5323)  time: 0.6822  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/115]  eta: 0:00:51  Lr: 0.001875  Loss: 0.9653  Acc@1: 68.7500 (72.8659)  Acc@5: 87.5000 (91.3110)  time: 0.6823  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/115]  eta: 0:00:44  Lr: 0.001875  Loss: 1.0271  Acc@1: 75.0000 (73.2843)  Acc@5: 93.7500 (91.4216)  time: 0.6826  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/115]  eta: 0:00:37  Lr: 0.001875  Loss: 1.1189  Acc@1: 75.0000 (73.2582)  Acc@5: 93.7500 (91.7008)  time: 0.6831  data: 0.0028  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/115]  eta: 0:00:30  Lr: 0.001875  Loss: 1.2805  Acc@1: 81.2500 (73.8556)  Acc@5: 93.7500 (91.8134)  time: 0.6822  data: 0.0022  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/115]  eta: 0:00:24  Lr: 0.001875  Loss: 1.1006  Acc@1: 81.2500 (73.7654)  Acc@5: 93.7500 (92.2840)  time: 0.6817  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/115]  eta: 0:00:17  Lr: 0.001875  Loss: 0.5537  Acc@1: 75.0000 (73.5577)  Acc@5: 93.7500 (92.1703)  time: 0.6825  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/115]  eta: 0:00:10  Lr: 0.001875  Loss: 1.2956  Acc@1: 68.7500 (73.0817)  Acc@5: 93.7500 (92.0173)  time: 0.6825  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/115]  eta: 0:00:03  Lr: 0.001875  Loss: 1.0865  Acc@1: 68.7500 (72.9730)  Acc@5: 93.7500 (92.1734)  time: 0.6828  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[4/5]  [114/115]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3347  Acc@1: 68.7500 (72.9258)  Acc@5: 93.7500 (92.2489)  time: 0.6662  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:18 (0.6836 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3347  Acc@1: 68.7500 (72.9258)  Acc@5: 93.7500 (92.2489)\n",
            "Train: Epoch[5/5]  [  0/115]  eta: 0:02:20  Lr: 0.001875  Loss: 0.2914  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 1.2213  data: 0.5727  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/115]  eta: 0:01:16  Lr: 0.001875  Loss: 0.4999  Acc@1: 81.2500 (80.6818)  Acc@5: 93.7500 (92.6136)  time: 0.7304  data: 0.0535  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/115]  eta: 0:01:07  Lr: 0.001875  Loss: 0.8282  Acc@1: 75.0000 (78.5714)  Acc@5: 93.7500 (94.0476)  time: 0.6818  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/115]  eta: 0:00:59  Lr: 0.001875  Loss: 0.9475  Acc@1: 75.0000 (76.4113)  Acc@5: 93.7500 (93.5484)  time: 0.6825  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/115]  eta: 0:00:52  Lr: 0.001875  Loss: 1.1673  Acc@1: 68.7500 (75.3049)  Acc@5: 93.7500 (93.9024)  time: 0.6823  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/115]  eta: 0:00:45  Lr: 0.001875  Loss: 0.9656  Acc@1: 68.7500 (74.7549)  Acc@5: 93.7500 (93.2598)  time: 0.6822  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/115]  eta: 0:00:38  Lr: 0.001875  Loss: 0.7693  Acc@1: 75.0000 (74.5902)  Acc@5: 93.7500 (93.7500)  time: 0.6828  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/115]  eta: 0:00:31  Lr: 0.001875  Loss: 1.2351  Acc@1: 75.0000 (74.5599)  Acc@5: 93.7500 (93.7500)  time: 0.6833  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/115]  eta: 0:00:24  Lr: 0.001875  Loss: 0.4801  Acc@1: 68.7500 (74.1512)  Acc@5: 93.7500 (93.6728)  time: 0.6831  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/115]  eta: 0:00:17  Lr: 0.001875  Loss: 0.4614  Acc@1: 75.0000 (74.4505)  Acc@5: 93.7500 (94.0247)  time: 0.6839  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/115]  eta: 0:00:10  Lr: 0.001875  Loss: 0.7945  Acc@1: 75.0000 (74.6287)  Acc@5: 93.7500 (93.8119)  time: 0.6837  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/115]  eta: 0:00:03  Lr: 0.001875  Loss: 0.7473  Acc@1: 68.7500 (73.6486)  Acc@5: 87.5000 (93.4685)  time: 0.6827  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5]  [114/115]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5427  Acc@1: 68.7500 (73.6354)  Acc@5: 93.7500 (93.5590)  time: 0.6661  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:18 (0.6852 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5427  Acc@1: 68.7500 (73.6354)  Acc@5: 93.7500 (93.5590)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:33  Loss: 2.6499 (2.6499)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 0.8192  data: 0.4138  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.4174 (2.3366)  Acc@1: 56.2500 (51.1364)  Acc@5: 75.0000 (75.0000)  time: 0.4768  data: 0.0408  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9478 (2.0653)  Acc@1: 56.2500 (57.4405)  Acc@5: 75.0000 (78.8690)  time: 0.4427  data: 0.0021  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7993 (2.1060)  Acc@1: 62.5000 (56.4516)  Acc@5: 81.2500 (77.4194)  time: 0.4431  data: 0.0006  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 1.9353 (2.1070)  Acc@1: 62.5000 (58.0000)  Acc@5: 81.2500 (77.5385)  time: 0.4355  data: 0.0003  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4506 s / it)\n",
            "* Acc@1 58.000 Acc@5 77.538 loss 2.107\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:40  Loss: 0.9751 (0.9751)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.8566  data: 0.4400  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1781 (1.4313)  Acc@1: 81.2500 (76.7045)  Acc@5: 87.5000 (85.7955)  time: 0.4787  data: 0.0404  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.9016 (1.8928)  Acc@1: 68.7500 (68.4524)  Acc@5: 81.2500 (81.5476)  time: 0.4424  data: 0.0005  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0960 (1.9204)  Acc@1: 62.5000 (67.9435)  Acc@5: 81.2500 (81.8548)  time: 0.4433  data: 0.0010  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1342 (2.0477)  Acc@1: 62.5000 (63.5671)  Acc@5: 81.2500 (79.4207)  time: 0.4432  data: 0.0013  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0629 (1.9764)  Acc@1: 62.5000 (64.9333)  Acc@5: 75.0000 (79.4667)  time: 0.4407  data: 0.0007  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4518 s / it)\n",
            "* Acc@1 64.933 Acc@5 79.467 loss 1.976\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:24  Loss: 2.5675 (2.5675)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.9136  data: 0.5048  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.7334 (1.8420)  Acc@1: 50.0000 (55.1136)  Acc@5: 87.5000 (84.6591)  time: 0.4866  data: 0.0493  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7753 (1.9114)  Acc@1: 50.0000 (55.6548)  Acc@5: 81.2500 (81.2500)  time: 0.4436  data: 0.0030  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8660 (1.9929)  Acc@1: 50.0000 (52.9274)  Acc@5: 81.2500 (81.2646)  time: 0.4356  data: 0.0020  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4578 s / it)\n",
            "* Acc@1 52.927 Acc@5 81.265 loss 1.993\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:30  Loss: 3.0528 (3.0528)  Acc@1: 37.5000 (37.5000)  Acc@5: 43.7500 (43.7500)  time: 0.8422  data: 0.4239  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.5921 (2.2229)  Acc@1: 37.5000 (52.2727)  Acc@5: 75.0000 (73.8636)  time: 0.4787  data: 0.0403  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.2073 (2.3495)  Acc@1: 50.0000 (50.8929)  Acc@5: 75.0000 (71.7262)  time: 0.4426  data: 0.0026  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.2073 (2.3131)  Acc@1: 50.0000 (51.6129)  Acc@5: 75.0000 (75.2016)  time: 0.4434  data: 0.0017  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.2073 (2.3162)  Acc@1: 50.0000 (51.7361)  Acc@5: 75.0000 (74.6528)  time: 0.4436  data: 0.0003  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4562 s / it)\n",
            "* Acc@1 51.736 Acc@5 74.653 loss 2.316\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:36  Loss: 1.6310 (1.6310)  Acc@1: 81.2500 (81.2500)  Acc@5: 81.2500 (81.2500)  time: 0.8123  data: 0.4065  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.0798 (2.1542)  Acc@1: 62.5000 (67.0455)  Acc@5: 81.2500 (84.6591)  time: 0.4755  data: 0.0388  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.0798 (2.0838)  Acc@1: 62.5000 (63.6905)  Acc@5: 81.2500 (81.8452)  time: 0.4445  data: 0.0012  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1553 (2.1996)  Acc@1: 50.0000 (59.8790)  Acc@5: 75.0000 (76.8145)  time: 0.4463  data: 0.0004  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.3929 (2.2037)  Acc@1: 50.0000 (57.9268)  Acc@5: 75.0000 (77.1341)  time: 0.4455  data: 0.0010  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.2243 (2.1811)  Acc@1: 50.0000 (58.2402)  Acc@5: 75.0000 (77.2346)  time: 0.4407  data: 0.0010  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4525 s / it)\n",
            "* Acc@1 58.240 Acc@5 77.235 loss 2.181\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:29  Loss: 2.4428 (2.4428)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.8526  data: 0.4391  max mem: 2378\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:12  Loss: 2.4428 (2.4763)  Acc@1: 43.7500 (47.7273)  Acc@5: 68.7500 (73.2955)  time: 0.4824  data: 0.0438  max mem: 2378\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:06  Loss: 2.3245 (2.4221)  Acc@1: 43.7500 (49.1071)  Acc@5: 68.7500 (73.8095)  time: 0.4450  data: 0.0024  max mem: 2378\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0010 (2.1262)  Acc@1: 68.7500 (58.4677)  Acc@5: 81.2500 (77.8226)  time: 0.4435  data: 0.0014  max mem: 2378\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7173 (2.0618)  Acc@1: 75.0000 (60.5735)  Acc@5: 87.5000 (79.3907)  time: 0.4412  data: 0.0013  max mem: 2378\n",
            "Test: [Task 6] Total time: 0:00:15 (0.4561 s / it)\n",
            "* Acc@1 60.573 Acc@5 79.391 loss 2.062\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:27  Loss: 2.4080 (2.4080)  Acc@1: 75.0000 (75.0000)  Acc@5: 75.0000 (75.0000)  time: 0.7839  data: 0.3743  max mem: 2378\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:12  Loss: 2.4745 (2.4079)  Acc@1: 62.5000 (57.9545)  Acc@5: 75.0000 (73.8636)  time: 0.5139  data: 0.0798  max mem: 2378\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4877 (2.4572)  Acc@1: 56.2500 (57.4405)  Acc@5: 75.0000 (74.4048)  time: 0.4645  data: 0.0267  max mem: 2378\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4802 (2.3524)  Acc@1: 56.2500 (60.0806)  Acc@5: 75.0000 (75.8065)  time: 0.4416  data: 0.0017  max mem: 2378\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.8723 (2.2263)  Acc@1: 68.7500 (62.4776)  Acc@5: 81.2500 (77.3788)  time: 0.4393  data: 0.0017  max mem: 2378\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4647 s / it)\n",
            "* Acc@1 62.478 Acc@5 77.379 loss 2.226\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:22  Loss: 1.9272 (1.9272)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 0.7884  data: 0.3751  max mem: 2378\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:08  Loss: 2.0150 (1.9356)  Acc@1: 68.7500 (68.1818)  Acc@5: 81.2500 (78.9773)  time: 0.4707  data: 0.0346  max mem: 2378\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.0733 (2.1500)  Acc@1: 62.5000 (63.3929)  Acc@5: 81.2500 (76.4881)  time: 0.4402  data: 0.0011  max mem: 2378\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.0150 (2.0551)  Acc@1: 68.7500 (64.9438)  Acc@5: 81.2500 (78.2022)  time: 0.4380  data: 0.0009  max mem: 2378\n",
            "Test: [Task 8] Total time: 0:00:12 (0.4533 s / it)\n",
            "* Acc@1 64.944 Acc@5 78.202 loss 2.055\n",
            "[Average accuracy till task8]\tAcc@1: 59.2290\tAcc@5: 78.1411\tLoss: 2.1146\tForgetting: 6.5104\tBackward: -6.5104\n",
            "Train: Epoch[1/5]  [  0/156]  eta: 0:03:00  Lr: 0.001875  Loss: 2.8802  Acc@1: 6.2500 (6.2500)  Acc@5: 6.2500 (6.2500)  time: 1.1561  data: 0.5274  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/156]  eta: 0:01:44  Lr: 0.001875  Loss: 2.5753  Acc@1: 25.0000 (26.7045)  Acc@5: 62.5000 (51.1364)  time: 0.7191  data: 0.0483  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/156]  eta: 0:01:35  Lr: 0.001875  Loss: 2.2731  Acc@1: 43.7500 (40.7738)  Acc@5: 68.7500 (64.5833)  time: 0.6761  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/156]  eta: 0:01:27  Lr: 0.001875  Loss: 2.4151  Acc@1: 56.2500 (47.1774)  Acc@5: 75.0000 (67.7419)  time: 0.6783  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/156]  eta: 0:01:20  Lr: 0.001875  Loss: 1.4798  Acc@1: 56.2500 (50.1524)  Acc@5: 81.2500 (70.8841)  time: 0.6803  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/156]  eta: 0:01:12  Lr: 0.001875  Loss: 2.0485  Acc@1: 62.5000 (53.9216)  Acc@5: 87.5000 (73.8971)  time: 0.6817  data: 0.0005  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/156]  eta: 0:01:06  Lr: 0.001875  Loss: 1.7972  Acc@1: 68.7500 (56.1475)  Acc@5: 87.5000 (75.8197)  time: 0.6832  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/156]  eta: 0:00:59  Lr: 0.001875  Loss: 1.2348  Acc@1: 62.5000 (57.7465)  Acc@5: 87.5000 (77.9049)  time: 0.6849  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/156]  eta: 0:00:52  Lr: 0.001875  Loss: 1.1922  Acc@1: 68.7500 (59.1821)  Acc@5: 87.5000 (79.0895)  time: 0.6839  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/156]  eta: 0:00:45  Lr: 0.001875  Loss: 1.7662  Acc@1: 68.7500 (60.0962)  Acc@5: 87.5000 (80.1511)  time: 0.6818  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/156]  eta: 0:00:38  Lr: 0.001875  Loss: 1.4358  Acc@1: 68.7500 (60.7673)  Acc@5: 87.5000 (80.6312)  time: 0.6811  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/156]  eta: 0:00:31  Lr: 0.001875  Loss: 1.5274  Acc@1: 68.7500 (61.2613)  Acc@5: 87.5000 (81.3063)  time: 0.6807  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/156]  eta: 0:00:24  Lr: 0.001875  Loss: 1.0512  Acc@1: 62.5000 (61.8802)  Acc@5: 93.7500 (82.3347)  time: 0.6816  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/156]  eta: 0:00:17  Lr: 0.001875  Loss: 0.8996  Acc@1: 62.5000 (62.2615)  Acc@5: 93.7500 (82.8721)  time: 0.6816  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [140/156]  eta: 0:00:10  Lr: 0.001875  Loss: 1.0262  Acc@1: 68.7500 (62.8989)  Acc@5: 93.7500 (83.4663)  time: 0.6806  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [150/156]  eta: 0:00:04  Lr: 0.001875  Loss: 1.1719  Acc@1: 75.0000 (63.4934)  Acc@5: 93.7500 (84.0646)  time: 0.6802  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[1/5]  [155/156]  eta: 0:00:00  Lr: 0.001875  Loss: 1.3304  Acc@1: 75.0000 (63.9146)  Acc@5: 93.7500 (84.2529)  time: 0.6528  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:01:46 (0.6809 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.3304  Acc@1: 75.0000 (63.9146)  Acc@5: 93.7500 (84.2529)\n",
            "Train: Epoch[2/5]  [  0/156]  eta: 0:02:26  Lr: 0.001875  Loss: 0.8482  Acc@1: 81.2500 (81.2500)  Acc@5: 87.5000 (87.5000)  time: 0.9409  data: 0.3031  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/156]  eta: 0:01:42  Lr: 0.001875  Loss: 0.8129  Acc@1: 81.2500 (78.9773)  Acc@5: 93.7500 (90.9091)  time: 0.7036  data: 0.0280  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/156]  eta: 0:01:34  Lr: 0.001875  Loss: 1.0726  Acc@1: 81.2500 (77.3810)  Acc@5: 93.7500 (90.7738)  time: 0.6796  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/156]  eta: 0:01:26  Lr: 0.001875  Loss: 1.0995  Acc@1: 81.2500 (78.0242)  Acc@5: 93.7500 (91.1290)  time: 0.6790  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/156]  eta: 0:01:19  Lr: 0.001875  Loss: 1.0576  Acc@1: 81.2500 (76.8293)  Acc@5: 93.7500 (91.1585)  time: 0.6787  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/156]  eta: 0:01:12  Lr: 0.001875  Loss: 0.5945  Acc@1: 75.0000 (77.0833)  Acc@5: 93.7500 (91.5441)  time: 0.6791  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/156]  eta: 0:01:05  Lr: 0.001875  Loss: 0.7903  Acc@1: 75.0000 (77.7664)  Acc@5: 93.7500 (92.0082)  time: 0.6786  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/156]  eta: 0:00:58  Lr: 0.001875  Loss: 0.3910  Acc@1: 75.0000 (76.9366)  Acc@5: 93.7500 (92.2535)  time: 0.6785  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/156]  eta: 0:00:51  Lr: 0.001875  Loss: 1.2777  Acc@1: 75.0000 (76.4660)  Acc@5: 93.7500 (92.0525)  time: 0.6790  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/156]  eta: 0:00:44  Lr: 0.001875  Loss: 0.8090  Acc@1: 75.0000 (76.5797)  Acc@5: 93.7500 (92.1703)  time: 0.6786  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/156]  eta: 0:00:38  Lr: 0.001875  Loss: 0.7234  Acc@1: 75.0000 (76.7327)  Acc@5: 93.7500 (92.4505)  time: 0.6782  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/156]  eta: 0:00:31  Lr: 0.001875  Loss: 1.1758  Acc@1: 75.0000 (76.4640)  Acc@5: 93.7500 (92.5113)  time: 0.6787  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/156]  eta: 0:00:24  Lr: 0.001875  Loss: 0.7172  Acc@1: 81.2500 (76.7045)  Acc@5: 93.7500 (92.5620)  time: 0.6790  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/156]  eta: 0:00:17  Lr: 0.001875  Loss: 0.4863  Acc@1: 81.2500 (76.5267)  Acc@5: 93.7500 (92.6527)  time: 0.6784  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[2/5]  [140/156]  eta: 0:00:10  Lr: 0.001875  Loss: 1.0087  Acc@1: 75.0000 (76.5071)  Acc@5: 93.7500 (92.8191)  time: 0.6772  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [150/156]  eta: 0:00:04  Lr: 0.001875  Loss: 0.5365  Acc@1: 75.0000 (76.5315)  Acc@5: 93.7500 (92.7566)  time: 0.6774  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [155/156]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0757  Acc@1: 75.0000 (76.4398)  Acc@5: 93.7500 (92.8715)  time: 0.6508  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:01:45 (0.6773 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0757  Acc@1: 75.0000 (76.4398)  Acc@5: 93.7500 (92.8715)\n",
            "Train: Epoch[3/5]  [  0/156]  eta: 0:03:03  Lr: 0.001875  Loss: 0.4220  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 1.1744  data: 0.5148  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/156]  eta: 0:01:45  Lr: 0.001875  Loss: 0.3916  Acc@1: 81.2500 (77.8409)  Acc@5: 93.7500 (94.3182)  time: 0.7208  data: 0.0479  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/156]  eta: 0:01:35  Lr: 0.001875  Loss: 0.7767  Acc@1: 81.2500 (78.5714)  Acc@5: 93.7500 (95.5357)  time: 0.6758  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/156]  eta: 0:01:27  Lr: 0.001875  Loss: 0.9510  Acc@1: 81.2500 (78.0242)  Acc@5: 93.7500 (94.3548)  time: 0.6761  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/156]  eta: 0:01:19  Lr: 0.001875  Loss: 0.4803  Acc@1: 75.0000 (77.8963)  Acc@5: 93.7500 (94.2073)  time: 0.6771  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/156]  eta: 0:01:12  Lr: 0.001875  Loss: 0.6812  Acc@1: 81.2500 (78.9216)  Acc@5: 93.7500 (94.2402)  time: 0.6776  data: 0.0028  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/156]  eta: 0:01:05  Lr: 0.001875  Loss: 0.5479  Acc@1: 81.2500 (79.0984)  Acc@5: 93.7500 (93.8525)  time: 0.6774  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 70/156]  eta: 0:00:58  Lr: 0.001875  Loss: 0.7299  Acc@1: 81.2500 (79.7535)  Acc@5: 93.7500 (94.1021)  time: 0.6776  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/156]  eta: 0:00:51  Lr: 0.001875  Loss: 0.9765  Acc@1: 81.2500 (79.4753)  Acc@5: 93.7500 (93.9815)  time: 0.6780  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/156]  eta: 0:00:45  Lr: 0.001875  Loss: 0.5106  Acc@1: 75.0000 (78.5714)  Acc@5: 93.7500 (94.1621)  time: 0.6788  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/156]  eta: 0:00:38  Lr: 0.001875  Loss: 0.5595  Acc@1: 75.0000 (78.5272)  Acc@5: 93.7500 (94.2450)  time: 0.6783  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/156]  eta: 0:00:31  Lr: 0.001875  Loss: 0.7850  Acc@1: 75.0000 (78.5473)  Acc@5: 93.7500 (93.9189)  time: 0.6775  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/156]  eta: 0:00:24  Lr: 0.001875  Loss: 0.4315  Acc@1: 81.2500 (78.7190)  Acc@5: 93.7500 (93.8017)  time: 0.6774  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/156]  eta: 0:00:17  Lr: 0.001875  Loss: 0.2699  Acc@1: 75.0000 (78.6260)  Acc@5: 93.7500 (93.7500)  time: 0.6776  data: 0.0021  max mem: 2378\n",
            "Train: Epoch[3/5]  [140/156]  eta: 0:00:10  Lr: 0.001875  Loss: 0.2991  Acc@1: 81.2500 (78.9450)  Acc@5: 100.0000 (93.9716)  time: 0.6781  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[3/5]  [150/156]  eta: 0:00:04  Lr: 0.001875  Loss: 0.6587  Acc@1: 81.2500 (79.0563)  Acc@5: 100.0000 (94.0811)  time: 0.6778  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [155/156]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1150  Acc@1: 81.2500 (79.2590)  Acc@5: 100.0000 (94.1200)  time: 0.6508  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:01:45 (0.6777 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1150  Acc@1: 81.2500 (79.2590)  Acc@5: 100.0000 (94.1200)\n",
            "Train: Epoch[4/5]  [  0/156]  eta: 0:02:43  Lr: 0.001875  Loss: 0.2702  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 1.0454  data: 0.4145  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/156]  eta: 0:01:43  Lr: 0.001875  Loss: 1.3578  Acc@1: 87.5000 (80.6818)  Acc@5: 93.7500 (93.1818)  time: 0.7103  data: 0.0397  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/156]  eta: 0:01:34  Lr: 0.001875  Loss: 0.9751  Acc@1: 75.0000 (77.3810)  Acc@5: 93.7500 (93.1548)  time: 0.6764  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/156]  eta: 0:01:26  Lr: 0.001875  Loss: 0.4384  Acc@1: 81.2500 (79.0323)  Acc@5: 93.7500 (94.9597)  time: 0.6773  data: 0.0035  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/156]  eta: 0:01:19  Lr: 0.001875  Loss: 0.9760  Acc@1: 81.2500 (78.3537)  Acc@5: 100.0000 (94.5122)  time: 0.6784  data: 0.0035  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/156]  eta: 0:01:12  Lr: 0.001875  Loss: 0.5827  Acc@1: 81.2500 (78.6765)  Acc@5: 93.7500 (94.2402)  time: 0.6784  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/156]  eta: 0:01:05  Lr: 0.001875  Loss: 0.9051  Acc@1: 81.2500 (78.8934)  Acc@5: 93.7500 (94.4672)  time: 0.6790  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/156]  eta: 0:00:58  Lr: 0.001875  Loss: 0.7229  Acc@1: 81.2500 (79.2254)  Acc@5: 93.7500 (94.6303)  time: 0.6792  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/156]  eta: 0:00:51  Lr: 0.001875  Loss: 0.8735  Acc@1: 81.2500 (79.5525)  Acc@5: 93.7500 (94.2130)  time: 0.6791  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/156]  eta: 0:00:45  Lr: 0.001875  Loss: 0.9138  Acc@1: 81.2500 (80.0137)  Acc@5: 93.7500 (94.2308)  time: 0.6798  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/156]  eta: 0:00:38  Lr: 0.001875  Loss: 0.7325  Acc@1: 81.2500 (79.7649)  Acc@5: 93.7500 (93.8119)  time: 0.6801  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/156]  eta: 0:00:31  Lr: 0.001875  Loss: 0.4244  Acc@1: 81.2500 (79.7297)  Acc@5: 93.7500 (94.0315)  time: 0.6803  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/156]  eta: 0:00:24  Lr: 0.001875  Loss: 0.5495  Acc@1: 75.0000 (79.4938)  Acc@5: 93.7500 (93.9566)  time: 0.6809  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/156]  eta: 0:00:17  Lr: 0.001875  Loss: 0.4913  Acc@1: 75.0000 (79.3893)  Acc@5: 93.7500 (93.9885)  time: 0.6809  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[4/5]  [140/156]  eta: 0:00:10  Lr: 0.001875  Loss: 0.9562  Acc@1: 75.0000 (79.2110)  Acc@5: 93.7500 (93.9716)  time: 0.6806  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[4/5]  [150/156]  eta: 0:00:04  Lr: 0.001875  Loss: 0.3392  Acc@1: 81.2500 (79.4702)  Acc@5: 93.7500 (94.0397)  time: 0.6811  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[4/5]  [155/156]  eta: 0:00:00  Lr: 0.001875  Loss: 0.1981  Acc@1: 81.2500 (79.5812)  Acc@5: 93.7500 (94.1200)  time: 0.6543  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:01:45 (0.6788 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.1981  Acc@1: 81.2500 (79.5812)  Acc@5: 93.7500 (94.1200)\n",
            "Train: Epoch[5/5]  [  0/156]  eta: 0:02:44  Lr: 0.001875  Loss: 0.6394  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 1.0559  data: 0.4195  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/156]  eta: 0:01:44  Lr: 0.001875  Loss: 0.1685  Acc@1: 87.5000 (85.7955)  Acc@5: 100.0000 (96.5909)  time: 0.7138  data: 0.0389  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/156]  eta: 0:01:34  Lr: 0.001875  Loss: 0.3946  Acc@1: 87.5000 (83.0357)  Acc@5: 93.7500 (96.4286)  time: 0.6806  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/156]  eta: 0:01:27  Lr: 0.001875  Loss: 0.3870  Acc@1: 81.2500 (82.6613)  Acc@5: 93.7500 (95.9677)  time: 0.6814  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/156]  eta: 0:01:20  Lr: 0.001875  Loss: 0.6509  Acc@1: 81.2500 (82.6220)  Acc@5: 93.7500 (95.8841)  time: 0.6814  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/156]  eta: 0:01:12  Lr: 0.001875  Loss: 0.2046  Acc@1: 81.2500 (82.4755)  Acc@5: 100.0000 (96.5686)  time: 0.6815  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/156]  eta: 0:01:05  Lr: 0.001875  Loss: 0.7332  Acc@1: 75.0000 (81.8648)  Acc@5: 100.0000 (96.1066)  time: 0.6816  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/156]  eta: 0:00:59  Lr: 0.001875  Loss: 0.4125  Acc@1: 81.2500 (82.3944)  Acc@5: 93.7500 (95.6866)  time: 0.6820  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/156]  eta: 0:00:52  Lr: 0.001875  Loss: 0.9936  Acc@1: 75.0000 (81.4815)  Acc@5: 87.5000 (94.9074)  time: 0.6825  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/156]  eta: 0:00:45  Lr: 0.001875  Loss: 0.4767  Acc@1: 75.0000 (81.3874)  Acc@5: 93.7500 (94.9176)  time: 0.6826  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/156]  eta: 0:00:38  Lr: 0.001875  Loss: 0.6484  Acc@1: 81.2500 (81.3738)  Acc@5: 93.7500 (94.9876)  time: 0.6829  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/156]  eta: 0:00:31  Lr: 0.001875  Loss: 0.6414  Acc@1: 75.0000 (81.2500)  Acc@5: 93.7500 (94.9887)  time: 0.6830  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/156]  eta: 0:00:24  Lr: 0.001875  Loss: 0.2152  Acc@1: 81.2500 (81.3533)  Acc@5: 93.7500 (95.1446)  time: 0.6831  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/156]  eta: 0:00:17  Lr: 0.001875  Loss: 0.4839  Acc@1: 81.2500 (81.6794)  Acc@5: 93.7500 (95.0382)  time: 0.6835  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [140/156]  eta: 0:00:10  Lr: 0.001875  Loss: 0.7525  Acc@1: 87.5000 (81.5603)  Acc@5: 93.7500 (94.9911)  time: 0.6830  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[5/5]  [150/156]  eta: 0:00:04  Lr: 0.001875  Loss: 0.5249  Acc@1: 81.2500 (81.4570)  Acc@5: 93.7500 (94.8262)  time: 0.6831  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[5/5]  [155/156]  eta: 0:00:00  Lr: 0.001875  Loss: 0.3194  Acc@1: 87.5000 (81.5948)  Acc@5: 93.7500 (94.8449)  time: 0.6563  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:01:46 (0.6817 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.3194  Acc@1: 87.5000 (81.5948)  Acc@5: 93.7500 (94.8449)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:35  Loss: 2.4918 (2.4918)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.8723  data: 0.4695  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.3498 (2.3550)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (73.8636)  time: 0.4806  data: 0.0438  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0566 (2.0609)  Acc@1: 56.2500 (57.1429)  Acc@5: 75.0000 (76.7857)  time: 0.4425  data: 0.0008  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7757 (2.1099)  Acc@1: 56.2500 (56.0484)  Acc@5: 75.0000 (76.0081)  time: 0.4433  data: 0.0014  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 1.9226 (2.1063)  Acc@1: 60.0000 (57.3846)  Acc@5: 75.0000 (76.1538)  time: 0.4360  data: 0.0014  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4514 s / it)\n",
            "* Acc@1 57.385 Acc@5 76.154 loss 2.106\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:38  Loss: 0.9917 (0.9917)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.8089  data: 0.3992  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1954 (1.4226)  Acc@1: 75.0000 (73.2955)  Acc@5: 87.5000 (86.3636)  time: 0.4772  data: 0.0366  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8327 (1.8836)  Acc@1: 68.7500 (66.0714)  Acc@5: 81.2500 (81.5476)  time: 0.4439  data: 0.0019  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0562 (1.9186)  Acc@1: 62.5000 (66.5323)  Acc@5: 75.0000 (81.6532)  time: 0.4434  data: 0.0023  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.0999 (2.0632)  Acc@1: 62.5000 (62.3476)  Acc@5: 75.0000 (78.9634)  time: 0.4435  data: 0.0008  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0562 (2.0025)  Acc@1: 62.5000 (63.6000)  Acc@5: 75.0000 (79.0667)  time: 0.4416  data: 0.0003  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4529 s / it)\n",
            "* Acc@1 63.600 Acc@5 79.067 loss 2.002\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:30  Loss: 2.5486 (2.5486)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 1.1380  data: 0.7334  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.7438 (1.8550)  Acc@1: 50.0000 (55.6818)  Acc@5: 87.5000 (85.7955)  time: 0.5067  data: 0.0711  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7640 (1.9338)  Acc@1: 56.2500 (56.2500)  Acc@5: 81.2500 (82.1429)  time: 0.4436  data: 0.0026  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8419 (2.0211)  Acc@1: 50.0000 (53.8642)  Acc@5: 81.2500 (81.4988)  time: 0.4358  data: 0.0003  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4672 s / it)\n",
            "* Acc@1 53.864 Acc@5 81.499 loss 2.021\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:40  Loss: 2.9006 (2.9006)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 1.1230  data: 0.7189  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:13  Loss: 2.5418 (2.1742)  Acc@1: 43.7500 (52.8409)  Acc@5: 81.2500 (74.4318)  time: 0.5019  data: 0.0673  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.2069 (2.3178)  Acc@1: 50.0000 (50.8929)  Acc@5: 75.0000 (71.7262)  time: 0.4414  data: 0.0021  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.2069 (2.2689)  Acc@1: 56.2500 (52.6210)  Acc@5: 75.0000 (75.0000)  time: 0.4428  data: 0.0014  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.2069 (2.2755)  Acc@1: 56.2500 (52.4306)  Acc@5: 75.0000 (74.8264)  time: 0.4430  data: 0.0010  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4631 s / it)\n",
            "* Acc@1 52.431 Acc@5 74.826 loss 2.275\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:39  Loss: 1.5649 (1.5649)  Acc@1: 81.2500 (81.2500)  Acc@5: 87.5000 (87.5000)  time: 0.8715  data: 0.4608  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.1275 (2.1712)  Acc@1: 56.2500 (60.7955)  Acc@5: 87.5000 (82.9545)  time: 0.4815  data: 0.0424  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.1275 (2.0767)  Acc@1: 56.2500 (60.7143)  Acc@5: 81.2500 (81.2500)  time: 0.4435  data: 0.0005  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1292 (2.1940)  Acc@1: 56.2500 (57.8629)  Acc@5: 75.0000 (76.2097)  time: 0.4440  data: 0.0011  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4457 (2.1930)  Acc@1: 50.0000 (56.4024)  Acc@5: 68.7500 (76.5244)  time: 0.4439  data: 0.0010  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.2914 (2.1674)  Acc@1: 50.0000 (56.9832)  Acc@5: 68.7500 (76.6760)  time: 0.4394  data: 0.0010  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4527 s / it)\n",
            "* Acc@1 56.983 Acc@5 76.676 loss 2.167\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:26  Loss: 2.3755 (2.3755)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.7669  data: 0.3604  max mem: 2378\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:11  Loss: 2.4056 (2.4709)  Acc@1: 43.7500 (47.1591)  Acc@5: 68.7500 (71.0227)  time: 0.4789  data: 0.0440  max mem: 2378\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:06  Loss: 2.2707 (2.4359)  Acc@1: 37.5000 (46.7262)  Acc@5: 68.7500 (72.3214)  time: 0.4465  data: 0.0068  max mem: 2378\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0411 (2.1256)  Acc@1: 75.0000 (56.8548)  Acc@5: 81.2500 (77.0161)  time: 0.4426  data: 0.0013  max mem: 2378\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.6260 (2.0610)  Acc@1: 75.0000 (58.7814)  Acc@5: 87.5000 (78.4946)  time: 0.4408  data: 0.0009  max mem: 2378\n",
            "Test: [Task 6] Total time: 0:00:15 (0.4552 s / it)\n",
            "* Acc@1 58.781 Acc@5 78.495 loss 2.061\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:27  Loss: 2.2154 (2.2154)  Acc@1: 68.7500 (68.7500)  Acc@5: 75.0000 (75.0000)  time: 0.7840  data: 0.3716  max mem: 2378\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:14  Loss: 2.4305 (2.3813)  Acc@1: 56.2500 (55.1136)  Acc@5: 68.7500 (73.2955)  time: 0.5782  data: 0.1481  max mem: 2378\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4550 (2.4662)  Acc@1: 50.0000 (52.6786)  Acc@5: 75.0000 (74.1071)  time: 0.4997  data: 0.0632  max mem: 2378\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4524 (2.3630)  Acc@1: 56.2500 (55.8468)  Acc@5: 75.0000 (75.6048)  time: 0.4426  data: 0.0004  max mem: 2378\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.9469 (2.2411)  Acc@1: 68.7500 (58.5278)  Acc@5: 81.2500 (77.0197)  time: 0.4398  data: 0.0004  max mem: 2378\n",
            "Test: [Task 7] Total time: 0:00:17 (0.4861 s / it)\n",
            "* Acc@1 58.528 Acc@5 77.020 loss 2.241\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:35  Loss: 2.0237 (2.0237)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 1.2801  data: 0.8729  max mem: 2378\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:09  Loss: 2.1102 (1.9993)  Acc@1: 62.5000 (63.6364)  Acc@5: 81.2500 (77.2727)  time: 0.5198  data: 0.0809  max mem: 2378\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.1328 (2.1917)  Acc@1: 62.5000 (58.9286)  Acc@5: 81.2500 (75.8929)  time: 0.4442  data: 0.0010  max mem: 2378\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.0412 (2.0967)  Acc@1: 62.5000 (60.6742)  Acc@5: 81.2500 (77.7528)  time: 0.4412  data: 0.0003  max mem: 2378\n",
            "Test: [Task 8] Total time: 0:00:13 (0.4748 s / it)\n",
            "* Acc@1 60.674 Acc@5 77.753 loss 2.097\n",
            "Test: [Task 9]  [ 0/40]  eta: 0:00:50  Loss: 1.8466 (1.8466)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 1.2574  data: 0.8267  max mem: 2378\n",
            "Test: [Task 9]  [10/40]  eta: 0:00:15  Loss: 1.2106 (1.4253)  Acc@1: 81.2500 (77.2727)  Acc@5: 93.7500 (88.6364)  time: 0.5169  data: 0.0761  max mem: 2378\n",
            "Test: [Task 9]  [30/40]  eta: 0:00:04  Loss: 1.1117 (1.3689)  Acc@1: 81.2500 (78.2258)  Acc@5: 87.5000 (90.3226)  time: 0.4439  data: 0.0013  max mem: 2378\n",
            "Test: [Task 9]  [39/40]  eta: 0:00:00  Loss: 1.2729 (1.3819)  Acc@1: 81.2500 (78.4689)  Acc@5: 87.5000 (90.1116)  time: 0.4272  data: 0.0012  max mem: 2378\n",
            "Test: [Task 9] Total time: 0:00:18 (0.4573 s / it)\n",
            "* Acc@1 78.469 Acc@5 90.112 loss 1.382\n",
            "[Average accuracy till task9]\tAcc@1: 60.0794\tAcc@5: 79.0667\tLoss: 2.0393\tForgetting: 7.1448\tBackward: -7.1448\n",
            "Train: Epoch[1/5]  [  0/178]  eta: 0:03:52  Lr: 0.001875  Loss: 2.8852  Acc@1: 6.2500 (6.2500)  Acc@5: 18.7500 (18.7500)  time: 1.3062  data: 0.6735  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 10/178]  eta: 0:02:03  Lr: 0.001875  Loss: 2.6506  Acc@1: 12.5000 (14.7727)  Acc@5: 37.5000 (43.7500)  time: 0.7371  data: 0.0615  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 20/178]  eta: 0:01:52  Lr: 0.001875  Loss: 2.6957  Acc@1: 18.7500 (20.2381)  Acc@5: 50.0000 (46.1310)  time: 0.6799  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 30/178]  eta: 0:01:43  Lr: 0.001875  Loss: 2.5458  Acc@1: 31.2500 (26.6129)  Acc@5: 56.2500 (53.0242)  time: 0.6797  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 40/178]  eta: 0:01:35  Lr: 0.001875  Loss: 2.3593  Acc@1: 37.5000 (30.6402)  Acc@5: 75.0000 (58.6890)  time: 0.6795  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 50/178]  eta: 0:01:28  Lr: 0.001875  Loss: 2.2103  Acc@1: 43.7500 (33.5784)  Acc@5: 75.0000 (61.0294)  time: 0.6794  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 60/178]  eta: 0:01:21  Lr: 0.001875  Loss: 2.3224  Acc@1: 43.7500 (36.2705)  Acc@5: 68.7500 (63.3197)  time: 0.6795  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 70/178]  eta: 0:01:14  Lr: 0.001875  Loss: 1.9355  Acc@1: 50.0000 (38.2923)  Acc@5: 75.0000 (65.4930)  time: 0.6792  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 80/178]  eta: 0:01:07  Lr: 0.001875  Loss: 1.9914  Acc@1: 50.0000 (40.2006)  Acc@5: 81.2500 (67.3611)  time: 0.6791  data: 0.0005  max mem: 2378\n",
            "Train: Epoch[1/5]  [ 90/178]  eta: 0:01:00  Lr: 0.001875  Loss: 1.5776  Acc@1: 62.5000 (42.5824)  Acc@5: 81.2500 (68.9560)  time: 0.6797  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [100/178]  eta: 0:00:53  Lr: 0.001875  Loss: 1.9265  Acc@1: 50.0000 (43.0693)  Acc@5: 81.2500 (69.8639)  time: 0.6794  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [110/178]  eta: 0:00:46  Lr: 0.001875  Loss: 1.4600  Acc@1: 50.0000 (44.5383)  Acc@5: 81.2500 (71.4527)  time: 0.6779  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[1/5]  [120/178]  eta: 0:00:39  Lr: 0.001875  Loss: 1.5778  Acc@1: 56.2500 (45.7645)  Acc@5: 87.5000 (72.3140)  time: 0.6774  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [130/178]  eta: 0:00:32  Lr: 0.001875  Loss: 1.7400  Acc@1: 56.2500 (46.5649)  Acc@5: 81.2500 (72.9008)  time: 0.6779  data: 0.0005  max mem: 2378\n",
            "Train: Epoch[1/5]  [140/178]  eta: 0:00:25  Lr: 0.001875  Loss: 1.4687  Acc@1: 56.2500 (47.5621)  Acc@5: 81.2500 (73.8032)  time: 0.6777  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[1/5]  [150/178]  eta: 0:00:19  Lr: 0.001875  Loss: 1.2695  Acc@1: 62.5000 (48.7997)  Acc@5: 87.5000 (74.7517)  time: 0.6770  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[1/5]  [160/178]  eta: 0:00:12  Lr: 0.001875  Loss: 1.3300  Acc@1: 62.5000 (49.3401)  Acc@5: 87.5000 (75.5047)  time: 0.6771  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [170/178]  eta: 0:00:05  Lr: 0.001875  Loss: 1.8268  Acc@1: 56.2500 (50.2193)  Acc@5: 87.5000 (76.2061)  time: 0.6775  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[1/5]  [177/178]  eta: 0:00:00  Lr: 0.001875  Loss: 1.2143  Acc@1: 62.5000 (50.9321)  Acc@5: 87.5000 (76.7147)  time: 0.6663  data: 0.0003  max mem: 2378\n",
            "Train: Epoch[1/5] Total time: 0:02:01 (0.6813 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.2143  Acc@1: 62.5000 (50.9321)  Acc@5: 87.5000 (76.7147)\n",
            "Train: Epoch[2/5]  [  0/178]  eta: 0:02:52  Lr: 0.001875  Loss: 1.1895  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.9715  data: 0.3323  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 10/178]  eta: 0:01:58  Lr: 0.001875  Loss: 1.2540  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (85.7955)  time: 0.7026  data: 0.0310  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 20/178]  eta: 0:01:48  Lr: 0.001875  Loss: 1.4236  Acc@1: 62.5000 (65.4762)  Acc@5: 87.5000 (87.7976)  time: 0.6758  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 30/178]  eta: 0:01:41  Lr: 0.001875  Loss: 1.7797  Acc@1: 68.7500 (66.9355)  Acc@5: 87.5000 (88.9113)  time: 0.6762  data: 0.0008  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 40/178]  eta: 0:01:34  Lr: 0.001875  Loss: 1.6294  Acc@1: 68.7500 (67.5305)  Acc@5: 87.5000 (89.1768)  time: 0.6768  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 50/178]  eta: 0:01:27  Lr: 0.001875  Loss: 0.9748  Acc@1: 68.7500 (66.9118)  Acc@5: 87.5000 (89.3382)  time: 0.6776  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 60/178]  eta: 0:01:20  Lr: 0.001875  Loss: 1.2236  Acc@1: 68.7500 (66.9057)  Acc@5: 87.5000 (89.1393)  time: 0.6783  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 70/178]  eta: 0:01:13  Lr: 0.001875  Loss: 1.4632  Acc@1: 68.7500 (67.4296)  Acc@5: 93.7500 (89.4366)  time: 0.6781  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 80/178]  eta: 0:01:06  Lr: 0.001875  Loss: 1.3203  Acc@1: 62.5000 (66.7438)  Acc@5: 87.5000 (89.2747)  time: 0.6776  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [ 90/178]  eta: 0:00:59  Lr: 0.001875  Loss: 1.2093  Acc@1: 68.7500 (66.9643)  Acc@5: 87.5000 (89.3544)  time: 0.6787  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [100/178]  eta: 0:00:53  Lr: 0.001875  Loss: 1.2193  Acc@1: 68.7500 (67.2030)  Acc@5: 87.5000 (89.4802)  time: 0.6795  data: 0.0019  max mem: 2378\n",
            "Train: Epoch[2/5]  [110/178]  eta: 0:00:46  Lr: 0.001875  Loss: 1.0733  Acc@1: 68.7500 (66.9482)  Acc@5: 87.5000 (89.4707)  time: 0.6797  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[2/5]  [120/178]  eta: 0:00:39  Lr: 0.001875  Loss: 1.2927  Acc@1: 62.5000 (66.4773)  Acc@5: 87.5000 (89.2562)  time: 0.6801  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[2/5]  [130/178]  eta: 0:00:32  Lr: 0.001875  Loss: 1.0089  Acc@1: 68.7500 (66.8893)  Acc@5: 87.5000 (88.9790)  time: 0.6800  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [140/178]  eta: 0:00:25  Lr: 0.001875  Loss: 1.2047  Acc@1: 68.7500 (67.3316)  Acc@5: 87.5000 (88.8298)  time: 0.6802  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [150/178]  eta: 0:00:19  Lr: 0.001875  Loss: 0.7242  Acc@1: 68.7500 (67.5497)  Acc@5: 87.5000 (89.0315)  time: 0.6806  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[2/5]  [160/178]  eta: 0:00:12  Lr: 0.001875  Loss: 1.6186  Acc@1: 68.7500 (67.3913)  Acc@5: 87.5000 (88.8587)  time: 0.6806  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[2/5]  [170/178]  eta: 0:00:05  Lr: 0.001875  Loss: 0.6851  Acc@1: 68.7500 (67.7266)  Acc@5: 87.5000 (88.9254)  time: 0.6802  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[2/5]  [177/178]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1976  Acc@1: 68.7500 (67.7102)  Acc@5: 93.7500 (89.2015)  time: 0.6695  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[2/5] Total time: 0:02:00 (0.6796 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1976  Acc@1: 68.7500 (67.7102)  Acc@5: 93.7500 (89.2015)\n",
            "Train: Epoch[3/5]  [  0/178]  eta: 0:02:56  Lr: 0.001875  Loss: 0.9518  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 0.9910  data: 0.3571  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 10/178]  eta: 0:01:58  Lr: 0.001875  Loss: 0.8330  Acc@1: 75.0000 (74.4318)  Acc@5: 93.7500 (93.7500)  time: 0.7078  data: 0.0352  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 20/178]  eta: 0:01:49  Lr: 0.001875  Loss: 0.6073  Acc@1: 68.7500 (71.7262)  Acc@5: 93.7500 (93.7500)  time: 0.6809  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 30/178]  eta: 0:01:42  Lr: 0.001875  Loss: 1.1589  Acc@1: 68.7500 (71.5726)  Acc@5: 93.7500 (94.1532)  time: 0.6816  data: 0.0004  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 40/178]  eta: 0:01:34  Lr: 0.001875  Loss: 1.0796  Acc@1: 68.7500 (71.4939)  Acc@5: 93.7500 (92.9878)  time: 0.6801  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 50/178]  eta: 0:01:27  Lr: 0.001875  Loss: 0.6508  Acc@1: 68.7500 (72.0588)  Acc@5: 87.5000 (92.7696)  time: 0.6802  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 60/178]  eta: 0:01:20  Lr: 0.001875  Loss: 0.8127  Acc@1: 75.0000 (72.0287)  Acc@5: 93.7500 (92.6230)  time: 0.6811  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 80/178]  eta: 0:01:07  Lr: 0.001875  Loss: 0.7154  Acc@1: 68.7500 (71.6821)  Acc@5: 93.7500 (92.1296)  time: 0.6820  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [ 90/178]  eta: 0:01:00  Lr: 0.001875  Loss: 0.8438  Acc@1: 75.0000 (71.9780)  Acc@5: 93.7500 (91.8956)  time: 0.6822  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[3/5]  [100/178]  eta: 0:00:53  Lr: 0.001875  Loss: 0.7076  Acc@1: 75.0000 (72.0297)  Acc@5: 87.5000 (91.5223)  time: 0.6824  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [110/178]  eta: 0:00:46  Lr: 0.001875  Loss: 0.3792  Acc@1: 75.0000 (72.1847)  Acc@5: 93.7500 (91.7793)  time: 0.6827  data: 0.0006  max mem: 2378\n",
            "Train: Epoch[3/5]  [120/178]  eta: 0:00:39  Lr: 0.001875  Loss: 0.7246  Acc@1: 68.7500 (72.0041)  Acc@5: 93.7500 (91.8388)  time: 0.6826  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [130/178]  eta: 0:00:32  Lr: 0.001875  Loss: 0.7770  Acc@1: 68.7500 (71.3740)  Acc@5: 93.7500 (91.6508)  time: 0.6820  data: 0.0007  max mem: 2378\n",
            "Train: Epoch[3/5]  [140/178]  eta: 0:00:25  Lr: 0.001875  Loss: 0.7274  Acc@1: 68.7500 (71.4539)  Acc@5: 93.7500 (91.7996)  time: 0.6822  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [150/178]  eta: 0:00:19  Lr: 0.001875  Loss: 0.8472  Acc@1: 75.0000 (71.8957)  Acc@5: 93.7500 (91.8046)  time: 0.6826  data: 0.0016  max mem: 2378\n",
            "Train: Epoch[3/5]  [160/178]  eta: 0:00:12  Lr: 0.001875  Loss: 0.9061  Acc@1: 75.0000 (71.6227)  Acc@5: 93.7500 (91.6537)  time: 0.6823  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[3/5]  [170/178]  eta: 0:00:05  Lr: 0.001875  Loss: 1.5038  Acc@1: 68.7500 (71.3450)  Acc@5: 87.5000 (91.4839)  time: 0.6821  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[3/5]  [177/178]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6873  Acc@1: 68.7500 (71.4738)  Acc@5: 87.5000 (91.4879)  time: 0.6714  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[3/5] Total time: 0:02:01 (0.6829 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6873  Acc@1: 68.7500 (71.4738)  Acc@5: 87.5000 (91.4879)\n",
            "Train: Epoch[4/5]  [  0/178]  eta: 0:03:40  Lr: 0.001875  Loss: 0.8819  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 1.2370  data: 0.5966  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 10/178]  eta: 0:02:02  Lr: 0.001875  Loss: 0.7876  Acc@1: 75.0000 (75.5682)  Acc@5: 93.7500 (93.7500)  time: 0.7316  data: 0.0545  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 20/178]  eta: 0:01:51  Lr: 0.001875  Loss: 1.0052  Acc@1: 75.0000 (75.8929)  Acc@5: 93.7500 (92.5595)  time: 0.6810  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 30/178]  eta: 0:01:43  Lr: 0.001875  Loss: 0.7337  Acc@1: 75.0000 (75.2016)  Acc@5: 93.7500 (92.3387)  time: 0.6817  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 40/178]  eta: 0:01:35  Lr: 0.001875  Loss: 1.3254  Acc@1: 75.0000 (74.0854)  Acc@5: 93.7500 (92.9878)  time: 0.6823  data: 0.0005  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 50/178]  eta: 0:01:28  Lr: 0.001875  Loss: 0.8197  Acc@1: 75.0000 (74.8775)  Acc@5: 93.7500 (92.6471)  time: 0.6824  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 60/178]  eta: 0:01:21  Lr: 0.001875  Loss: 0.4445  Acc@1: 75.0000 (74.8975)  Acc@5: 93.7500 (92.5205)  time: 0.6824  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 70/178]  eta: 0:01:14  Lr: 0.001875  Loss: 0.6768  Acc@1: 75.0000 (74.2077)  Acc@5: 93.7500 (92.6056)  time: 0.6824  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 80/178]  eta: 0:01:07  Lr: 0.001875  Loss: 1.1595  Acc@1: 68.7500 (74.1512)  Acc@5: 93.7500 (92.6698)  time: 0.6824  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [ 90/178]  eta: 0:01:00  Lr: 0.001875  Loss: 1.1288  Acc@1: 68.7500 (73.8324)  Acc@5: 87.5000 (92.6511)  time: 0.6827  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [100/178]  eta: 0:00:53  Lr: 0.001875  Loss: 0.4937  Acc@1: 81.2500 (74.5050)  Acc@5: 93.7500 (92.9455)  time: 0.6837  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5]  [110/178]  eta: 0:00:46  Lr: 0.001875  Loss: 0.9347  Acc@1: 81.2500 (74.0991)  Acc@5: 93.7500 (92.7928)  time: 0.6836  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [120/178]  eta: 0:00:39  Lr: 0.001875  Loss: 0.5023  Acc@1: 75.0000 (74.3285)  Acc@5: 93.7500 (92.8202)  time: 0.6832  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [130/178]  eta: 0:00:32  Lr: 0.001875  Loss: 0.7835  Acc@1: 75.0000 (74.1412)  Acc@5: 93.7500 (92.8912)  time: 0.6832  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[4/5]  [140/178]  eta: 0:00:26  Lr: 0.001875  Loss: 0.6045  Acc@1: 68.7500 (73.9362)  Acc@5: 93.7500 (92.8191)  time: 0.6829  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[4/5]  [150/178]  eta: 0:00:19  Lr: 0.001875  Loss: 0.9836  Acc@1: 68.7500 (73.7169)  Acc@5: 93.7500 (92.7980)  time: 0.6821  data: 0.0012  max mem: 2378\n",
            "Train: Epoch[4/5]  [160/178]  eta: 0:00:12  Lr: 0.001875  Loss: 0.9272  Acc@1: 75.0000 (73.4860)  Acc@5: 93.7500 (92.5078)  time: 0.6811  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[4/5]  [170/178]  eta: 0:00:05  Lr: 0.001875  Loss: 0.7465  Acc@1: 75.0000 (73.7573)  Acc@5: 93.7500 (92.6170)  time: 0.6805  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[4/5]  [177/178]  eta: 0:00:00  Lr: 0.001875  Loss: 0.4823  Acc@1: 81.2500 (73.8656)  Acc@5: 93.7500 (92.7190)  time: 0.6698  data: 0.0011  max mem: 2378\n",
            "Train: Epoch[4/5] Total time: 0:02:01 (0.6849 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.4823  Acc@1: 81.2500 (73.8656)  Acc@5: 93.7500 (92.7190)\n",
            "Train: Epoch[5/5]  [  0/178]  eta: 0:03:46  Lr: 0.001875  Loss: 0.8310  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 1.2707  data: 0.6189  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 10/178]  eta: 0:02:02  Lr: 0.001875  Loss: 0.7621  Acc@1: 75.0000 (76.1364)  Acc@5: 100.0000 (97.1591)  time: 0.7316  data: 0.0578  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 20/178]  eta: 0:01:51  Lr: 0.001875  Loss: 0.0739  Acc@1: 75.0000 (74.7024)  Acc@5: 93.7500 (95.8333)  time: 0.6789  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 30/178]  eta: 0:01:43  Lr: 0.001875  Loss: 0.5825  Acc@1: 68.7500 (72.1774)  Acc@5: 93.7500 (94.7581)  time: 0.6807  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 40/178]  eta: 0:01:35  Lr: 0.001875  Loss: 0.5757  Acc@1: 68.7500 (72.1037)  Acc@5: 93.7500 (94.9695)  time: 0.6809  data: 0.0013  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 50/178]  eta: 0:01:28  Lr: 0.001875  Loss: 1.0622  Acc@1: 75.0000 (73.1618)  Acc@5: 93.7500 (94.7304)  time: 0.6804  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 60/178]  eta: 0:01:21  Lr: 0.001875  Loss: 0.6647  Acc@1: 81.2500 (73.6680)  Acc@5: 93.7500 (94.9795)  time: 0.6800  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 70/178]  eta: 0:01:14  Lr: 0.001875  Loss: 0.8764  Acc@1: 81.2500 (73.6796)  Acc@5: 93.7500 (94.7183)  time: 0.6793  data: 0.0018  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 80/178]  eta: 0:01:07  Lr: 0.001875  Loss: 0.7697  Acc@1: 75.0000 (73.8426)  Acc@5: 93.7500 (94.7531)  time: 0.6784  data: 0.0014  max mem: 2378\n",
            "Train: Epoch[5/5]  [ 90/178]  eta: 0:01:00  Lr: 0.001875  Loss: 0.8816  Acc@1: 75.0000 (74.2445)  Acc@5: 93.7500 (94.7802)  time: 0.6790  data: 0.0026  max mem: 2378\n",
            "Train: Epoch[5/5]  [100/178]  eta: 0:00:53  Lr: 0.001875  Loss: 0.4123  Acc@1: 81.2500 (74.4431)  Acc@5: 100.0000 (94.8020)  time: 0.6793  data: 0.0020  max mem: 2378\n",
            "Train: Epoch[5/5]  [110/178]  eta: 0:00:46  Lr: 0.001875  Loss: 0.9771  Acc@1: 75.0000 (74.3243)  Acc@5: 93.7500 (94.4820)  time: 0.6780  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [120/178]  eta: 0:00:39  Lr: 0.001875  Loss: 1.7755  Acc@1: 75.0000 (74.6384)  Acc@5: 93.7500 (94.5248)  time: 0.6772  data: 0.0015  max mem: 2378\n",
            "Train: Epoch[5/5]  [130/178]  eta: 0:00:32  Lr: 0.001875  Loss: 0.6538  Acc@1: 75.0000 (74.7137)  Acc@5: 93.7500 (94.4656)  time: 0.6764  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [140/178]  eta: 0:00:25  Lr: 0.001875  Loss: 0.7746  Acc@1: 75.0000 (74.8670)  Acc@5: 93.7500 (94.3706)  time: 0.6759  data: 0.0017  max mem: 2378\n",
            "Train: Epoch[5/5]  [150/178]  eta: 0:00:19  Lr: 0.001875  Loss: 0.4301  Acc@1: 75.0000 (74.7930)  Acc@5: 93.7500 (94.2881)  time: 0.6760  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[5/5]  [160/178]  eta: 0:00:12  Lr: 0.001875  Loss: 0.5877  Acc@1: 68.7500 (74.7283)  Acc@5: 93.7500 (94.0606)  time: 0.6757  data: 0.0024  max mem: 2378\n",
            "Train: Epoch[5/5]  [170/178]  eta: 0:00:05  Lr: 0.001875  Loss: 0.7577  Acc@1: 75.0000 (74.7807)  Acc@5: 93.7500 (93.9693)  time: 0.6757  data: 0.0010  max mem: 2378\n",
            "Train: Epoch[5/5]  [177/178]  eta: 0:00:00  Lr: 0.001875  Loss: 1.1266  Acc@1: 75.0000 (74.8153)  Acc@5: 93.7500 (94.0204)  time: 0.6650  data: 0.0009  max mem: 2378\n",
            "Train: Epoch[5/5] Total time: 0:02:01 (0.6807 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 1.1266  Acc@1: 75.0000 (74.8153)  Acc@5: 93.7500 (94.0204)\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:34  Loss: 2.3888 (2.3888)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.8466  data: 0.4347  max mem: 2378\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.3888 (2.4879)  Acc@1: 50.0000 (47.7273)  Acc@5: 68.7500 (69.3182)  time: 0.4757  data: 0.0419  max mem: 2378\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0067 (2.1192)  Acc@1: 50.0000 (56.5476)  Acc@5: 75.0000 (74.1071)  time: 0.4394  data: 0.0026  max mem: 2378\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.6480 (2.1108)  Acc@1: 62.5000 (55.6452)  Acc@5: 81.2500 (74.7984)  time: 0.4405  data: 0.0015  max mem: 2378\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.1192 (2.1320)  Acc@1: 56.2500 (55.5385)  Acc@5: 75.0000 (74.6154)  time: 0.4339  data: 0.0004  max mem: 2378\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4491 s / it)\n",
            "* Acc@1 55.538 Acc@5 74.615 loss 2.132\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:51  Loss: 0.9263 (0.9263)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 1.0967  data: 0.6936  max mem: 2378\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:18  Loss: 1.0728 (1.4142)  Acc@1: 81.2500 (76.7045)  Acc@5: 93.7500 (86.3636)  time: 0.5011  data: 0.0644  max mem: 2378\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.7483 (1.8736)  Acc@1: 62.5000 (66.9643)  Acc@5: 81.2500 (80.3571)  time: 0.4426  data: 0.0014  max mem: 2378\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 1.9374 (1.9082)  Acc@1: 62.5000 (65.5242)  Acc@5: 81.2500 (81.0484)  time: 0.4440  data: 0.0012  max mem: 2378\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1568 (2.0723)  Acc@1: 56.2500 (61.4329)  Acc@5: 75.0000 (77.4390)  time: 0.4440  data: 0.0010  max mem: 2378\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1568 (2.0235)  Acc@1: 50.0000 (62.2667)  Acc@5: 75.0000 (77.3333)  time: 0.4414  data: 0.0007  max mem: 2378\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4576 s / it)\n",
            "* Acc@1 62.267 Acc@5 77.333 loss 2.023\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:24  Loss: 2.6432 (2.6432)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.9074  data: 0.4914  max mem: 2378\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.8827 (1.9236)  Acc@1: 50.0000 (56.2500)  Acc@5: 87.5000 (84.0909)  time: 0.4836  data: 0.0464  max mem: 2378\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.8827 (1.9735)  Acc@1: 50.0000 (55.6548)  Acc@5: 87.5000 (82.7381)  time: 0.4420  data: 0.0031  max mem: 2378\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.9367 (2.0780)  Acc@1: 50.0000 (52.6932)  Acc@5: 81.2500 (80.7963)  time: 0.4353  data: 0.0022  max mem: 2378\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4562 s / it)\n",
            "* Acc@1 52.693 Acc@5 80.796 loss 2.078\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:29  Loss: 2.9355 (2.9355)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 0.8143  data: 0.4151  max mem: 2378\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.6793 (2.2253)  Acc@1: 43.7500 (50.0000)  Acc@5: 68.7500 (72.7273)  time: 0.4756  data: 0.0383  max mem: 2378\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1882 (2.3498)  Acc@1: 50.0000 (47.9167)  Acc@5: 75.0000 (70.8333)  time: 0.4419  data: 0.0032  max mem: 2378\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1882 (2.2720)  Acc@1: 50.0000 (49.7984)  Acc@5: 75.0000 (74.7984)  time: 0.4424  data: 0.0034  max mem: 2378\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1882 (2.2723)  Acc@1: 50.0000 (49.8264)  Acc@5: 81.2500 (74.6528)  time: 0.4426  data: 0.0018  max mem: 2378\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4545 s / it)\n",
            "* Acc@1 49.826 Acc@5 74.653 loss 2.272\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:33  Loss: 1.3966 (1.3966)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7501  data: 0.3488  max mem: 2378\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 1.9468 (2.1250)  Acc@1: 62.5000 (59.6591)  Acc@5: 81.2500 (85.2273)  time: 0.4683  data: 0.0340  max mem: 2378\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 1.9468 (2.0586)  Acc@1: 62.5000 (61.0119)  Acc@5: 81.2500 (81.8452)  time: 0.4410  data: 0.0023  max mem: 2378\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1150 (2.1634)  Acc@1: 50.0000 (58.4677)  Acc@5: 68.7500 (76.2097)  time: 0.4415  data: 0.0016  max mem: 2378\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4788 (2.1647)  Acc@1: 50.0000 (57.0122)  Acc@5: 68.7500 (76.3720)  time: 0.4413  data: 0.0007  max mem: 2378\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.4195 (2.1463)  Acc@1: 50.0000 (57.2626)  Acc@5: 68.7500 (76.2570)  time: 0.4367  data: 0.0006  max mem: 2378\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4479 s / it)\n",
            "* Acc@1 57.263 Acc@5 76.257 loss 2.146\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:31  Loss: 2.3383 (2.3383)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.8901  data: 0.4841  max mem: 2378\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:12  Loss: 2.4003 (2.4055)  Acc@1: 43.7500 (46.0227)  Acc@5: 75.0000 (73.8636)  time: 0.4939  data: 0.0600  max mem: 2378\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 2.3959 (2.4066)  Acc@1: 43.7500 (46.1310)  Acc@5: 75.0000 (74.1071)  time: 0.4475  data: 0.0093  max mem: 2378\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 1.9285 (2.1294)  Acc@1: 68.7500 (55.0403)  Acc@5: 81.2500 (77.6210)  time: 0.4401  data: 0.0013  max mem: 2378\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7559 (2.0630)  Acc@1: 68.7500 (57.1685)  Acc@5: 81.2500 (79.0323)  time: 0.4379  data: 0.0010  max mem: 2378\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4578 s / it)\n",
            "* Acc@1 57.168 Acc@5 79.032 loss 2.063\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:27  Loss: 2.4299 (2.4299)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.7928  data: 0.3735  max mem: 2378\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:12  Loss: 2.5369 (2.4197)  Acc@1: 56.2500 (52.8409)  Acc@5: 68.7500 (71.5909)  time: 0.5175  data: 0.0861  max mem: 2378\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.5488 (2.4947)  Acc@1: 43.7500 (50.0000)  Acc@5: 68.7500 (72.9167)  time: 0.4648  data: 0.0307  max mem: 2378\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.5512 (2.4290)  Acc@1: 50.0000 (52.4194)  Acc@5: 75.0000 (73.3871)  time: 0.4399  data: 0.0022  max mem: 2378\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 2.0825 (2.3153)  Acc@1: 56.2500 (54.5781)  Acc@5: 75.0000 (75.0449)  time: 0.4374  data: 0.0021  max mem: 2378\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4645 s / it)\n",
            "* Acc@1 54.578 Acc@5 75.045 loss 2.315\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:20  Loss: 2.0592 (2.0592)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.7349  data: 0.3214  max mem: 2378\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:08  Loss: 2.1082 (2.0780)  Acc@1: 68.7500 (65.9091)  Acc@5: 75.0000 (76.7045)  time: 0.4667  data: 0.0304  max mem: 2378\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.3582 (2.2925)  Acc@1: 56.2500 (57.7381)  Acc@5: 75.0000 (73.5119)  time: 0.4401  data: 0.0027  max mem: 2378\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.2346 (2.2057)  Acc@1: 56.2500 (59.3258)  Acc@5: 75.0000 (74.8315)  time: 0.4382  data: 0.0023  max mem: 2378\n",
            "Test: [Task 8] Total time: 0:00:12 (0.4511 s / it)\n",
            "* Acc@1 59.326 Acc@5 74.831 loss 2.206\n",
            "Test: [Task 9]  [ 0/40]  eta: 0:00:34  Loss: 1.8562 (1.8562)  Acc@1: 75.0000 (75.0000)  Acc@5: 81.2500 (81.2500)  time: 0.8537  data: 0.4536  max mem: 2378\n",
            "Test: [Task 9]  [10/40]  eta: 0:00:14  Loss: 1.3858 (1.5586)  Acc@1: 75.0000 (72.7273)  Acc@5: 87.5000 (85.2273)  time: 0.4776  data: 0.0425  max mem: 2378\n",
            "Test: [Task 9]  [20/40]  eta: 0:00:09  Loss: 1.2988 (1.5850)  Acc@1: 75.0000 (72.6190)  Acc@5: 87.5000 (86.0119)  time: 0.4406  data: 0.0028  max mem: 2378\n",
            "Test: [Task 9]  [30/40]  eta: 0:00:04  Loss: 1.2451 (1.5071)  Acc@1: 81.2500 (75.4032)  Acc@5: 87.5000 (86.8952)  time: 0.4411  data: 0.0027  max mem: 2378\n",
            "Test: [Task 9]  [39/40]  eta: 0:00:00  Loss: 1.4978 (1.5010)  Acc@1: 75.0000 (75.1196)  Acc@5: 87.5000 (86.7624)  time: 0.4243  data: 0.0010  max mem: 2378\n",
            "Test: [Task 9] Total time: 0:00:17 (0.4443 s / it)\n",
            "* Acc@1 75.120 Acc@5 86.762 loss 1.501\n",
            "Test: [Task 10]  [ 0/44]  eta: 0:00:39  Loss: 1.3731 (1.3731)  Acc@1: 81.2500 (81.2500)  Acc@5: 87.5000 (87.5000)  time: 0.8996  data: 0.4997  max mem: 2378\n",
            "Test: [Task 10]  [10/44]  eta: 0:00:16  Loss: 1.7728 (1.7896)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (79.5455)  time: 0.4819  data: 0.0472  max mem: 2378\n",
            "Test: [Task 10]  [20/44]  eta: 0:00:11  Loss: 1.7986 (1.8167)  Acc@1: 62.5000 (66.9643)  Acc@5: 81.2500 (80.0595)  time: 0.4410  data: 0.0013  max mem: 2378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py cifar100_l2p --eval"
      ],
      "metadata": {
        "id": "dbiUfIHyQ02t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py TinyImagenet --eval"
      ],
      "metadata": {
        "id": "O6gAp7JXsW7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py Imagenet-R --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_s5jV_xsGF7",
        "outputId": "680780d8-b627-4ec4-f050-b92be9300b26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "Downloading from https://people.eecs.berkeley.edu/~hendrycks/imagenet-r.tar\n",
            "Using downloaded and verified file: /local_datasets/imagenet-r.tar\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='Imagenet-R', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', name='Imagenet-R', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=True, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=200)\n",
            "Loading checkpoint from: ./output/checkpoint/task1_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:01:08  Loss: 2.8129 (2.8129)  Acc@1: 62.5000 (62.5000)  Acc@5: 93.7500 (93.7500)  time: 1.6779  data: 0.4996  max mem: 1158\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:16  Loss: 2.5032 (2.4082)  Acc@1: 62.5000 (68.1818)  Acc@5: 93.7500 (92.6136)  time: 0.5311  data: 0.0474  max mem: 1159\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:10  Loss: 1.8888 (2.0743)  Acc@1: 75.0000 (72.9167)  Acc@5: 93.7500 (94.0476)  time: 0.4206  data: 0.0027  max mem: 1159\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 2.0549 (2.1880)  Acc@1: 68.7500 (70.5645)  Acc@5: 93.7500 (92.9435)  time: 0.4290  data: 0.0018  max mem: 1159\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2250 (2.1905)  Acc@1: 68.7500 (71.8462)  Acc@5: 93.7500 (92.3077)  time: 0.4303  data: 0.0004  max mem: 1159\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4574 s / it)\n",
            "* Acc@1 71.846 Acc@5 92.308 loss 2.190\n",
            "[Average accuracy till task1]\tAcc@1: 71.8462\tAcc@5: 92.3077\tLoss: 2.1905\n",
            "Loading checkpoint from: ./output/checkpoint/task2_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:44  Loss: 2.8991 (2.8991)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 1.0907  data: 0.6243  max mem: 1326\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.5544 (2.4487)  Acc@1: 62.5000 (60.2273)  Acc@5: 87.5000 (87.5000)  time: 0.5037  data: 0.0587  max mem: 1326\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:10  Loss: 1.8473 (2.0227)  Acc@1: 75.0000 (68.4524)  Acc@5: 93.7500 (90.4762)  time: 0.4508  data: 0.0013  max mem: 1326\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.8519 (2.1232)  Acc@1: 68.7500 (65.1210)  Acc@5: 87.5000 (88.5081)  time: 0.4598  data: 0.0011  max mem: 1326\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2136 (2.1528)  Acc@1: 62.5000 (64.7692)  Acc@5: 87.5000 (87.8462)  time: 0.4567  data: 0.0020  max mem: 1326\n",
            "Test: [Task 1] Total time: 0:00:19 (0.4708 s / it)\n",
            "* Acc@1 64.769 Acc@5 87.846 loss 2.153\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:40  Loss: 1.0352 (1.0352)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.8570  data: 0.4262  max mem: 1326\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:18  Loss: 1.1268 (1.3580)  Acc@1: 81.2500 (79.5455)  Acc@5: 93.7500 (93.1818)  time: 0.5042  data: 0.0407  max mem: 1326\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:13  Loss: 2.0074 (1.8725)  Acc@1: 75.0000 (72.3214)  Acc@5: 93.7500 (87.7976)  time: 0.4638  data: 0.0026  max mem: 1326\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:08  Loss: 2.1648 (1.9389)  Acc@1: 75.0000 (73.1855)  Acc@5: 87.5000 (87.7016)  time: 0.4550  data: 0.0017  max mem: 1326\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1175 (2.0206)  Acc@1: 68.7500 (70.7317)  Acc@5: 87.5000 (87.8049)  time: 0.4486  data: 0.0004  max mem: 1326\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0422 (1.9505)  Acc@1: 62.5000 (71.6000)  Acc@5: 87.5000 (88.0000)  time: 0.4453  data: 0.0003  max mem: 1326\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4651 s / it)\n",
            "* Acc@1 71.600 Acc@5 88.000 loss 1.951\n",
            "[Average accuracy till task2]\tAcc@1: 68.1846\tAcc@5: 87.9231\tLoss: 2.0517\tForgetting: 7.0769\tBackward: -7.0769\n",
            "Loading checkpoint from: ./output/checkpoint/task3_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:42  Loss: 3.0207 (3.0207)  Acc@1: 62.5000 (62.5000)  Acc@5: 81.2500 (81.2500)  time: 1.0471  data: 0.5965  max mem: 1327\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.7513 (2.6899)  Acc@1: 56.2500 (55.6818)  Acc@5: 81.2500 (78.4091)  time: 0.4877  data: 0.0547  max mem: 1327\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0140 (2.2036)  Acc@1: 68.7500 (65.7738)  Acc@5: 87.5000 (83.9286)  time: 0.4323  data: 0.0004  max mem: 1327\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.8679 (2.2373)  Acc@1: 68.7500 (63.7097)  Acc@5: 87.5000 (83.2661)  time: 0.4327  data: 0.0025  max mem: 1327\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2206 (2.2442)  Acc@1: 62.5000 (63.8462)  Acc@5: 81.2500 (82.6154)  time: 0.4258  data: 0.0025  max mem: 1327\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4458 s / it)\n",
            "* Acc@1 63.846 Acc@5 82.615 loss 2.244\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:38  Loss: 0.9311 (0.9311)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.8131  data: 0.4123  max mem: 1327\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.3228 (1.4388)  Acc@1: 81.2500 (79.5455)  Acc@5: 93.7500 (90.3409)  time: 0.4686  data: 0.0390  max mem: 1327\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.9351 (1.9605)  Acc@1: 68.7500 (70.8333)  Acc@5: 87.5000 (84.2262)  time: 0.4357  data: 0.0027  max mem: 1327\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.2262 (2.0376)  Acc@1: 68.7500 (70.9677)  Acc@5: 87.5000 (85.2823)  time: 0.4381  data: 0.0023  max mem: 1327\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2365 (2.1530)  Acc@1: 62.5000 (67.6829)  Acc@5: 87.5000 (83.9939)  time: 0.4407  data: 0.0009  max mem: 1327\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.2262 (2.0714)  Acc@1: 56.2500 (68.1333)  Acc@5: 81.2500 (84.2667)  time: 0.4405  data: 0.0006  max mem: 1327\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4484 s / it)\n",
            "* Acc@1 68.133 Acc@5 84.267 loss 2.071\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:31  Loss: 2.7883 (2.7883)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (87.5000)  time: 1.1738  data: 0.7599  max mem: 1327\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.6382 (1.7531)  Acc@1: 62.5000 (61.3636)  Acc@5: 93.7500 (90.3409)  time: 0.5448  data: 0.1114  max mem: 1327\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.6711 (1.8206)  Acc@1: 62.5000 (60.4167)  Acc@5: 93.7500 (88.9881)  time: 0.4642  data: 0.0235  max mem: 1327\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.7242 (1.8649)  Acc@1: 62.5000 (58.7822)  Acc@5: 93.7500 (89.6956)  time: 0.4417  data: 0.0003  max mem: 1327\n",
            "Test: [Task 3] Total time: 0:00:13 (0.4873 s / it)\n",
            "* Acc@1 58.782 Acc@5 89.696 loss 1.865\n",
            "[Average accuracy till task3]\tAcc@1: 63.5872\tAcc@5: 85.5259\tLoss: 2.0602\tForgetting: 5.7333\tBackward: -5.7333\n",
            "Loading checkpoint from: ./output/checkpoint/task4_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:45  Loss: 2.9212 (2.9212)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 1.1099  data: 0.6508  max mem: 1327\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.6791 (2.6872)  Acc@1: 43.7500 (48.2955)  Acc@5: 81.2500 (74.4318)  time: 0.5024  data: 0.0607  max mem: 1327\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0007 (2.2151)  Acc@1: 56.2500 (58.6310)  Acc@5: 87.5000 (81.8452)  time: 0.4428  data: 0.0011  max mem: 1327\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.8298 (2.2529)  Acc@1: 62.5000 (57.6613)  Acc@5: 87.5000 (80.4435)  time: 0.4441  data: 0.0017  max mem: 1327\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.2340 (2.2691)  Acc@1: 56.2500 (58.1538)  Acc@5: 75.0000 (79.3846)  time: 0.4370  data: 0.0016  max mem: 1327\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4580 s / it)\n",
            "* Acc@1 58.154 Acc@5 79.385 loss 2.269\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:36  Loss: 1.0037 (1.0037)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.7784  data: 0.3730  max mem: 1327\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.2320 (1.4801)  Acc@1: 87.5000 (78.4091)  Acc@5: 93.7500 (89.2045)  time: 0.4730  data: 0.0346  max mem: 1327\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8390 (1.9986)  Acc@1: 68.7500 (66.6667)  Acc@5: 87.5000 (82.1429)  time: 0.4426  data: 0.0022  max mem: 1327\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.3694 (2.1176)  Acc@1: 56.2500 (66.3306)  Acc@5: 81.2500 (81.6532)  time: 0.4424  data: 0.0029  max mem: 1327\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.3694 (2.2281)  Acc@1: 56.2500 (62.9573)  Acc@5: 81.2500 (80.1829)  time: 0.4417  data: 0.0013  max mem: 1327\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.3441 (2.1411)  Acc@1: 56.2500 (63.8667)  Acc@5: 81.2500 (80.6667)  time: 0.4396  data: 0.0003  max mem: 1327\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4504 s / it)\n",
            "* Acc@1 63.867 Acc@5 80.667 loss 2.141\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:32  Loss: 2.9388 (2.9388)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 1.1964  data: 0.7893  max mem: 1327\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.7004 (1.8509)  Acc@1: 62.5000 (59.0909)  Acc@5: 93.7500 (88.6364)  time: 0.5577  data: 0.1275  max mem: 1327\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7180 (1.8874)  Acc@1: 62.5000 (59.2262)  Acc@5: 93.7500 (86.9048)  time: 0.4650  data: 0.0308  max mem: 1327\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.7180 (1.9356)  Acc@1: 56.2500 (57.3770)  Acc@5: 93.7500 (87.3536)  time: 0.4310  data: 0.0003  max mem: 1327\n",
            "Test: [Task 3] Total time: 0:00:13 (0.4837 s / it)\n",
            "* Acc@1 57.377 Acc@5 87.354 loss 1.936\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:31  Loss: 2.3813 (2.3813)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 0.8650  data: 0.4525  max mem: 1327\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.1479 (1.8624)  Acc@1: 62.5000 (58.5227)  Acc@5: 87.5000 (86.3636)  time: 0.4790  data: 0.0494  max mem: 1327\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1259 (2.0558)  Acc@1: 62.5000 (57.1429)  Acc@5: 81.2500 (82.1429)  time: 0.4410  data: 0.0051  max mem: 1327\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1259 (2.0700)  Acc@1: 62.5000 (58.4677)  Acc@5: 81.2500 (84.0726)  time: 0.4417  data: 0.0010  max mem: 1327\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1323 (2.0768)  Acc@1: 56.2500 (58.6806)  Acc@5: 87.5000 (84.0278)  time: 0.4424  data: 0.0006  max mem: 1327\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4562 s / it)\n",
            "* Acc@1 58.681 Acc@5 84.028 loss 2.077\n",
            "[Average accuracy till task4]\tAcc@1: 59.5195\tAcc@5: 82.8582\tLoss: 2.1056\tForgetting: 7.6103\tBackward: -7.6103\n",
            "Loading checkpoint from: ./output/checkpoint/task5_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:39  Loss: 2.5753 (2.5753)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.9652  data: 0.5010  max mem: 1327\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.4745 (2.4950)  Acc@1: 50.0000 (51.1364)  Acc@5: 75.0000 (76.1364)  time: 0.4879  data: 0.0469  max mem: 1327\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9920 (2.1204)  Acc@1: 56.2500 (59.2262)  Acc@5: 81.2500 (81.2500)  time: 0.4420  data: 0.0010  max mem: 1327\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7868 (2.1373)  Acc@1: 62.5000 (58.4677)  Acc@5: 87.5000 (80.4435)  time: 0.4438  data: 0.0013  max mem: 1327\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0508 (2.1310)  Acc@1: 56.2500 (59.3846)  Acc@5: 81.2500 (80.1538)  time: 0.4365  data: 0.0012  max mem: 1327\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4538 s / it)\n",
            "* Acc@1 59.385 Acc@5 80.154 loss 2.131\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:34  Loss: 1.1189 (1.1189)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.7445  data: 0.3352  max mem: 1327\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1189 (1.3813)  Acc@1: 81.2500 (76.7045)  Acc@5: 93.7500 (87.5000)  time: 0.4704  data: 0.0314  max mem: 1327\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.7779 (1.9240)  Acc@1: 68.7500 (64.8810)  Acc@5: 87.5000 (81.2500)  time: 0.4432  data: 0.0027  max mem: 1327\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0319 (1.9470)  Acc@1: 62.5000 (66.9355)  Acc@5: 87.5000 (82.6613)  time: 0.4432  data: 0.0027  max mem: 1327\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1133 (2.0751)  Acc@1: 62.5000 (63.5671)  Acc@5: 87.5000 (80.7927)  time: 0.4423  data: 0.0009  max mem: 1327\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1742 (2.0058)  Acc@1: 56.2500 (64.5333)  Acc@5: 75.0000 (80.9333)  time: 0.4393  data: 0.0006  max mem: 1327\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4501 s / it)\n",
            "* Acc@1 64.533 Acc@5 80.933 loss 2.006\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:31  Loss: 2.6615 (2.6615)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 1.1732  data: 0.7689  max mem: 1327\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.7814 (1.8744)  Acc@1: 62.5000 (59.6591)  Acc@5: 87.5000 (88.6364)  time: 0.5226  data: 0.0934  max mem: 1327\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7874 (1.9149)  Acc@1: 62.5000 (58.9286)  Acc@5: 87.5000 (86.0119)  time: 0.4486  data: 0.0131  max mem: 1327\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8718 (2.0098)  Acc@1: 56.2500 (55.5035)  Acc@5: 87.5000 (84.7775)  time: 0.4332  data: 0.0003  max mem: 1327\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4719 s / it)\n",
            "* Acc@1 55.504 Acc@5 84.778 loss 2.010\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:43  Loss: 2.7613 (2.7613)  Acc@1: 37.5000 (37.5000)  Acc@5: 62.5000 (62.5000)  time: 1.1962  data: 0.7930  max mem: 1327\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:13  Loss: 2.3557 (2.0714)  Acc@1: 56.2500 (56.2500)  Acc@5: 81.2500 (78.4091)  time: 0.5114  data: 0.0800  max mem: 1327\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1041 (2.1907)  Acc@1: 50.0000 (52.9762)  Acc@5: 81.2500 (77.6786)  time: 0.4418  data: 0.0050  max mem: 1327\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1041 (2.1482)  Acc@1: 56.2500 (55.4435)  Acc@5: 81.2500 (80.8468)  time: 0.4404  data: 0.0011  max mem: 1327\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1041 (2.1509)  Acc@1: 56.2500 (55.9028)  Acc@5: 87.5000 (81.0764)  time: 0.4403  data: 0.0008  max mem: 1327\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4646 s / it)\n",
            "* Acc@1 55.903 Acc@5 81.076 loss 2.151\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:47  Loss: 1.3606 (1.3606)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 1.0511  data: 0.6363  max mem: 1327\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:17  Loss: 1.8693 (2.0054)  Acc@1: 75.0000 (70.4545)  Acc@5: 81.2500 (86.3636)  time: 0.4941  data: 0.0584  max mem: 1327\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 1.9922 (1.9592)  Acc@1: 68.7500 (67.8571)  Acc@5: 81.2500 (85.1190)  time: 0.4397  data: 0.0007  max mem: 1327\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.0383 (2.0809)  Acc@1: 62.5000 (63.9113)  Acc@5: 75.0000 (80.8468)  time: 0.4409  data: 0.0013  max mem: 1327\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4369 (2.0745)  Acc@1: 56.2500 (62.8049)  Acc@5: 75.0000 (80.7927)  time: 0.4411  data: 0.0013  max mem: 1327\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.0333 (2.0381)  Acc@1: 62.5000 (63.6872)  Acc@5: 75.0000 (80.8659)  time: 0.4373  data: 0.0013  max mem: 1327\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4542 s / it)\n",
            "* Acc@1 63.687 Acc@5 80.866 loss 2.038\n",
            "[Average accuracy till task5]\tAcc@1: 59.8023\tAcc@5: 81.5614\tLoss: 2.0671\tForgetting: 6.3962\tBackward: -6.3962\n",
            "Loading checkpoint from: ./output/checkpoint/task6_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:32  Loss: 2.4021 (2.4021)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 0.7886  data: 0.3319  max mem: 1329\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.4021 (2.4954)  Acc@1: 50.0000 (50.5682)  Acc@5: 75.0000 (74.4318)  time: 0.4678  data: 0.0331  max mem: 1329\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0439 (2.1049)  Acc@1: 56.2500 (60.1190)  Acc@5: 81.2500 (80.0595)  time: 0.4379  data: 0.0032  max mem: 1329\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.8216 (2.1127)  Acc@1: 62.5000 (58.6694)  Acc@5: 81.2500 (78.6290)  time: 0.4415  data: 0.0018  max mem: 1329\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0785 (2.1301)  Acc@1: 56.2500 (58.4615)  Acc@5: 75.0000 (78.4615)  time: 0.4358  data: 0.0003  max mem: 1329\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4483 s / it)\n",
            "* Acc@1 58.462 Acc@5 78.462 loss 2.130\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:52  Loss: 1.0775 (1.0775)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 1.1217  data: 0.7184  max mem: 1329\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:18  Loss: 1.0775 (1.3438)  Acc@1: 81.2500 (77.2727)  Acc@5: 93.7500 (88.0682)  time: 0.5072  data: 0.0664  max mem: 1329\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.6870 (1.8954)  Acc@1: 68.7500 (65.7738)  Acc@5: 87.5000 (81.2500)  time: 0.4468  data: 0.0008  max mem: 1329\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.1157 (1.9681)  Acc@1: 62.5000 (65.7258)  Acc@5: 81.2500 (81.4516)  time: 0.4472  data: 0.0018  max mem: 1329\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2513 (2.0892)  Acc@1: 56.2500 (62.3476)  Acc@5: 81.2500 (79.8780)  time: 0.4457  data: 0.0028  max mem: 1329\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.2513 (2.0263)  Acc@1: 56.2500 (62.9333)  Acc@5: 75.0000 (79.8667)  time: 0.4422  data: 0.0027  max mem: 1329\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4609 s / it)\n",
            "* Acc@1 62.933 Acc@5 79.867 loss 2.026\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:24  Loss: 2.6889 (2.6889)  Acc@1: 43.7500 (43.7500)  Acc@5: 75.0000 (75.0000)  time: 0.8892  data: 0.4863  max mem: 1329\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.8613 (1.8697)  Acc@1: 62.5000 (57.3864)  Acc@5: 87.5000 (86.3636)  time: 0.4826  data: 0.0460  max mem: 1329\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7865 (1.8875)  Acc@1: 62.5000 (57.4405)  Acc@5: 87.5000 (85.4167)  time: 0.4414  data: 0.0022  max mem: 1329\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8034 (1.9873)  Acc@1: 56.2500 (55.0351)  Acc@5: 87.5000 (84.7775)  time: 0.4339  data: 0.0014  max mem: 1329\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4554 s / it)\n",
            "* Acc@1 55.035 Acc@5 84.778 loss 1.987\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:25  Loss: 2.5869 (2.5869)  Acc@1: 37.5000 (37.5000)  Acc@5: 68.7500 (68.7500)  time: 0.7094  data: 0.3043  max mem: 1329\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.2529 (2.0438)  Acc@1: 43.7500 (55.6818)  Acc@5: 81.2500 (79.5455)  time: 0.4661  data: 0.0339  max mem: 1329\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.0475 (2.1448)  Acc@1: 56.2500 (55.0595)  Acc@5: 81.2500 (77.6786)  time: 0.4404  data: 0.0070  max mem: 1329\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.0475 (2.1118)  Acc@1: 56.2500 (57.0565)  Acc@5: 81.2500 (80.2419)  time: 0.4389  data: 0.0065  max mem: 1329\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.0836 (2.1160)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (80.5556)  time: 0.4386  data: 0.0058  max mem: 1329\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4488 s / it)\n",
            "* Acc@1 56.250 Acc@5 80.556 loss 2.116\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:37  Loss: 1.5374 (1.5374)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.8299  data: 0.4155  max mem: 1329\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.0608 (2.1210)  Acc@1: 68.7500 (63.0682)  Acc@5: 87.5000 (84.0909)  time: 0.4740  data: 0.0383  max mem: 1329\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.0608 (2.0592)  Acc@1: 62.5000 (63.0952)  Acc@5: 81.2500 (83.6310)  time: 0.4396  data: 0.0020  max mem: 1329\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.0868 (2.1094)  Acc@1: 62.5000 (60.8871)  Acc@5: 81.2500 (80.4435)  time: 0.4418  data: 0.0019  max mem: 1329\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4164 (2.1084)  Acc@1: 50.0000 (59.4512)  Acc@5: 75.0000 (80.0305)  time: 0.4436  data: 0.0004  max mem: 1329\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.0569 (2.0712)  Acc@1: 50.0000 (60.4749)  Acc@5: 75.0000 (80.3073)  time: 0.4397  data: 0.0004  max mem: 1329\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4501 s / it)\n",
            "* Acc@1 60.475 Acc@5 80.307 loss 2.071\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:33  Loss: 2.3000 (2.3000)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 0.9588  data: 0.5510  max mem: 1329\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:12  Loss: 2.3000 (2.0083)  Acc@1: 56.2500 (60.7955)  Acc@5: 87.5000 (84.0909)  time: 0.5117  data: 0.0781  max mem: 1329\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 1.9122 (2.0149)  Acc@1: 56.2500 (58.9286)  Acc@5: 87.5000 (83.9286)  time: 0.4565  data: 0.0165  max mem: 1329\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 1.8476 (1.8950)  Acc@1: 62.5000 (64.3145)  Acc@5: 87.5000 (84.2742)  time: 0.4456  data: 0.0013  max mem: 1329\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7301 (1.8804)  Acc@1: 75.0000 (65.4122)  Acc@5: 87.5000 (84.7670)  time: 0.4433  data: 0.0006  max mem: 1329\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4678 s / it)\n",
            "* Acc@1 65.412 Acc@5 84.767 loss 1.880\n",
            "[Average accuracy till task6]\tAcc@1: 59.7612\tAcc@5: 81.4559\tLoss: 2.0352\tForgetting: 6.2882\tBackward: -6.2882\n",
            "Loading checkpoint from: ./output/checkpoint/task7_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:39  Loss: 2.6268 (2.6268)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.9742  data: 0.5219  max mem: 1329\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.4485 (2.4059)  Acc@1: 50.0000 (49.4318)  Acc@5: 75.0000 (75.5682)  time: 0.4875  data: 0.0486  max mem: 1329\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9147 (2.0507)  Acc@1: 56.2500 (57.7381)  Acc@5: 81.2500 (80.3571)  time: 0.4409  data: 0.0009  max mem: 1329\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7557 (2.0773)  Acc@1: 68.7500 (57.4597)  Acc@5: 81.2500 (79.4355)  time: 0.4421  data: 0.0031  max mem: 1329\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.0201 (2.0924)  Acc@1: 62.5000 (58.6154)  Acc@5: 81.2500 (79.2308)  time: 0.4341  data: 0.0030  max mem: 1329\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4523 s / it)\n",
            "* Acc@1 58.615 Acc@5 79.231 loss 2.092\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:38  Loss: 1.0016 (1.0016)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.8268  data: 0.4179  max mem: 1329\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.0190 (1.3555)  Acc@1: 81.2500 (75.5682)  Acc@5: 87.5000 (87.5000)  time: 0.4761  data: 0.0395  max mem: 1329\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8335 (1.8390)  Acc@1: 68.7500 (67.5595)  Acc@5: 81.2500 (82.1429)  time: 0.4409  data: 0.0018  max mem: 1329\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.1344 (1.9022)  Acc@1: 62.5000 (67.9435)  Acc@5: 81.2500 (81.8548)  time: 0.4414  data: 0.0015  max mem: 1329\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.2393 (2.0246)  Acc@1: 62.5000 (64.1768)  Acc@5: 81.2500 (80.1829)  time: 0.4427  data: 0.0009  max mem: 1329\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1676 (1.9679)  Acc@1: 62.5000 (65.0667)  Acc@5: 75.0000 (80.1333)  time: 0.4409  data: 0.0005  max mem: 1329\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4513 s / it)\n",
            "* Acc@1 65.067 Acc@5 80.133 loss 1.968\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:30  Loss: 2.5866 (2.5866)  Acc@1: 56.2500 (56.2500)  Acc@5: 87.5000 (87.5000)  time: 1.1366  data: 0.7336  max mem: 1329\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.6910 (1.8417)  Acc@1: 56.2500 (57.9545)  Acc@5: 87.5000 (87.5000)  time: 0.5222  data: 0.0898  max mem: 1329\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7726 (1.9007)  Acc@1: 50.0000 (57.4405)  Acc@5: 81.2500 (83.6310)  time: 0.4523  data: 0.0129  max mem: 1329\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8199 (1.9914)  Acc@1: 50.0000 (55.0351)  Acc@5: 81.2500 (83.1382)  time: 0.4367  data: 0.0003  max mem: 1329\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4744 s / it)\n",
            "* Acc@1 55.035 Acc@5 83.138 loss 1.991\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:35  Loss: 2.8800 (2.8800)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 0.9974  data: 0.5925  max mem: 1329\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:13  Loss: 2.4860 (2.1216)  Acc@1: 43.7500 (55.1136)  Acc@5: 81.2500 (75.5682)  time: 0.5090  data: 0.0751  max mem: 1329\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1931 (2.2724)  Acc@1: 50.0000 (52.6786)  Acc@5: 81.2500 (73.8095)  time: 0.4525  data: 0.0119  max mem: 1329\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1931 (2.2445)  Acc@1: 56.2500 (54.0323)  Acc@5: 81.2500 (77.2177)  time: 0.4441  data: 0.0004  max mem: 1329\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1931 (2.2467)  Acc@1: 56.2500 (53.6458)  Acc@5: 81.2500 (76.7361)  time: 0.4438  data: 0.0003  max mem: 1329\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4661 s / it)\n",
            "* Acc@1 53.646 Acc@5 76.736 loss 2.247\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:37  Loss: 1.6073 (1.6073)  Acc@1: 81.2500 (81.2500)  Acc@5: 81.2500 (81.2500)  time: 0.8257  data: 0.4286  max mem: 1329\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.0510 (2.1353)  Acc@1: 62.5000 (67.0455)  Acc@5: 81.2500 (84.6591)  time: 0.4784  data: 0.0406  max mem: 1329\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.1318 (2.0852)  Acc@1: 62.5000 (63.6905)  Acc@5: 81.2500 (82.1429)  time: 0.4435  data: 0.0019  max mem: 1329\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.2009 (2.1809)  Acc@1: 50.0000 (59.8790)  Acc@5: 75.0000 (77.4194)  time: 0.4426  data: 0.0026  max mem: 1329\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4364 (2.1818)  Acc@1: 50.0000 (58.5366)  Acc@5: 75.0000 (77.4390)  time: 0.4422  data: 0.0017  max mem: 1329\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.1937 (2.1542)  Acc@1: 50.0000 (58.9385)  Acc@5: 75.0000 (77.5140)  time: 0.4377  data: 0.0017  max mem: 1329\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4510 s / it)\n",
            "* Acc@1 58.939 Acc@5 77.514 loss 2.154\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:27  Loss: 2.3714 (2.3714)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 0.7914  data: 0.3856  max mem: 1329\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:12  Loss: 2.3714 (2.3698)  Acc@1: 50.0000 (48.2955)  Acc@5: 75.0000 (74.4318)  time: 0.4860  data: 0.0515  max mem: 1329\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:06  Loss: 2.2070 (2.3339)  Acc@1: 43.7500 (50.0000)  Acc@5: 75.0000 (75.5952)  time: 0.4483  data: 0.0111  max mem: 1329\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0067 (2.0690)  Acc@1: 75.0000 (59.4758)  Acc@5: 87.5000 (79.0323)  time: 0.4424  data: 0.0025  max mem: 1329\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7181 (2.0069)  Acc@1: 75.0000 (61.6487)  Acc@5: 87.5000 (80.6452)  time: 0.4405  data: 0.0013  max mem: 1329\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4574 s / it)\n",
            "* Acc@1 61.649 Acc@5 80.645 loss 2.007\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:25  Loss: 2.2426 (2.2426)  Acc@1: 75.0000 (75.0000)  Acc@5: 75.0000 (75.0000)  time: 0.7374  data: 0.3300  max mem: 1329\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:13  Loss: 2.4538 (2.3822)  Acc@1: 56.2500 (58.5227)  Acc@5: 75.0000 (75.0000)  time: 0.5300  data: 0.1001  max mem: 1329\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4538 (2.4102)  Acc@1: 56.2500 (59.8214)  Acc@5: 75.0000 (76.1905)  time: 0.4761  data: 0.0389  max mem: 1329\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4225 (2.3242)  Acc@1: 62.5000 (62.5000)  Acc@5: 81.2500 (77.2177)  time: 0.4435  data: 0.0006  max mem: 1329\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.8790 (2.2077)  Acc@1: 68.7500 (64.4524)  Acc@5: 87.5000 (78.8151)  time: 0.4424  data: 0.0005  max mem: 1329\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4726 s / it)\n",
            "* Acc@1 64.452 Acc@5 78.815 loss 2.208\n",
            "[Average accuracy till task7]\tAcc@1: 59.6290\tAcc@5: 79.4589\tLoss: 2.0953\tForgetting: 6.1763\tBackward: -6.1763\n",
            "Loading checkpoint from: ./output/checkpoint/task8_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:41  Loss: 2.6499 (2.6499)  Acc@1: 56.2500 (56.2500)  Acc@5: 75.0000 (75.0000)  time: 1.0126  data: 0.5992  max mem: 1329\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:15  Loss: 2.4174 (2.3366)  Acc@1: 56.2500 (51.1364)  Acc@5: 75.0000 (75.0000)  time: 0.4948  data: 0.0573  max mem: 1329\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 1.9478 (2.0653)  Acc@1: 56.2500 (57.4405)  Acc@5: 75.0000 (78.8690)  time: 0.4432  data: 0.0018  max mem: 1329\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.7993 (2.1060)  Acc@1: 62.5000 (56.4516)  Acc@5: 81.2500 (77.4194)  time: 0.4438  data: 0.0012  max mem: 1329\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 1.9353 (2.1070)  Acc@1: 62.5000 (58.0000)  Acc@5: 81.2500 (77.5385)  time: 0.4367  data: 0.0011  max mem: 1329\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4555 s / it)\n",
            "* Acc@1 58.000 Acc@5 77.538 loss 2.107\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:42  Loss: 0.9751 (0.9751)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.9133  data: 0.4990  max mem: 1329\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:17  Loss: 1.1781 (1.4313)  Acc@1: 81.2500 (76.7045)  Acc@5: 87.5000 (85.7955)  time: 0.4858  data: 0.0460  max mem: 1329\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.9016 (1.8928)  Acc@1: 68.7500 (68.4524)  Acc@5: 81.2500 (81.5476)  time: 0.4427  data: 0.0011  max mem: 1329\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0960 (1.9204)  Acc@1: 62.5000 (67.9435)  Acc@5: 81.2500 (81.8548)  time: 0.4417  data: 0.0020  max mem: 1329\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1342 (2.0477)  Acc@1: 62.5000 (63.5671)  Acc@5: 81.2500 (79.4207)  time: 0.4415  data: 0.0020  max mem: 1329\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0629 (1.9764)  Acc@1: 62.5000 (64.9333)  Acc@5: 75.0000 (79.4667)  time: 0.4398  data: 0.0009  max mem: 1329\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4527 s / it)\n",
            "* Acc@1 64.933 Acc@5 79.467 loss 1.976\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:26  Loss: 2.5675 (2.5675)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.9962  data: 0.5774  max mem: 1329\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:09  Loss: 1.7334 (1.8420)  Acc@1: 50.0000 (55.1136)  Acc@5: 87.5000 (84.6591)  time: 0.5489  data: 0.1185  max mem: 1329\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7753 (1.9114)  Acc@1: 50.0000 (55.6548)  Acc@5: 81.2500 (81.2500)  time: 0.4712  data: 0.0366  max mem: 1329\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8660 (1.9929)  Acc@1: 50.0000 (52.9274)  Acc@5: 81.2500 (81.2646)  time: 0.4319  data: 0.0009  max mem: 1329\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4808 s / it)\n",
            "* Acc@1 52.927 Acc@5 81.265 loss 1.993\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:31  Loss: 3.0528 (3.0528)  Acc@1: 37.5000 (37.5000)  Acc@5: 43.7500 (43.7500)  time: 0.8788  data: 0.4705  max mem: 1329\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.5921 (2.2229)  Acc@1: 37.5000 (52.2727)  Acc@5: 75.0000 (73.8636)  time: 0.4778  data: 0.0473  max mem: 1329\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.2073 (2.3495)  Acc@1: 50.0000 (50.8929)  Acc@5: 75.0000 (71.7262)  time: 0.4383  data: 0.0032  max mem: 1329\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.2073 (2.3131)  Acc@1: 50.0000 (51.6129)  Acc@5: 75.0000 (75.2016)  time: 0.4393  data: 0.0012  max mem: 1329\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.2073 (2.3162)  Acc@1: 50.0000 (51.7361)  Acc@5: 75.0000 (74.6528)  time: 0.4394  data: 0.0006  max mem: 1329\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4540 s / it)\n",
            "* Acc@1 51.736 Acc@5 74.653 loss 2.316\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:52  Loss: 1.6310 (1.6310)  Acc@1: 81.2500 (81.2500)  Acc@5: 81.2500 (81.2500)  time: 1.1682  data: 0.7697  max mem: 1329\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:17  Loss: 2.0798 (2.1542)  Acc@1: 62.5000 (67.0455)  Acc@5: 81.2500 (84.6591)  time: 0.5058  data: 0.0725  max mem: 1329\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.0798 (2.0838)  Acc@1: 62.5000 (63.6905)  Acc@5: 81.2500 (81.8452)  time: 0.4400  data: 0.0020  max mem: 1329\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1553 (2.1996)  Acc@1: 50.0000 (59.8790)  Acc@5: 75.0000 (76.8145)  time: 0.4404  data: 0.0008  max mem: 1329\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.3929 (2.2037)  Acc@1: 50.0000 (57.9268)  Acc@5: 75.0000 (77.1341)  time: 0.4407  data: 0.0011  max mem: 1329\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.2243 (2.1811)  Acc@1: 50.0000 (58.2402)  Acc@5: 75.0000 (77.2346)  time: 0.4368  data: 0.0011  max mem: 1329\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4561 s / it)\n",
            "* Acc@1 58.240 Acc@5 77.235 loss 2.181\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:29  Loss: 2.4428 (2.4428)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 0.8561  data: 0.4478  max mem: 1329\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:12  Loss: 2.4428 (2.4763)  Acc@1: 43.7500 (47.7273)  Acc@5: 68.7500 (73.2955)  time: 0.4809  data: 0.0470  max mem: 1329\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:06  Loss: 2.3245 (2.4221)  Acc@1: 43.7500 (49.1071)  Acc@5: 68.7500 (73.8095)  time: 0.4433  data: 0.0040  max mem: 1329\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0010 (2.1262)  Acc@1: 68.7500 (58.4677)  Acc@5: 81.2500 (77.8226)  time: 0.4423  data: 0.0012  max mem: 1329\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7173 (2.0618)  Acc@1: 75.0000 (60.5735)  Acc@5: 87.5000 (79.3907)  time: 0.4399  data: 0.0008  max mem: 1329\n",
            "Test: [Task 6] Total time: 0:00:15 (0.4552 s / it)\n",
            "* Acc@1 60.573 Acc@5 79.391 loss 2.062\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:24  Loss: 2.4080 (2.4080)  Acc@1: 75.0000 (75.0000)  Acc@5: 75.0000 (75.0000)  time: 0.7069  data: 0.2901  max mem: 1329\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:13  Loss: 2.4745 (2.4079)  Acc@1: 62.5000 (57.9545)  Acc@5: 75.0000 (73.8636)  time: 0.5300  data: 0.0964  max mem: 1329\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4877 (2.4572)  Acc@1: 56.2500 (57.4405)  Acc@5: 75.0000 (74.4048)  time: 0.4777  data: 0.0398  max mem: 1329\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4802 (2.3524)  Acc@1: 56.2500 (60.0806)  Acc@5: 75.0000 (75.8065)  time: 0.4427  data: 0.0015  max mem: 1329\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.8723 (2.2263)  Acc@1: 68.7500 (62.4776)  Acc@5: 81.2500 (77.3788)  time: 0.4404  data: 0.0006  max mem: 1329\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4703 s / it)\n",
            "* Acc@1 62.478 Acc@5 77.379 loss 2.226\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:24  Loss: 1.9272 (1.9272)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 0.8880  data: 0.4651  max mem: 1329\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:08  Loss: 2.0150 (1.9356)  Acc@1: 68.7500 (68.1818)  Acc@5: 81.2500 (78.9773)  time: 0.4833  data: 0.0468  max mem: 1329\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.0733 (2.1500)  Acc@1: 62.5000 (63.3929)  Acc@5: 81.2500 (76.4881)  time: 0.4432  data: 0.0048  max mem: 1329\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.0150 (2.0551)  Acc@1: 68.7500 (64.9438)  Acc@5: 81.2500 (78.2022)  time: 0.4406  data: 0.0035  max mem: 1329\n",
            "Test: [Task 8] Total time: 0:00:12 (0.4591 s / it)\n",
            "* Acc@1 64.944 Acc@5 78.202 loss 2.055\n",
            "[Average accuracy till task8]\tAcc@1: 59.2290\tAcc@5: 78.1411\tLoss: 2.1146\tForgetting: 6.5104\tBackward: -6.5104\n",
            "Loading checkpoint from: ./output/checkpoint/task9_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:32  Loss: 2.4918 (2.4918)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.8044  data: 0.4047  max mem: 1329\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.3498 (2.3550)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (73.8636)  time: 0.4747  data: 0.0374  max mem: 1329\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0566 (2.0609)  Acc@1: 56.2500 (57.1429)  Acc@5: 75.0000 (76.7857)  time: 0.4418  data: 0.0022  max mem: 1329\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:04  Loss: 1.7757 (2.1099)  Acc@1: 56.2500 (56.0484)  Acc@5: 75.0000 (76.0081)  time: 0.4423  data: 0.0020  max mem: 1329\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 1.9226 (2.1063)  Acc@1: 60.0000 (57.3846)  Acc@5: 75.0000 (76.1538)  time: 0.4352  data: 0.0003  max mem: 1329\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4503 s / it)\n",
            "* Acc@1 57.385 Acc@5 76.154 loss 2.106\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:52  Loss: 0.9917 (0.9917)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 1.1083  data: 0.6953  max mem: 1329\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:18  Loss: 1.1954 (1.4226)  Acc@1: 75.0000 (73.2955)  Acc@5: 87.5000 (86.3636)  time: 0.5022  data: 0.0670  max mem: 1329\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.8327 (1.8836)  Acc@1: 68.7500 (66.0714)  Acc@5: 81.2500 (81.5476)  time: 0.4418  data: 0.0032  max mem: 1329\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 2.0562 (1.9186)  Acc@1: 62.5000 (66.5323)  Acc@5: 75.0000 (81.6532)  time: 0.4421  data: 0.0015  max mem: 1329\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.0999 (2.0632)  Acc@1: 62.5000 (62.3476)  Acc@5: 75.0000 (78.9634)  time: 0.4415  data: 0.0019  max mem: 1329\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.0562 (2.0025)  Acc@1: 62.5000 (63.6000)  Acc@5: 75.0000 (79.0667)  time: 0.4388  data: 0.0017  max mem: 1329\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4564 s / it)\n",
            "* Acc@1 63.600 Acc@5 79.067 loss 2.002\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:22  Loss: 2.5486 (2.5486)  Acc@1: 50.0000 (50.0000)  Acc@5: 81.2500 (81.2500)  time: 0.8393  data: 0.4240  max mem: 1329\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.7438 (1.8550)  Acc@1: 50.0000 (55.6818)  Acc@5: 87.5000 (85.7955)  time: 0.4768  data: 0.0402  max mem: 1329\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.7640 (1.9338)  Acc@1: 56.2500 (56.2500)  Acc@5: 81.2500 (82.1429)  time: 0.4402  data: 0.0025  max mem: 1329\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.8419 (2.0211)  Acc@1: 50.0000 (53.8642)  Acc@5: 81.2500 (81.4988)  time: 0.4331  data: 0.0017  max mem: 1329\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4524 s / it)\n",
            "* Acc@1 53.864 Acc@5 81.499 loss 2.021\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:24  Loss: 2.9006 (2.9006)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 0.6818  data: 0.2755  max mem: 1329\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.5418 (2.1742)  Acc@1: 43.7500 (52.8409)  Acc@5: 81.2500 (74.4318)  time: 0.4708  data: 0.0375  max mem: 1329\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.2069 (2.3178)  Acc@1: 50.0000 (50.8929)  Acc@5: 75.0000 (71.7262)  time: 0.4452  data: 0.0083  max mem: 1329\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.2069 (2.2689)  Acc@1: 56.2500 (52.6210)  Acc@5: 75.0000 (75.0000)  time: 0.4408  data: 0.0025  max mem: 1329\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.2069 (2.2755)  Acc@1: 56.2500 (52.4306)  Acc@5: 75.0000 (74.8264)  time: 0.4415  data: 0.0020  max mem: 1329\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4524 s / it)\n",
            "* Acc@1 52.431 Acc@5 74.826 loss 2.275\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:37  Loss: 1.5649 (1.5649)  Acc@1: 81.2500 (81.2500)  Acc@5: 87.5000 (87.5000)  time: 0.8359  data: 0.4283  max mem: 1329\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 2.1275 (2.1712)  Acc@1: 56.2500 (60.7955)  Acc@5: 87.5000 (82.9545)  time: 0.4763  data: 0.0396  max mem: 1329\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 2.1275 (2.0767)  Acc@1: 56.2500 (60.7143)  Acc@5: 81.2500 (81.2500)  time: 0.4412  data: 0.0023  max mem: 1329\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1292 (2.1940)  Acc@1: 56.2500 (57.8629)  Acc@5: 75.0000 (76.2097)  time: 0.4427  data: 0.0022  max mem: 1329\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4457 (2.1930)  Acc@1: 50.0000 (56.4024)  Acc@5: 68.7500 (76.5244)  time: 0.4437  data: 0.0004  max mem: 1329\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.2914 (2.1674)  Acc@1: 50.0000 (56.9832)  Acc@5: 68.7500 (76.6760)  time: 0.4391  data: 0.0003  max mem: 1329\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4519 s / it)\n",
            "* Acc@1 56.983 Acc@5 76.676 loss 2.167\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:35  Loss: 2.3755 (2.3755)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 1.0162  data: 0.6113  max mem: 1329\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:13  Loss: 2.4056 (2.4709)  Acc@1: 43.7500 (47.1591)  Acc@5: 68.7500 (71.0227)  time: 0.5338  data: 0.1001  max mem: 1329\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 2.2707 (2.4359)  Acc@1: 37.5000 (46.7262)  Acc@5: 68.7500 (72.3214)  time: 0.4644  data: 0.0252  max mem: 1329\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 2.0411 (2.1256)  Acc@1: 75.0000 (56.8548)  Acc@5: 81.2500 (77.0161)  time: 0.4433  data: 0.0010  max mem: 1329\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.6260 (2.0610)  Acc@1: 75.0000 (58.7814)  Acc@5: 87.5000 (78.4946)  time: 0.4408  data: 0.0005  max mem: 1329\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4725 s / it)\n",
            "* Acc@1 58.781 Acc@5 78.495 loss 2.061\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:24  Loss: 2.2154 (2.2154)  Acc@1: 68.7500 (68.7500)  Acc@5: 75.0000 (75.0000)  time: 0.7121  data: 0.2984  max mem: 1329\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:13  Loss: 2.4305 (2.3813)  Acc@1: 56.2500 (55.1136)  Acc@5: 68.7500 (73.2955)  time: 0.5294  data: 0.0943  max mem: 1329\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.4550 (2.4662)  Acc@1: 50.0000 (52.6786)  Acc@5: 75.0000 (74.1071)  time: 0.4771  data: 0.0373  max mem: 1329\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.4524 (2.3630)  Acc@1: 56.2500 (55.8468)  Acc@5: 75.0000 (75.6048)  time: 0.4431  data: 0.0014  max mem: 1329\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 1.9469 (2.2411)  Acc@1: 68.7500 (58.5278)  Acc@5: 81.2500 (77.0197)  time: 0.4407  data: 0.0014  max mem: 1329\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4703 s / it)\n",
            "* Acc@1 58.528 Acc@5 77.020 loss 2.241\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:24  Loss: 2.0237 (2.0237)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.8920  data: 0.4860  max mem: 1329\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:08  Loss: 2.1102 (1.9993)  Acc@1: 62.5000 (63.6364)  Acc@5: 81.2500 (77.2727)  time: 0.4839  data: 0.0456  max mem: 1329\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.1328 (2.1917)  Acc@1: 62.5000 (58.9286)  Acc@5: 81.2500 (75.8929)  time: 0.4428  data: 0.0018  max mem: 1329\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.0412 (2.0967)  Acc@1: 62.5000 (60.6742)  Acc@5: 81.2500 (77.7528)  time: 0.4396  data: 0.0012  max mem: 1329\n",
            "Test: [Task 8] Total time: 0:00:12 (0.4589 s / it)\n",
            "* Acc@1 60.674 Acc@5 77.753 loss 2.097\n",
            "Test: [Task 9]  [ 0/40]  eta: 0:00:32  Loss: 1.8466 (1.8466)  Acc@1: 68.7500 (68.7500)  Acc@5: 81.2500 (81.2500)  time: 0.8240  data: 0.4172  max mem: 1329\n",
            "Test: [Task 9]  [10/40]  eta: 0:00:14  Loss: 1.2106 (1.4253)  Acc@1: 81.2500 (77.2727)  Acc@5: 93.7500 (88.6364)  time: 0.4764  data: 0.0399  max mem: 1329\n",
            "Test: [Task 9]  [20/40]  eta: 0:00:09  Loss: 1.1117 (1.4377)  Acc@1: 87.5000 (77.0833)  Acc@5: 93.7500 (89.5833)  time: 0.4425  data: 0.0020  max mem: 1329\n",
            "Test: [Task 9]  [30/40]  eta: 0:00:04  Loss: 1.1117 (1.3689)  Acc@1: 81.2500 (78.2258)  Acc@5: 87.5000 (90.3226)  time: 0.4431  data: 0.0027  max mem: 1329\n",
            "Test: [Task 9]  [39/40]  eta: 0:00:00  Loss: 1.2729 (1.3819)  Acc@1: 81.2500 (78.4689)  Acc@5: 87.5000 (90.1116)  time: 0.4260  data: 0.0019  max mem: 1329\n",
            "Test: [Task 9] Total time: 0:00:17 (0.4456 s / it)\n",
            "* Acc@1 78.469 Acc@5 90.112 loss 1.382\n",
            "[Average accuracy till task9]\tAcc@1: 60.0794\tAcc@5: 79.0667\tLoss: 2.0393\tForgetting: 7.1448\tBackward: -7.1448\n",
            "Loading checkpoint from: ./output/checkpoint/task10_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/41]  eta: 0:00:36  Loss: 2.3888 (2.3888)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.8843  data: 0.4729  max mem: 1329\n",
            "Test: [Task 1]  [10/41]  eta: 0:00:14  Loss: 2.3888 (2.4879)  Acc@1: 50.0000 (47.7273)  Acc@5: 68.7500 (69.3182)  time: 0.4830  data: 0.0443  max mem: 1329\n",
            "Test: [Task 1]  [20/41]  eta: 0:00:09  Loss: 2.0067 (2.1192)  Acc@1: 50.0000 (56.5476)  Acc@5: 75.0000 (74.1071)  time: 0.4426  data: 0.0022  max mem: 1329\n",
            "Test: [Task 1]  [30/41]  eta: 0:00:05  Loss: 1.6480 (2.1108)  Acc@1: 62.5000 (55.6452)  Acc@5: 81.2500 (74.7984)  time: 0.4421  data: 0.0017  max mem: 1329\n",
            "Test: [Task 1]  [40/41]  eta: 0:00:00  Loss: 2.1192 (2.1320)  Acc@1: 56.2500 (55.5385)  Acc@5: 75.0000 (74.6154)  time: 0.4349  data: 0.0004  max mem: 1329\n",
            "Test: [Task 1] Total time: 0:00:18 (0.4520 s / it)\n",
            "* Acc@1 55.538 Acc@5 74.615 loss 2.132\n",
            "Test: [Task 2]  [ 0/47]  eta: 0:00:52  Loss: 0.9263 (0.9263)  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 1.1224  data: 0.7189  max mem: 1329\n",
            "Test: [Task 2]  [10/47]  eta: 0:00:18  Loss: 1.0728 (1.4142)  Acc@1: 81.2500 (76.7045)  Acc@5: 93.7500 (86.3636)  time: 0.5020  data: 0.0676  max mem: 1329\n",
            "Test: [Task 2]  [20/47]  eta: 0:00:12  Loss: 1.7483 (1.8736)  Acc@1: 62.5000 (66.9643)  Acc@5: 81.2500 (80.3571)  time: 0.4412  data: 0.0020  max mem: 1329\n",
            "Test: [Task 2]  [30/47]  eta: 0:00:07  Loss: 1.9374 (1.9082)  Acc@1: 62.5000 (65.5242)  Acc@5: 81.2500 (81.0484)  time: 0.4424  data: 0.0010  max mem: 1329\n",
            "Test: [Task 2]  [40/47]  eta: 0:00:03  Loss: 2.1568 (2.0723)  Acc@1: 56.2500 (61.4329)  Acc@5: 75.0000 (77.4390)  time: 0.4413  data: 0.0017  max mem: 1329\n",
            "Test: [Task 2]  [46/47]  eta: 0:00:00  Loss: 2.1568 (2.0235)  Acc@1: 50.0000 (62.2667)  Acc@5: 75.0000 (77.3333)  time: 0.4385  data: 0.0016  max mem: 1329\n",
            "Test: [Task 2] Total time: 0:00:21 (0.4564 s / it)\n",
            "* Acc@1 62.267 Acc@5 77.333 loss 2.023\n",
            "Test: [Task 3]  [ 0/27]  eta: 0:00:21  Loss: 2.6432 (2.6432)  Acc@1: 50.0000 (50.0000)  Acc@5: 75.0000 (75.0000)  time: 0.7819  data: 0.3630  max mem: 1329\n",
            "Test: [Task 3]  [10/27]  eta: 0:00:08  Loss: 1.8827 (1.9236)  Acc@1: 50.0000 (56.2500)  Acc@5: 87.5000 (84.0909)  time: 0.4720  data: 0.0368  max mem: 1329\n",
            "Test: [Task 3]  [20/27]  eta: 0:00:03  Loss: 1.8827 (1.9735)  Acc@1: 50.0000 (55.6548)  Acc@5: 87.5000 (82.7381)  time: 0.4406  data: 0.0030  max mem: 1329\n",
            "Test: [Task 3]  [26/27]  eta: 0:00:00  Loss: 1.9367 (2.0780)  Acc@1: 50.0000 (52.6932)  Acc@5: 81.2500 (80.7963)  time: 0.4338  data: 0.0010  max mem: 1329\n",
            "Test: [Task 3] Total time: 0:00:12 (0.4507 s / it)\n",
            "* Acc@1 52.693 Acc@5 80.796 loss 2.078\n",
            "Test: [Task 4]  [ 0/36]  eta: 0:00:28  Loss: 2.9355 (2.9355)  Acc@1: 43.7500 (43.7500)  Acc@5: 43.7500 (43.7500)  time: 0.7783  data: 0.3670  max mem: 1329\n",
            "Test: [Task 4]  [10/36]  eta: 0:00:12  Loss: 2.6793 (2.2253)  Acc@1: 43.7500 (50.0000)  Acc@5: 68.7500 (72.7273)  time: 0.4711  data: 0.0355  max mem: 1329\n",
            "Test: [Task 4]  [20/36]  eta: 0:00:07  Loss: 2.1882 (2.3498)  Acc@1: 50.0000 (47.9167)  Acc@5: 75.0000 (70.8333)  time: 0.4407  data: 0.0018  max mem: 1329\n",
            "Test: [Task 4]  [30/36]  eta: 0:00:02  Loss: 2.1882 (2.2720)  Acc@1: 50.0000 (49.7984)  Acc@5: 75.0000 (74.7984)  time: 0.4406  data: 0.0011  max mem: 1329\n",
            "Test: [Task 4]  [35/36]  eta: 0:00:00  Loss: 2.1882 (2.2723)  Acc@1: 50.0000 (49.8264)  Acc@5: 81.2500 (74.6528)  time: 0.4408  data: 0.0009  max mem: 1329\n",
            "Test: [Task 4] Total time: 0:00:16 (0.4519 s / it)\n",
            "* Acc@1 49.826 Acc@5 74.653 loss 2.272\n",
            "Test: [Task 5]  [ 0/45]  eta: 0:00:32  Loss: 1.3966 (1.3966)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7244  data: 0.3216  max mem: 1329\n",
            "Test: [Task 5]  [10/45]  eta: 0:00:16  Loss: 1.9468 (2.1250)  Acc@1: 62.5000 (59.6591)  Acc@5: 81.2500 (85.2273)  time: 0.4701  data: 0.0339  max mem: 1329\n",
            "Test: [Task 5]  [20/45]  eta: 0:00:11  Loss: 1.9468 (2.0586)  Acc@1: 62.5000 (61.0119)  Acc@5: 81.2500 (81.8452)  time: 0.4421  data: 0.0054  max mem: 1329\n",
            "Test: [Task 5]  [30/45]  eta: 0:00:06  Loss: 2.1150 (2.1634)  Acc@1: 50.0000 (58.4677)  Acc@5: 68.7500 (76.2097)  time: 0.4407  data: 0.0029  max mem: 1329\n",
            "Test: [Task 5]  [40/45]  eta: 0:00:02  Loss: 2.4788 (2.1647)  Acc@1: 50.0000 (57.0122)  Acc@5: 68.7500 (76.3720)  time: 0.4418  data: 0.0003  max mem: 1329\n",
            "Test: [Task 5]  [44/45]  eta: 0:00:00  Loss: 2.4195 (2.1463)  Acc@1: 50.0000 (57.2626)  Acc@5: 68.7500 (76.2570)  time: 0.4367  data: 0.0003  max mem: 1329\n",
            "Test: [Task 5] Total time: 0:00:20 (0.4484 s / it)\n",
            "* Acc@1 57.263 Acc@5 76.257 loss 2.146\n",
            "Test: [Task 6]  [ 0/35]  eta: 0:00:39  Loss: 2.3383 (2.3383)  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (81.2500)  time: 1.1391  data: 0.7376  max mem: 1329\n",
            "Test: [Task 6]  [10/35]  eta: 0:00:13  Loss: 2.4003 (2.4055)  Acc@1: 43.7500 (46.0227)  Acc@5: 75.0000 (73.8636)  time: 0.5385  data: 0.1079  max mem: 1329\n",
            "Test: [Task 6]  [20/35]  eta: 0:00:07  Loss: 2.3959 (2.4066)  Acc@1: 43.7500 (46.1310)  Acc@5: 75.0000 (74.1071)  time: 0.4585  data: 0.0229  max mem: 1329\n",
            "Test: [Task 6]  [30/35]  eta: 0:00:02  Loss: 1.9285 (2.1294)  Acc@1: 68.7500 (55.0403)  Acc@5: 81.2500 (77.6210)  time: 0.4401  data: 0.0006  max mem: 1329\n",
            "Test: [Task 6]  [34/35]  eta: 0:00:00  Loss: 1.7559 (2.0630)  Acc@1: 68.7500 (57.1685)  Acc@5: 81.2500 (79.0323)  time: 0.4379  data: 0.0006  max mem: 1329\n",
            "Test: [Task 6] Total time: 0:00:16 (0.4720 s / it)\n",
            "* Acc@1 57.168 Acc@5 79.032 loss 2.063\n",
            "Test: [Task 7]  [ 0/35]  eta: 0:00:27  Loss: 2.4299 (2.4299)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.7777  data: 0.3705  max mem: 1329\n",
            "Test: [Task 7]  [10/35]  eta: 0:00:12  Loss: 2.5369 (2.4197)  Acc@1: 56.2500 (52.8409)  Acc@5: 68.7500 (71.5909)  time: 0.5185  data: 0.0872  max mem: 1329\n",
            "Test: [Task 7]  [20/35]  eta: 0:00:07  Loss: 2.5488 (2.4947)  Acc@1: 43.7500 (50.0000)  Acc@5: 68.7500 (72.9167)  time: 0.4664  data: 0.0300  max mem: 1329\n",
            "Test: [Task 7]  [30/35]  eta: 0:00:02  Loss: 2.5512 (2.4290)  Acc@1: 50.0000 (52.4194)  Acc@5: 75.0000 (73.3871)  time: 0.4408  data: 0.0015  max mem: 1329\n",
            "Test: [Task 7]  [34/35]  eta: 0:00:00  Loss: 2.0825 (2.3153)  Acc@1: 56.2500 (54.5781)  Acc@5: 75.0000 (75.0449)  time: 0.4384  data: 0.0015  max mem: 1329\n",
            "Test: [Task 7] Total time: 0:00:16 (0.4656 s / it)\n",
            "* Acc@1 54.578 Acc@5 75.045 loss 2.315\n",
            "Test: [Task 8]  [ 0/28]  eta: 0:00:26  Loss: 2.0592 (2.0592)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (87.5000)  time: 0.9345  data: 0.5301  max mem: 1329\n",
            "Test: [Task 8]  [10/28]  eta: 0:00:08  Loss: 2.1082 (2.0780)  Acc@1: 68.7500 (65.9091)  Acc@5: 75.0000 (76.7045)  time: 0.4860  data: 0.0490  max mem: 1329\n",
            "Test: [Task 8]  [20/28]  eta: 0:00:03  Loss: 2.3582 (2.2925)  Acc@1: 56.2500 (57.7381)  Acc@5: 75.0000 (73.5119)  time: 0.4418  data: 0.0021  max mem: 1329\n",
            "Test: [Task 8]  [27/28]  eta: 0:00:00  Loss: 2.2346 (2.2057)  Acc@1: 56.2500 (59.3258)  Acc@5: 75.0000 (74.8315)  time: 0.4394  data: 0.0017  max mem: 1329\n",
            "Test: [Task 8] Total time: 0:00:12 (0.4598 s / it)\n",
            "* Acc@1 59.326 Acc@5 74.831 loss 2.206\n",
            "Test: [Task 9]  [ 0/40]  eta: 0:00:35  Loss: 1.8562 (1.8562)  Acc@1: 75.0000 (75.0000)  Acc@5: 81.2500 (81.2500)  time: 0.8857  data: 0.4737  max mem: 1329\n",
            "Test: [Task 9]  [10/40]  eta: 0:00:14  Loss: 1.3858 (1.5586)  Acc@1: 75.0000 (72.7273)  Acc@5: 87.5000 (85.2273)  time: 0.4832  data: 0.0479  max mem: 1329\n",
            "Test: [Task 9]  [20/40]  eta: 0:00:09  Loss: 1.2988 (1.5850)  Acc@1: 75.0000 (72.6190)  Acc@5: 87.5000 (86.0119)  time: 0.4432  data: 0.0032  max mem: 1329\n",
            "Test: [Task 9]  [30/40]  eta: 0:00:04  Loss: 1.2451 (1.5071)  Acc@1: 81.2500 (75.4032)  Acc@5: 87.5000 (86.8952)  time: 0.4432  data: 0.0025  max mem: 1329\n",
            "Test: [Task 9]  [39/40]  eta: 0:00:00  Loss: 1.4978 (1.5010)  Acc@1: 75.0000 (75.1196)  Acc@5: 87.5000 (86.7624)  time: 0.4262  data: 0.0021  max mem: 1329\n",
            "Test: [Task 9] Total time: 0:00:17 (0.4474 s / it)\n",
            "* Acc@1 75.120 Acc@5 86.762 loss 1.501\n",
            "Test: [Task 10]  [ 0/44]  eta: 0:00:32  Loss: 1.3731 (1.3731)  Acc@1: 81.2500 (81.2500)  Acc@5: 87.5000 (87.5000)  time: 0.7290  data: 0.3180  max mem: 1329\n",
            "Test: [Task 10]  [10/44]  eta: 0:00:15  Loss: 1.7728 (1.7896)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.5000 (79.5455)  time: 0.4691  data: 0.0309  max mem: 1329\n",
            "Test: [Task 10]  [20/44]  eta: 0:00:10  Loss: 1.7986 (1.8167)  Acc@1: 62.5000 (66.9643)  Acc@5: 81.2500 (80.0595)  time: 0.4429  data: 0.0028  max mem: 1329\n",
            "Test: [Task 10]  [30/44]  eta: 0:00:06  Loss: 1.7986 (1.8472)  Acc@1: 62.5000 (65.3226)  Acc@5: 81.2500 (80.0403)  time: 0.4430  data: 0.0022  max mem: 1329\n",
            "Test: [Task 10]  [40/44]  eta: 0:00:01  Loss: 1.8852 (1.8899)  Acc@1: 68.7500 (66.0061)  Acc@5: 87.5000 (80.7927)  time: 0.4434  data: 0.0007  max mem: 1329\n",
            "Test: [Task 10]  [43/44]  eta: 0:00:00  Loss: 2.0430 (1.9435)  Acc@1: 68.7500 (64.9856)  Acc@5: 81.2500 (80.1153)  time: 0.4305  data: 0.0006  max mem: 1329\n",
            "Test: [Task 10] Total time: 0:00:19 (0.4465 s / it)\n",
            "* Acc@1 64.986 Acc@5 80.115 loss 1.944\n",
            "[Average accuracy till task10]\tAcc@1: 58.8765\tAcc@5: 77.9441\tLoss: 2.0681\tForgetting: 8.2327\tBackward: -8.2327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsIBLktiu31Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}