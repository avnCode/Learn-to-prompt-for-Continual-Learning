{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90TRrimFstfu",
        "outputId": "d9acf7c6-03a7-4952-f424-810e16ae77e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'l2p-pytorch'...\n",
            "remote: Enumerating objects: 172, done.\u001b[K\n",
            "remote: Counting objects: 100% (172/172), done.\u001b[K\n",
            "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
            "remote: Total 172 (delta 91), reused 105 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (172/172), 67.80 KiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/JH-LEE-KR/l2p-pytorch\n",
        "!cd l2p-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/l2p-pytorch/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "vfW9EfPptjAf",
        "outputId": "63716e30-9d05-46cf-fe84-a68bc619a598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm==0.6.7\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==9.2.0\n",
            "  Downloading Pillow-9.2.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting matplotlib==3.5.3\n",
            "  Downloading matplotlib-3.5.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchprofile==0.0.4\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (0.15.1+cu118)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.3->-r /content/l2p-pytorch/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.4->timm==0.6.7->-r /content/l2p-pytorch/requirements.txt (line 1)) (1.3.0)\n",
            "Installing collected packages: pillow, matplotlib, torchprofile, timm\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.7.1\n",
            "    Uninstalling matplotlib-3.7.1:\n",
            "      Successfully uninstalled matplotlib-3.7.1\n",
            "Successfully installed matplotlib-3.5.3 pillow-9.2.0 timm-0.6.7 torchprofile-0.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch \\\n",
        "        --nproc_per_node=1 \\\n",
        "        --use_env /content/l2p-pytorch/main.py \\\n",
        "        cifar100_l2p \\\n",
        "        --model vit_base_patch16_224 \\\n",
        "        --batch-size 16 \\\n",
        "        --data-path /local_datasets/ \\\n",
        "        --output_dir ./output "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YB7KPZw1tpvm",
        "outputId": "fe120fd4-af7c-4a21-f98b-6ef6ea1b01e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /local_datasets/cifar-100-python.tar.gz\n",
            "100% 169001437/169001437 [00:06<00:00, 28161085.02it/s]\n",
            "Extracting /local_datasets/cifar-100-python.tar.gz to /local_datasets/\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='cifar100_l2p', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 122980\n",
            "Start training for 5 epochs\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:15:31  Lr: 0.001875  Loss: 2.3091  Acc@1: 25.0000 (25.0000)  Acc@5: 43.7500 (43.7500)  time: 2.9769  data: 0.4180  max mem: 2371\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:51  Lr: 0.001875  Loss: 2.0322  Acc@1: 43.7500 (40.9091)  Acc@5: 75.0000 (73.2955)  time: 0.7625  data: 0.0383  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:13  Lr: 0.001875  Loss: 1.8645  Acc@1: 50.0000 (52.0833)  Acc@5: 81.2500 (80.0595)  time: 0.5460  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:02:57  Lr: 0.001875  Loss: 1.7378  Acc@1: 62.5000 (55.4435)  Acc@5: 93.7500 (84.4758)  time: 0.5541  data: 0.0027  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:02:47  Lr: 0.001875  Loss: 1.3602  Acc@1: 62.5000 (60.0610)  Acc@5: 93.7500 (86.8902)  time: 0.5596  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:38  Lr: 0.001875  Loss: 1.3255  Acc@1: 75.0000 (62.1324)  Acc@5: 93.7500 (88.3578)  time: 0.5630  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:31  Lr: 0.001875  Loss: 1.3745  Acc@1: 75.0000 (63.6270)  Acc@5: 93.7500 (89.0369)  time: 0.5672  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:24  Lr: 0.001875  Loss: 1.0908  Acc@1: 75.0000 (65.3169)  Acc@5: 93.7500 (89.9648)  time: 0.5750  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:18  Lr: 0.001875  Loss: 0.9642  Acc@1: 75.0000 (67.3611)  Acc@5: 93.7500 (90.8179)  time: 0.5833  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:12  Lr: 0.001875  Loss: 0.9140  Acc@1: 75.0000 (68.0632)  Acc@5: 93.7500 (91.2775)  time: 0.5883  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:06  Lr: 0.001875  Loss: 0.9436  Acc@1: 81.2500 (69.6163)  Acc@5: 100.0000 (91.8936)  time: 0.5902  data: 0.0025  max mem: 2372\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:00  Lr: 0.001875  Loss: 1.1699  Acc@1: 81.2500 (70.3266)  Acc@5: 93.7500 (92.0608)  time: 0.5961  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:01:54  Lr: 0.001875  Loss: 0.9465  Acc@1: 75.0000 (70.8678)  Acc@5: 93.7500 (92.5620)  time: 0.6063  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.6911  Acc@1: 75.0000 (71.5649)  Acc@5: 100.0000 (93.0821)  time: 0.6164  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:43  Lr: 0.001875  Loss: 0.3217  Acc@1: 81.2500 (72.5177)  Acc@5: 100.0000 (93.4397)  time: 0.6268  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:38  Lr: 0.001875  Loss: 0.5573  Acc@1: 81.2500 (73.0960)  Acc@5: 100.0000 (93.7086)  time: 0.6369  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:32  Lr: 0.001875  Loss: 0.7568  Acc@1: 81.2500 (73.4084)  Acc@5: 100.0000 (93.9441)  time: 0.6415  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:26  Lr: 0.001875  Loss: 0.7710  Acc@1: 81.2500 (74.0497)  Acc@5: 100.0000 (94.1520)  time: 0.6453  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:21  Lr: 0.001875  Loss: 0.8047  Acc@1: 81.2500 (74.1367)  Acc@5: 100.0000 (94.3025)  time: 0.6562  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.9071  Acc@1: 81.2500 (74.7055)  Acc@5: 100.0000 (94.4372)  time: 0.6685  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.7860  Acc@1: 81.2500 (75.0933)  Acc@5: 93.7500 (94.3408)  time: 0.6789  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:03  Lr: 0.001875  Loss: 0.6445  Acc@1: 81.2500 (75.2962)  Acc@5: 93.7500 (94.5201)  time: 0.6875  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:00:58  Lr: 0.001875  Loss: 0.5395  Acc@1: 81.2500 (75.5939)  Acc@5: 100.0000 (94.6833)  time: 0.6897  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:51  Lr: 0.001875  Loss: 0.8769  Acc@1: 81.2500 (75.7305)  Acc@5: 100.0000 (94.8593)  time: 0.6817  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:45  Lr: 0.001875  Loss: 0.4843  Acc@1: 87.5000 (76.4004)  Acc@5: 100.0000 (95.0207)  time: 0.6710  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:39  Lr: 0.001875  Loss: 0.2205  Acc@1: 87.5000 (76.6683)  Acc@5: 100.0000 (95.1195)  time: 0.6630  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:33  Lr: 0.001875  Loss: 0.3099  Acc@1: 81.2500 (76.9636)  Acc@5: 100.0000 (95.2826)  time: 0.6571  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:27  Lr: 0.001875  Loss: 0.0783  Acc@1: 81.2500 (77.1218)  Acc@5: 100.0000 (95.4105)  time: 0.6535  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:20  Lr: 0.001875  Loss: 0.2370  Acc@1: 81.2500 (77.4021)  Acc@5: 100.0000 (95.5738)  time: 0.6521  data: 0.0025  max mem: 2372\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:14  Lr: 0.001875  Loss: 0.3230  Acc@1: 81.2500 (77.4485)  Acc@5: 100.0000 (95.5971)  time: 0.6526  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.7512  Acc@1: 81.2500 (77.6993)  Acc@5: 100.0000 (95.6603)  time: 0.6552  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:01  Lr: 0.001875  Loss: 0.4585  Acc@1: 81.2500 (77.7733)  Acc@5: 93.7500 (95.6793)  time: 0.6590  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5840  Acc@1: 81.2500 (77.7400)  Acc@5: 93.7500 (95.7000)  time: 0.6460  data: 0.0006  max mem: 2372\n",
            "Train: Epoch[1/5] Total time: 0:03:18 (0.6337 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5840  Acc@1: 81.2500 (77.7400)  Acc@5: 93.7500 (95.7000)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:18  Lr: 0.001875  Loss: 0.2528  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8267  data: 0.2047  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: 0.6562  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (96.5909)  time: 0.6832  data: 0.0197  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.2138  Acc@1: 75.0000 (81.8452)  Acc@5: 100.0000 (97.6190)  time: 0.6705  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.4328  Acc@1: 87.5000 (82.6613)  Acc@5: 100.0000 (97.3790)  time: 0.6730  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: 0.4015  Acc@1: 87.5000 (83.8415)  Acc@5: 100.0000 (97.2561)  time: 0.6725  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.0885  Acc@1: 87.5000 (83.5784)  Acc@5: 100.0000 (97.5490)  time: 0.6705  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.2347  Acc@1: 87.5000 (83.7090)  Acc@5: 100.0000 (97.5410)  time: 0.6689  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.6191  Acc@1: 87.5000 (83.6268)  Acc@5: 100.0000 (97.7113)  time: 0.6670  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.1494  Acc@1: 81.2500 (83.4877)  Acc@5: 100.0000 (97.7623)  time: 0.6657  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.3033  Acc@1: 81.2500 (83.5852)  Acc@5: 100.0000 (97.8022)  time: 0.6660  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.2242  Acc@1: 87.5000 (83.6634)  Acc@5: 100.0000 (97.8960)  time: 0.6663  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.3785  Acc@1: 87.5000 (83.7838)  Acc@5: 100.0000 (97.9730)  time: 0.6655  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3364  Acc@1: 87.5000 (83.6260)  Acc@5: 100.0000 (97.9339)  time: 0.6655  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0186  Acc@1: 87.5000 (83.9218)  Acc@5: 100.0000 (97.8053)  time: 0.6659  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.6044  Acc@1: 87.5000 (83.9096)  Acc@5: 93.7500 (97.6507)  time: 0.6665  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.4341  Acc@1: 81.2500 (83.8576)  Acc@5: 93.7500 (97.6407)  time: 0.6666  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1125  Acc@1: 87.5000 (83.8509)  Acc@5: 100.0000 (97.5932)  time: 0.6665  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.1489  Acc@1: 81.2500 (83.8450)  Acc@5: 100.0000 (97.6608)  time: 0.6678  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.0754  Acc@1: 81.2500 (83.6671)  Acc@5: 100.0000 (97.6174)  time: 0.6690  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.2721  Acc@1: 81.2500 (83.8024)  Acc@5: 100.0000 (97.7094)  time: 0.6710  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.1978  Acc@1: 87.5000 (83.7687)  Acc@5: 100.0000 (97.7923)  time: 0.6725  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.3872  Acc@1: 87.5000 (84.0047)  Acc@5: 100.0000 (97.8673)  time: 0.6732  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.4166  Acc@1: 81.2500 (83.7952)  Acc@5: 100.0000 (97.8224)  time: 0.6731  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0356  Acc@1: 87.5000 (84.0368)  Acc@5: 100.0000 (97.9167)  time: 0.6720  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0036  Acc@1: 93.7500 (84.1805)  Acc@5: 100.0000 (98.0031)  time: 0.6708  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.6842  Acc@1: 81.2500 (84.1882)  Acc@5: 100.0000 (97.9582)  time: 0.6704  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.6743  Acc@1: 81.2500 (83.9080)  Acc@5: 100.0000 (97.9646)  time: 0.6702  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2213  Acc@1: 81.2500 (83.9022)  Acc@5: 100.0000 (98.0166)  time: 0.6703  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.5340  Acc@1: 81.2500 (83.8078)  Acc@5: 100.0000 (97.9760)  time: 0.6698  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.3708  Acc@1: 81.2500 (83.8488)  Acc@5: 100.0000 (97.9811)  time: 0.6691  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1848  Acc@1: 87.5000 (83.9909)  Acc@5: 100.0000 (98.0274)  time: 0.6692  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.4542  Acc@1: 87.5000 (84.0233)  Acc@5: 100.0000 (98.0305)  time: 0.6694  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1607  Acc@1: 87.5000 (84.0800)  Acc@5: 100.0000 (98.0400)  time: 0.6526  data: 0.0007  max mem: 2372\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6689 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1607  Acc@1: 87.5000 (84.0800)  Acc@5: 100.0000 (98.0400)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:36  Lr: 0.001875  Loss: 0.1863  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8842  data: 0.2561  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: 0.7276  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.2955)  time: 0.6895  data: 0.0252  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:19  Lr: 0.001875  Loss: 0.4384  Acc@1: 81.2500 (83.0357)  Acc@5: 100.0000 (98.8095)  time: 0.6693  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.0463  Acc@1: 87.5000 (84.6774)  Acc@5: 100.0000 (98.9919)  time: 0.6686  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0318  Acc@1: 87.5000 (85.8232)  Acc@5: 100.0000 (98.6280)  time: 0.6679  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.2113  Acc@1: 87.5000 (86.2745)  Acc@5: 100.0000 (98.7745)  time: 0.6679  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.5742  Acc@1: 81.2500 (85.2459)  Acc@5: 100.0000 (98.4631)  time: 0.6680  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.5744  Acc@1: 81.2500 (85.7394)  Acc@5: 100.0000 (98.5915)  time: 0.6678  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.2810  Acc@1: 87.5000 (86.2654)  Acc@5: 100.0000 (98.5340)  time: 0.6672  data: 0.0024  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.0214  Acc@1: 87.5000 (86.4011)  Acc@5: 100.0000 (98.4890)  time: 0.6673  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.2131  Acc@1: 87.5000 (86.0767)  Acc@5: 100.0000 (98.5149)  time: 0.6678  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0009  Acc@1: 87.5000 (86.5428)  Acc@5: 100.0000 (98.5360)  time: 0.6670  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.6609  Acc@1: 87.5000 (86.1054)  Acc@5: 100.0000 (98.6054)  time: 0.6670  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.2413  Acc@1: 87.5000 (86.6889)  Acc@5: 100.0000 (98.5687)  time: 0.6673  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.2396  Acc@1: 87.5000 (86.8794)  Acc@5: 100.0000 (98.5372)  time: 0.6672  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.2546  Acc@1: 87.5000 (86.7964)  Acc@5: 100.0000 (98.5099)  time: 0.6678  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1340  Acc@1: 87.5000 (86.9953)  Acc@5: 100.0000 (98.6025)  time: 0.6679  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3675  Acc@1: 87.5000 (86.8056)  Acc@5: 100.0000 (98.6111)  time: 0.6672  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.1285  Acc@1: 87.5000 (86.7058)  Acc@5: 100.0000 (98.6188)  time: 0.6664  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0825  Acc@1: 87.5000 (86.6165)  Acc@5: 100.0000 (98.6584)  time: 0.6659  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2120  Acc@1: 87.5000 (86.7537)  Acc@5: 100.0000 (98.6629)  time: 0.6663  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.1176  Acc@1: 87.5000 (86.4929)  Acc@5: 100.0000 (98.6671)  time: 0.6663  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.1071  Acc@1: 81.2500 (86.4536)  Acc@5: 100.0000 (98.6143)  time: 0.6662  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2978  Acc@1: 87.5000 (86.3907)  Acc@5: 100.0000 (98.6201)  time: 0.6661  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2812  Acc@1: 87.5000 (86.3330)  Acc@5: 100.0000 (98.5477)  time: 0.6661  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.1531  Acc@1: 87.5000 (86.3546)  Acc@5: 100.0000 (98.4811)  time: 0.6662  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2807  Acc@1: 87.5000 (86.4224)  Acc@5: 100.0000 (98.4674)  time: 0.6664  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: -0.0488  Acc@1: 87.5000 (86.4852)  Acc@5: 100.0000 (98.4779)  time: 0.6666  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.0917  Acc@1: 87.5000 (86.4769)  Acc@5: 100.0000 (98.4875)  time: 0.6664  data: 0.0006  max mem: 2372\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.3934  Acc@1: 87.5000 (86.4905)  Acc@5: 100.0000 (98.4751)  time: 0.6665  data: 0.0006  max mem: 2372\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1255  Acc@1: 87.5000 (86.5656)  Acc@5: 100.0000 (98.5050)  time: 0.6664  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1430  Acc@1: 87.5000 (86.6559)  Acc@5: 100.0000 (98.5531)  time: 0.6667  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.8807  Acc@1: 87.5000 (86.6400)  Acc@5: 100.0000 (98.5200)  time: 0.6499  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[3/5] Total time: 0:03:28 (0.6670 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.8807  Acc@1: 87.5000 (86.6400)  Acc@5: 100.0000 (98.5200)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:04:51  Lr: 0.001875  Loss: 0.0966  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.9325  data: 0.3103  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:29  Lr: 0.001875  Loss: 0.2146  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (98.2955)  time: 0.6913  data: 0.0304  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:19  Lr: 0.001875  Loss: 0.1460  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (98.5119)  time: 0.6667  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.0895  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (98.5887)  time: 0.6665  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0412  Acc@1: 87.5000 (87.1951)  Acc@5: 100.0000 (98.1707)  time: 0.6667  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0216  Acc@1: 87.5000 (87.7451)  Acc@5: 100.0000 (98.5294)  time: 0.6668  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.2503  Acc@1: 87.5000 (87.0902)  Acc@5: 100.0000 (98.3607)  time: 0.6662  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.4234  Acc@1: 81.2500 (86.2676)  Acc@5: 100.0000 (98.3275)  time: 0.6664  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.1854  Acc@1: 81.2500 (86.0340)  Acc@5: 100.0000 (98.3025)  time: 0.6667  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.0226  Acc@1: 87.5000 (86.1264)  Acc@5: 100.0000 (98.2830)  time: 0.6665  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.0804  Acc@1: 87.5000 (86.2005)  Acc@5: 100.0000 (98.4530)  time: 0.6672  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.1171  Acc@1: 87.5000 (86.1486)  Acc@5: 100.0000 (98.4797)  time: 0.6667  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.2152  Acc@1: 81.2500 (85.8988)  Acc@5: 100.0000 (98.3471)  time: 0.6662  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0789  Acc@1: 87.5000 (86.1164)  Acc@5: 100.0000 (98.3779)  time: 0.6665  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0221  Acc@1: 93.7500 (86.4362)  Acc@5: 100.0000 (98.4043)  time: 0.6667  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.6444  Acc@1: 87.5000 (86.7550)  Acc@5: 100.0000 (98.4272)  time: 0.6675  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0709  Acc@1: 87.5000 (86.6071)  Acc@5: 100.0000 (98.4472)  time: 0.6672  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.0666  Acc@1: 87.5000 (86.5132)  Acc@5: 100.0000 (98.4284)  time: 0.6661  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.2139  Acc@1: 87.5000 (86.7403)  Acc@5: 100.0000 (98.5152)  time: 0.6661  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0939  Acc@1: 87.5000 (86.7801)  Acc@5: 100.0000 (98.4620)  time: 0.6668  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.3599  Acc@1: 87.5000 (87.0025)  Acc@5: 100.0000 (98.5075)  time: 0.6661  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.5481  Acc@1: 87.5000 (86.9668)  Acc@5: 100.0000 (98.5486)  time: 0.6657  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.3948  Acc@1: 87.5000 (87.1324)  Acc@5: 100.0000 (98.5577)  time: 0.6662  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0499  Acc@1: 87.5000 (87.1483)  Acc@5: 100.0000 (98.5390)  time: 0.6663  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0050  Acc@1: 87.5000 (87.2407)  Acc@5: 100.0000 (98.4959)  time: 0.6671  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2352  Acc@1: 87.5000 (87.2759)  Acc@5: 100.0000 (98.5309)  time: 0.6671  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2890  Acc@1: 87.5000 (87.1648)  Acc@5: 100.0000 (98.4914)  time: 0.6670  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.5211  Acc@1: 81.2500 (87.1079)  Acc@5: 100.0000 (98.4087)  time: 0.6669  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.0986  Acc@1: 87.5000 (87.1219)  Acc@5: 100.0000 (98.4208)  time: 0.6667  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2215  Acc@1: 87.5000 (87.1134)  Acc@5: 100.0000 (98.4536)  time: 0.6669  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1878  Acc@1: 87.5000 (87.1470)  Acc@5: 100.0000 (98.5050)  time: 0.6664  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3699  Acc@1: 87.5000 (87.0981)  Acc@5: 100.0000 (98.4928)  time: 0.6664  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.0986  Acc@1: 87.5000 (87.1200)  Acc@5: 100.0000 (98.4800)  time: 0.6505  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[4/5] Total time: 0:03:28 (0.6667 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.0986  Acc@1: 87.5000 (87.1200)  Acc@5: 100.0000 (98.4800)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:04:16  Lr: 0.001875  Loss: 0.2012  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.8200  data: 0.1973  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: 0.2491  Acc@1: 87.5000 (85.7955)  Acc@5: 100.0000 (98.8636)  time: 0.6817  data: 0.0202  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.2670  Acc@1: 87.5000 (88.3929)  Acc@5: 100.0000 (97.9167)  time: 0.6669  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.4991  Acc@1: 87.5000 (86.6935)  Acc@5: 100.0000 (97.7823)  time: 0.6666  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.4785  Acc@1: 81.2500 (85.2134)  Acc@5: 100.0000 (98.1707)  time: 0.6672  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0503  Acc@1: 81.2500 (85.5392)  Acc@5: 100.0000 (98.2843)  time: 0.6670  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.3004  Acc@1: 81.2500 (84.9385)  Acc@5: 100.0000 (98.0533)  time: 0.6666  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: -0.0688  Acc@1: 81.2500 (84.7711)  Acc@5: 100.0000 (98.0634)  time: 0.6670  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.6196  Acc@1: 87.5000 (85.0309)  Acc@5: 100.0000 (98.2253)  time: 0.6674  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.1611  Acc@1: 81.2500 (84.6841)  Acc@5: 100.0000 (98.3516)  time: 0.6669  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.2543  Acc@1: 81.2500 (84.7153)  Acc@5: 100.0000 (98.5149)  time: 0.6668  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.1909  Acc@1: 87.5000 (84.9099)  Acc@5: 100.0000 (98.4797)  time: 0.6668  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: 0.0556  Acc@1: 87.5000 (85.1240)  Acc@5: 100.0000 (98.5537)  time: 0.6672  data: 0.0010  max mem: 2372\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.2380  Acc@1: 87.5000 (85.0191)  Acc@5: 100.0000 (98.5687)  time: 0.6671  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0502  Acc@1: 87.5000 (85.2837)  Acc@5: 100.0000 (98.6702)  time: 0.6658  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.0822  Acc@1: 87.5000 (85.2235)  Acc@5: 100.0000 (98.6755)  time: 0.6662  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1473  Acc@1: 81.2500 (85.2096)  Acc@5: 100.0000 (98.6025)  time: 0.6670  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3280  Acc@1: 87.5000 (85.5629)  Acc@5: 100.0000 (98.6477)  time: 0.6672  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.2619  Acc@1: 87.5000 (85.7044)  Acc@5: 100.0000 (98.6188)  time: 0.6669  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.3385  Acc@1: 87.5000 (85.7330)  Acc@5: 100.0000 (98.6257)  time: 0.6664  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.1140  Acc@1: 87.5000 (86.0386)  Acc@5: 100.0000 (98.6007)  time: 0.6659  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.1264  Acc@1: 87.5000 (85.9301)  Acc@5: 100.0000 (98.5782)  time: 0.6658  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.0498  Acc@1: 87.5000 (86.0294)  Acc@5: 100.0000 (98.6143)  time: 0.6668  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.1109  Acc@1: 87.5000 (86.0931)  Acc@5: 100.0000 (98.6201)  time: 0.6685  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2343  Acc@1: 87.5000 (86.1515)  Acc@5: 100.0000 (98.6515)  time: 0.6682  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2149  Acc@1: 87.5000 (86.2799)  Acc@5: 100.0000 (98.6554)  time: 0.6670  data: 0.0007  max mem: 2372\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0204  Acc@1: 87.5000 (86.3266)  Acc@5: 100.0000 (98.6111)  time: 0.6673  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2236  Acc@1: 87.5000 (86.3238)  Acc@5: 100.0000 (98.5932)  time: 0.6676  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0912  Acc@1: 87.5000 (86.4324)  Acc@5: 100.0000 (98.5988)  time: 0.6673  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0982  Acc@1: 87.5000 (86.5765)  Acc@5: 100.0000 (98.6254)  time: 0.6676  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1967  Acc@1: 87.5000 (86.5449)  Acc@5: 100.0000 (98.6088)  time: 0.6685  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1607  Acc@1: 87.5000 (86.5957)  Acc@5: 100.0000 (98.6133)  time: 0.6685  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0215  Acc@1: 87.5000 (86.6000)  Acc@5: 100.0000 (98.6000)  time: 0.6515  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[5/5] Total time: 0:03:28 (0.6668 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0215  Acc@1: 87.5000 (86.6000)  Acc@5: 100.0000 (98.6000)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:38  Loss: 0.4389 (0.4389)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6042  data: 0.2053  max mem: 2372\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.4181 (0.4133)  Acc@1: 100.0000 (97.1591)  Acc@5: 100.0000 (99.4318)  time: 0.4479  data: 0.0197  max mem: 2372\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4181 (0.4811)  Acc@1: 93.7500 (96.4286)  Acc@5: 100.0000 (99.7024)  time: 0.4334  data: 0.0016  max mem: 2372\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3209 (0.4284)  Acc@1: 100.0000 (97.3790)  Acc@5: 100.0000 (99.7984)  time: 0.4338  data: 0.0012  max mem: 2372\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.2965 (0.4261)  Acc@1: 100.0000 (97.5610)  Acc@5: 100.0000 (99.8476)  time: 0.4330  data: 0.0014  max mem: 2372\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3595 (0.4154)  Acc@1: 100.0000 (97.7941)  Acc@5: 100.0000 (99.7549)  time: 0.4337  data: 0.0027  max mem: 2372\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4022 (0.4095)  Acc@1: 100.0000 (98.0533)  Acc@5: 100.0000 (99.7951)  time: 0.4338  data: 0.0016  max mem: 2372\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4030 (0.4104)  Acc@1: 100.0000 (98.1000)  Acc@5: 100.0000 (99.8000)  time: 0.4229  data: 0.0010  max mem: 2372\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4335 s / it)\n",
            "* Acc@1 98.100 Acc@5 99.800 loss 0.410\n",
            "[Average accuracy till task1]\tAcc@1: 98.1000\tAcc@5: 99.8000\tLoss: 0.4104\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:04:45  Lr: 0.001875  Loss: 2.0994  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (50.0000)  time: 0.9133  data: 0.2934  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: 1.8985  Acc@1: 37.5000 (36.3636)  Acc@5: 75.0000 (75.5682)  time: 0.6883  data: 0.0292  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 1.7582  Acc@1: 50.0000 (50.8929)  Acc@5: 87.5000 (83.9286)  time: 0.6653  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 1.4749  Acc@1: 68.7500 (59.4758)  Acc@5: 93.7500 (88.3065)  time: 0.6658  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 1.1460  Acc@1: 75.0000 (61.4329)  Acc@5: 100.0000 (89.9390)  time: 0.6673  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 1.1979  Acc@1: 68.7500 (64.4608)  Acc@5: 93.7500 (90.9314)  time: 0.6673  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.7456  Acc@1: 75.0000 (66.7008)  Acc@5: 93.7500 (91.8033)  time: 0.6670  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.8029  Acc@1: 81.2500 (68.6620)  Acc@5: 93.7500 (92.1655)  time: 0.6673  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.6998  Acc@1: 81.2500 (70.7562)  Acc@5: 100.0000 (92.8241)  time: 0.6691  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.6432  Acc@1: 81.2500 (71.5659)  Acc@5: 100.0000 (93.3379)  time: 0.6701  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.4416  Acc@1: 81.2500 (72.7104)  Acc@5: 100.0000 (93.7500)  time: 0.6695  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.5804  Acc@1: 81.2500 (73.8176)  Acc@5: 100.0000 (94.2005)  time: 0.6693  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.4370  Acc@1: 87.5000 (74.8450)  Acc@5: 100.0000 (94.5248)  time: 0.6701  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.9871  Acc@1: 81.2500 (75.1431)  Acc@5: 100.0000 (94.6565)  time: 0.6708  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.5877  Acc@1: 75.0000 (75.3103)  Acc@5: 100.0000 (94.9911)  time: 0.6710  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.6261  Acc@1: 81.2500 (75.9106)  Acc@5: 100.0000 (95.2401)  time: 0.6717  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3290  Acc@1: 81.2500 (76.4752)  Acc@5: 100.0000 (95.3416)  time: 0.6727  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3178  Acc@1: 81.2500 (76.7909)  Acc@5: 100.0000 (95.5044)  time: 0.6728  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.2678  Acc@1: 81.2500 (77.0718)  Acc@5: 100.0000 (95.5110)  time: 0.6718  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.3566  Acc@1: 81.2500 (77.4869)  Acc@5: 100.0000 (95.6479)  time: 0.6709  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.4157  Acc@1: 81.2500 (77.5498)  Acc@5: 100.0000 (95.7090)  time: 0.6700  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.2907  Acc@1: 81.2500 (77.8436)  Acc@5: 100.0000 (95.7938)  time: 0.6694  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.5207  Acc@1: 81.2500 (78.1391)  Acc@5: 100.0000 (95.8145)  time: 0.6694  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.3643  Acc@1: 87.5000 (78.4091)  Acc@5: 93.7500 (95.8333)  time: 0.6698  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.7277  Acc@1: 87.5000 (78.5270)  Acc@5: 100.0000 (95.9025)  time: 0.6703  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2287  Acc@1: 81.2500 (78.7102)  Acc@5: 100.0000 (95.9412)  time: 0.6699  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.3182  Acc@1: 87.5000 (78.9272)  Acc@5: 100.0000 (96.0010)  time: 0.6693  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0041  Acc@1: 81.2500 (79.0129)  Acc@5: 100.0000 (96.0332)  time: 0.6690  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2167  Acc@1: 81.2500 (79.3372)  Acc@5: 100.0000 (96.1077)  time: 0.6683  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2659  Acc@1: 87.5000 (79.4029)  Acc@5: 100.0000 (96.1770)  time: 0.6682  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.5338  Acc@1: 81.2500 (79.4435)  Acc@5: 100.0000 (96.2417)  time: 0.6686  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.5444  Acc@1: 75.0000 (79.4011)  Acc@5: 100.0000 (96.2420)  time: 0.6690  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.2993  Acc@1: 75.0000 (79.4000)  Acc@5: 100.0000 (96.2400)  time: 0.6526  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[1/5] Total time: 0:03:29 (0.6693 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.2993  Acc@1: 75.0000 (79.4000)  Acc@5: 100.0000 (96.2400)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:00  Lr: 0.001875  Loss: 0.5941  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7677  data: 0.1341  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.1430  Acc@1: 81.2500 (82.3864)  Acc@5: 100.0000 (96.5909)  time: 0.6781  data: 0.0145  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.3184  Acc@1: 81.2500 (83.0357)  Acc@5: 100.0000 (97.0238)  time: 0.6686  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.4320  Acc@1: 81.2500 (82.6613)  Acc@5: 100.0000 (97.1774)  time: 0.6679  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0346  Acc@1: 87.5000 (84.7561)  Acc@5: 100.0000 (97.4085)  time: 0.6675  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.4775  Acc@1: 87.5000 (84.8039)  Acc@5: 100.0000 (97.6716)  time: 0.6677  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.4733  Acc@1: 81.2500 (83.9139)  Acc@5: 100.0000 (97.7459)  time: 0.6686  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.3505  Acc@1: 81.2500 (84.0669)  Acc@5: 100.0000 (97.7113)  time: 0.6687  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.2552  Acc@1: 87.5000 (84.2593)  Acc@5: 100.0000 (97.5309)  time: 0.6690  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.6009  Acc@1: 87.5000 (83.9973)  Acc@5: 100.0000 (97.5962)  time: 0.6694  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.4308  Acc@1: 81.2500 (83.6015)  Acc@5: 100.0000 (97.7104)  time: 0.6700  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.4297  Acc@1: 81.2500 (83.8401)  Acc@5: 100.0000 (97.8041)  time: 0.6698  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.4520  Acc@1: 81.2500 (83.4711)  Acc@5: 100.0000 (97.7789)  time: 0.6691  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1935  Acc@1: 81.2500 (83.3015)  Acc@5: 100.0000 (97.6145)  time: 0.6686  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.1440  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (97.6950)  time: 0.6681  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.4864  Acc@1: 87.5000 (83.6921)  Acc@5: 100.0000 (97.7649)  time: 0.6680  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0005  Acc@1: 87.5000 (83.8121)  Acc@5: 100.0000 (97.8649)  time: 0.6675  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.2699  Acc@1: 87.5000 (83.6623)  Acc@5: 100.0000 (97.8801)  time: 0.6678  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.1855  Acc@1: 81.2500 (83.3909)  Acc@5: 100.0000 (97.9282)  time: 0.6683  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0761  Acc@1: 81.2500 (83.6387)  Acc@5: 100.0000 (97.8730)  time: 0.6678  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0444  Acc@1: 87.5000 (83.8930)  Acc@5: 100.0000 (97.8856)  time: 0.6670  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2924  Acc@1: 87.5000 (84.0047)  Acc@5: 100.0000 (97.8081)  time: 0.6675  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2065  Acc@1: 87.5000 (83.8235)  Acc@5: 100.0000 (97.7941)  time: 0.6686  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.3169  Acc@1: 87.5000 (83.9827)  Acc@5: 100.0000 (97.8084)  time: 0.6681  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.4557  Acc@1: 87.5000 (83.9212)  Acc@5: 100.0000 (97.7438)  time: 0.6675  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.6873  Acc@1: 81.2500 (83.8396)  Acc@5: 100.0000 (97.7590)  time: 0.6676  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.5583  Acc@1: 87.5000 (84.0038)  Acc@5: 100.0000 (97.7730)  time: 0.6675  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1349  Acc@1: 81.2500 (83.9253)  Acc@5: 100.0000 (97.7629)  time: 0.6678  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2959  Acc@1: 81.2500 (83.9858)  Acc@5: 100.0000 (97.7313)  time: 0.6684  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.5163  Acc@1: 87.5000 (84.0206)  Acc@5: 100.0000 (97.7448)  time: 0.6681  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.4962  Acc@1: 87.5000 (84.1777)  Acc@5: 100.0000 (97.7575)  time: 0.6676  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1221  Acc@1: 87.5000 (84.2042)  Acc@5: 100.0000 (97.7693)  time: 0.6685  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1416  Acc@1: 87.5000 (84.2600)  Acc@5: 100.0000 (97.7800)  time: 0.6522  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6678 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1416  Acc@1: 87.5000 (84.2600)  Acc@5: 100.0000 (97.7800)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:02  Lr: 0.001875  Loss: 0.2421  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7753  data: 0.1520  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: 0.3102  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (99.4318)  time: 0.6806  data: 0.0146  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.2419  Acc@1: 87.5000 (87.2024)  Acc@5: 100.0000 (99.1071)  time: 0.6696  data: 0.0007  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.0910  Acc@1: 87.5000 (86.8952)  Acc@5: 100.0000 (98.5887)  time: 0.6679  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: -0.0339  Acc@1: 87.5000 (87.1951)  Acc@5: 100.0000 (98.6280)  time: 0.6673  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0596  Acc@1: 87.5000 (87.1324)  Acc@5: 100.0000 (98.4069)  time: 0.6674  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.1432  Acc@1: 87.5000 (87.0902)  Acc@5: 100.0000 (98.6680)  time: 0.6682  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.3424  Acc@1: 87.5000 (86.7077)  Acc@5: 100.0000 (98.1514)  time: 0.6691  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.1108  Acc@1: 81.2500 (86.2654)  Acc@5: 100.0000 (98.3025)  time: 0.6688  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.2945  Acc@1: 87.5000 (87.0192)  Acc@5: 100.0000 (98.4203)  time: 0.6685  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.2023  Acc@1: 87.5000 (87.2525)  Acc@5: 100.0000 (98.4530)  time: 0.6688  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0608  Acc@1: 87.5000 (87.1622)  Acc@5: 100.0000 (98.4797)  time: 0.6677  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3514  Acc@1: 87.5000 (87.1384)  Acc@5: 100.0000 (98.3471)  time: 0.6679  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.5426  Acc@1: 87.5000 (87.0706)  Acc@5: 100.0000 (98.3302)  time: 0.6680  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.2708  Acc@1: 81.2500 (86.6135)  Acc@5: 100.0000 (98.3599)  time: 0.6672  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: -0.1217  Acc@1: 81.2500 (86.7136)  Acc@5: 100.0000 (98.4272)  time: 0.6676  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3771  Acc@1: 87.5000 (86.7624)  Acc@5: 100.0000 (98.4472)  time: 0.6680  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.0638  Acc@1: 87.5000 (86.8056)  Acc@5: 100.0000 (98.4649)  time: 0.6673  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.1425  Acc@1: 87.5000 (86.6367)  Acc@5: 100.0000 (98.3771)  time: 0.6669  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0207  Acc@1: 87.5000 (86.5510)  Acc@5: 100.0000 (98.3966)  time: 0.6674  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.5474  Acc@1: 81.2500 (86.4117)  Acc@5: 100.0000 (98.2587)  time: 0.6675  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2289  Acc@1: 81.2500 (86.4336)  Acc@5: 93.7500 (98.1931)  time: 0.6672  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.1889  Acc@1: 87.5000 (86.3688)  Acc@5: 100.0000 (98.2183)  time: 0.6669  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.9075  Acc@1: 87.5000 (86.4177)  Acc@5: 100.0000 (98.2413)  time: 0.6667  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2463  Acc@1: 87.5000 (86.3071)  Acc@5: 100.0000 (98.2365)  time: 0.6673  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0246  Acc@1: 87.5000 (86.3297)  Acc@5: 100.0000 (98.2570)  time: 0.6676  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2312  Acc@1: 81.2500 (86.2308)  Acc@5: 100.0000 (98.1561)  time: 0.6669  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2039  Acc@1: 81.2500 (86.1162)  Acc@5: 100.0000 (98.1780)  time: 0.6673  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1295  Acc@1: 81.2500 (86.0320)  Acc@5: 100.0000 (98.1762)  time: 0.6675  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1018  Acc@1: 87.5000 (86.1254)  Acc@5: 100.0000 (98.1744)  time: 0.6677  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.2605  Acc@1: 87.5000 (85.9842)  Acc@5: 100.0000 (98.1312)  time: 0.6685  data: 0.0025  max mem: 2372\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1892  Acc@1: 81.2500 (85.9124)  Acc@5: 100.0000 (98.0908)  time: 0.6690  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.7707  Acc@1: 81.2500 (85.9200)  Acc@5: 100.0000 (98.0800)  time: 0.6522  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[3/5] Total time: 0:03:28 (0.6674 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.7707  Acc@1: 81.2500 (85.9200)  Acc@5: 100.0000 (98.0800)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:04:25  Lr: 0.001875  Loss: 0.3295  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.8496  data: 0.2302  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: -0.0083  Acc@1: 93.7500 (89.7727)  Acc@5: 100.0000 (98.8636)  time: 0.6861  data: 0.0232  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.3847  Acc@1: 81.2500 (85.7143)  Acc@5: 100.0000 (98.2143)  time: 0.6691  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.2408  Acc@1: 81.2500 (86.4919)  Acc@5: 100.0000 (98.7903)  time: 0.6675  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0399  Acc@1: 87.5000 (86.2805)  Acc@5: 100.0000 (98.3232)  time: 0.6671  data: 0.0011  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.2644  Acc@1: 81.2500 (85.7843)  Acc@5: 100.0000 (98.2843)  time: 0.6681  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: -0.0838  Acc@1: 87.5000 (85.9631)  Acc@5: 100.0000 (98.3607)  time: 0.6683  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.1915  Acc@1: 87.5000 (86.0035)  Acc@5: 100.0000 (98.1514)  time: 0.6681  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.0842  Acc@1: 87.5000 (86.0340)  Acc@5: 100.0000 (98.0710)  time: 0.6686  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1348  Acc@1: 87.5000 (86.7445)  Acc@5: 100.0000 (98.2143)  time: 0.6687  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0773  Acc@1: 93.7500 (86.8812)  Acc@5: 100.0000 (98.2673)  time: 0.6683  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: -0.0016  Acc@1: 87.5000 (86.5428)  Acc@5: 100.0000 (98.1982)  time: 0.6688  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.4115  Acc@1: 87.5000 (86.6736)  Acc@5: 100.0000 (98.1921)  time: 0.6690  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.5626  Acc@1: 87.5000 (86.5458)  Acc@5: 100.0000 (98.2824)  time: 0.6681  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0811  Acc@1: 81.2500 (86.4362)  Acc@5: 100.0000 (98.3599)  time: 0.6677  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.1100  Acc@1: 87.5000 (86.3825)  Acc@5: 100.0000 (98.2616)  time: 0.6681  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0419  Acc@1: 87.5000 (86.3742)  Acc@5: 100.0000 (98.2919)  time: 0.6674  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.1294  Acc@1: 87.5000 (86.3304)  Acc@5: 100.0000 (98.2822)  time: 0.6669  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.2750  Acc@1: 81.2500 (86.1878)  Acc@5: 100.0000 (98.3080)  time: 0.6670  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1907  Acc@1: 87.5000 (86.2893)  Acc@5: 100.0000 (98.3639)  time: 0.6667  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.8431  Acc@1: 87.5000 (86.2873)  Acc@5: 100.0000 (98.2587)  time: 0.6664  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2458  Acc@1: 87.5000 (86.4040)  Acc@5: 100.0000 (98.2227)  time: 0.6666  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2143  Acc@1: 87.5000 (86.5950)  Acc@5: 100.0000 (98.2466)  time: 0.6676  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.1507  Acc@1: 87.5000 (86.5530)  Acc@5: 100.0000 (98.2684)  time: 0.6680  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.1394  Acc@1: 87.5000 (86.5664)  Acc@5: 100.0000 (98.3143)  time: 0.6683  data: 0.0021  max mem: 2372\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0745  Acc@1: 87.5000 (86.4542)  Acc@5: 100.0000 (98.3068)  time: 0.6682  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.4477  Acc@1: 87.5000 (86.5182)  Acc@5: 100.0000 (98.3477)  time: 0.6684  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1793  Acc@1: 87.5000 (86.5775)  Acc@5: 100.0000 (98.3395)  time: 0.6678  data: 0.0024  max mem: 2372\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0893  Acc@1: 93.7500 (86.6103)  Acc@5: 100.0000 (98.3541)  time: 0.6669  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1090  Acc@1: 87.5000 (86.6409)  Acc@5: 100.0000 (98.3677)  time: 0.6668  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: -0.0343  Acc@1: 87.5000 (86.6487)  Acc@5: 100.0000 (98.4219)  time: 0.6678  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1339  Acc@1: 87.5000 (86.7363)  Acc@5: 100.0000 (98.4124)  time: 0.6680  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0120  Acc@1: 87.5000 (86.7600)  Acc@5: 100.0000 (98.4200)  time: 0.6514  data: 0.0013  max mem: 2372\n",
            "Train: Epoch[4/5] Total time: 0:03:28 (0.6676 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0120  Acc@1: 87.5000 (86.7600)  Acc@5: 100.0000 (98.4200)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:03:58  Lr: 0.001875  Loss: 0.0275  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7628  data: 0.1368  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.1261  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (99.4318)  time: 0.6778  data: 0.0136  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.0926  Acc@1: 87.5000 (89.2857)  Acc@5: 100.0000 (99.7024)  time: 0.6688  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.2595  Acc@1: 93.7500 (88.7097)  Acc@5: 100.0000 (99.5968)  time: 0.6679  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.1827  Acc@1: 87.5000 (87.6524)  Acc@5: 100.0000 (99.0854)  time: 0.6679  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0709  Acc@1: 81.2500 (87.3775)  Acc@5: 100.0000 (98.8971)  time: 0.6679  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.0401  Acc@1: 81.2500 (86.8852)  Acc@5: 100.0000 (98.7705)  time: 0.6676  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.0659  Acc@1: 87.5000 (86.7077)  Acc@5: 100.0000 (98.9437)  time: 0.6675  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: -0.0070  Acc@1: 87.5000 (87.3457)  Acc@5: 100.0000 (98.9969)  time: 0.6672  data: 0.0012  max mem: 2372\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.3839  Acc@1: 87.5000 (87.3626)  Acc@5: 100.0000 (98.8324)  time: 0.6671  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.1728  Acc@1: 87.5000 (87.7475)  Acc@5: 100.0000 (98.7624)  time: 0.6671  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0380  Acc@1: 93.7500 (88.0631)  Acc@5: 100.0000 (98.7613)  time: 0.6666  data: 0.0014  max mem: 2372\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: -0.0326  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (98.7087)  time: 0.6663  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0826  Acc@1: 87.5000 (87.9771)  Acc@5: 100.0000 (98.6164)  time: 0.6663  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: -0.1134  Acc@1: 87.5000 (87.7660)  Acc@5: 100.0000 (98.5372)  time: 0.6661  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: -0.1669  Acc@1: 87.5000 (88.1623)  Acc@5: 100.0000 (98.5513)  time: 0.6669  data: 0.0023  max mem: 2372\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.1395  Acc@1: 87.5000 (88.1211)  Acc@5: 100.0000 (98.5637)  time: 0.6682  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.2252  Acc@1: 87.5000 (88.0482)  Acc@5: 100.0000 (98.5380)  time: 0.6684  data: 0.0016  max mem: 2372\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.0742  Acc@1: 87.5000 (88.0180)  Acc@5: 100.0000 (98.4116)  time: 0.6687  data: 0.0015  max mem: 2372\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.2289  Acc@1: 87.5000 (87.8599)  Acc@5: 100.0000 (98.3312)  time: 0.6692  data: 0.0026  max mem: 2372\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.1860  Acc@1: 87.5000 (88.0597)  Acc@5: 100.0000 (98.3209)  time: 0.6695  data: 0.0030  max mem: 2372\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.3015  Acc@1: 87.5000 (87.9443)  Acc@5: 100.0000 (98.2524)  time: 0.6690  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.0546  Acc@1: 87.5000 (88.1787)  Acc@5: 100.0000 (98.2749)  time: 0.6691  data: 0.0017  max mem: 2372\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.0254  Acc@1: 93.7500 (88.2846)  Acc@5: 100.0000 (98.3225)  time: 0.6700  data: 0.0025  max mem: 2372\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.0966  Acc@1: 87.5000 (88.2780)  Acc@5: 100.0000 (98.3402)  time: 0.6694  data: 0.0024  max mem: 2372\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1173  Acc@1: 87.5000 (88.1474)  Acc@5: 100.0000 (98.3068)  time: 0.6690  data: 0.0020  max mem: 2372\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.4291  Acc@1: 87.5000 (88.0987)  Acc@5: 100.0000 (98.3238)  time: 0.6686  data: 0.0022  max mem: 2372\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.5284  Acc@1: 81.2500 (87.8921)  Acc@5: 100.0000 (98.2703)  time: 0.6684  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0853  Acc@1: 87.5000 (87.8781)  Acc@5: 100.0000 (98.3096)  time: 0.6676  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.0904  Acc@1: 87.5000 (87.8436)  Acc@5: 100.0000 (98.3677)  time: 0.6674  data: 0.0018  max mem: 2372\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1491  Acc@1: 87.5000 (87.7699)  Acc@5: 100.0000 (98.3389)  time: 0.6673  data: 0.0019  max mem: 2372\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2961  Acc@1: 87.5000 (87.7814)  Acc@5: 100.0000 (98.3119)  time: 0.6666  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.6022  Acc@1: 87.5000 (87.7600)  Acc@5: 100.0000 (98.2800)  time: 0.6503  data: 0.0008  max mem: 2372\n",
            "Train: Epoch[5/5] Total time: 0:03:28 (0.6674 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.6022  Acc@1: 87.5000 (87.7600)  Acc@5: 100.0000 (98.2800)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:39  Loss: 0.4949 (0.4949)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6196  data: 0.2233  max mem: 2372\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.4524 (0.4691)  Acc@1: 93.7500 (93.1818)  Acc@5: 100.0000 (99.4318)  time: 0.4509  data: 0.0208  max mem: 2372\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.4427 (0.5171)  Acc@1: 93.7500 (92.2619)  Acc@5: 100.0000 (99.7024)  time: 0.4342  data: 0.0009  max mem: 2372\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3931 (0.4850)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (99.7984)  time: 0.4336  data: 0.0023  max mem: 2372\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4260 (0.4748)  Acc@1: 93.7500 (92.8354)  Acc@5: 100.0000 (99.8476)  time: 0.4332  data: 0.0019  max mem: 2372\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3854 (0.4554)  Acc@1: 93.7500 (93.6275)  Acc@5: 100.0000 (99.7549)  time: 0.4332  data: 0.0011  max mem: 2372\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3854 (0.4521)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.7951)  time: 0.4330  data: 0.0022  max mem: 2372\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3854 (0.4503)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.8000)  time: 0.4221  data: 0.0022  max mem: 2372\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4338 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.800 loss 0.450\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:37  Loss: 0.5508 (0.5508)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6018  data: 0.2031  max mem: 2372\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.5508 (0.5424)  Acc@1: 93.7500 (95.4545)  Acc@5: 100.0000 (99.4318)  time: 0.4488  data: 0.0195  max mem: 2372\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.5631 (0.6228)  Acc@1: 93.7500 (94.0476)  Acc@5: 100.0000 (99.4048)  time: 0.4332  data: 0.0028  max mem: 2372\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6539 (0.6270)  Acc@1: 93.7500 (93.1452)  Acc@5: 100.0000 (98.9919)  time: 0.4331  data: 0.0029  max mem: 2372\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6045 (0.6057)  Acc@1: 93.7500 (93.5976)  Acc@5: 100.0000 (99.0854)  time: 0.4330  data: 0.0009  max mem: 2372\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5034 (0.5978)  Acc@1: 93.7500 (93.2598)  Acc@5: 100.0000 (99.1422)  time: 0.4329  data: 0.0030  max mem: 2372\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.4732 (0.5745)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.2828)  time: 0.4334  data: 0.0033  max mem: 2372\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4657 (0.5666)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.3000)  time: 0.4224  data: 0.0033  max mem: 2372\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4336 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.300 loss 0.567\n",
            "[Average accuracy till task2]\tAcc@1: 93.8000\tAcc@5: 99.5500\tLoss: 0.5085\tForgetting: 4.3000\tBackward: -4.3000\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:04:43  Lr: 0.001875  Loss: 2.0689  Acc@1: 0.0000 (0.0000)  Acc@5: 50.0000 (50.0000)  time: 0.9056  data: 0.2850  max mem: 2372\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: 1.7600  Acc@1: 43.7500 (43.7500)  Acc@5: 81.2500 (80.1136)  time: 0.6886  data: 0.0285  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 1.5861  Acc@1: 62.5000 (56.8452)  Acc@5: 93.7500 (86.0119)  time: 0.6664  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 1.4166  Acc@1: 75.0000 (63.3065)  Acc@5: 93.7500 (89.9194)  time: 0.6658  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 1.3184  Acc@1: 81.2500 (67.3780)  Acc@5: 100.0000 (91.4634)  time: 0.6673  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.8066  Acc@1: 81.2500 (70.4657)  Acc@5: 93.7500 (91.9118)  time: 0.6681  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.7815  Acc@1: 81.2500 (71.9262)  Acc@5: 93.7500 (92.4180)  time: 0.6684  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.6514  Acc@1: 81.2500 (72.9754)  Acc@5: 93.7500 (92.6056)  time: 0.6703  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.5085  Acc@1: 81.2500 (73.9198)  Acc@5: 100.0000 (93.2870)  time: 0.6711  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.9265  Acc@1: 81.2500 (74.5192)  Acc@5: 100.0000 (93.7500)  time: 0.6712  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.5333  Acc@1: 81.2500 (75.8045)  Acc@5: 100.0000 (94.1213)  time: 0.6714  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.4291  Acc@1: 87.5000 (76.5766)  Acc@5: 100.0000 (94.4820)  time: 0.6709  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3929  Acc@1: 81.2500 (77.0661)  Acc@5: 100.0000 (94.7314)  time: 0.6703  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.4242  Acc@1: 81.2500 (77.5763)  Acc@5: 100.0000 (94.8950)  time: 0.6709  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.5253  Acc@1: 87.5000 (77.7926)  Acc@5: 93.7500 (94.9911)  time: 0.6716  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.6527  Acc@1: 87.5000 (78.2699)  Acc@5: 93.7500 (95.0331)  time: 0.6703  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1314  Acc@1: 87.5000 (78.6879)  Acc@5: 100.0000 (95.2252)  time: 0.6683  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.5731  Acc@1: 81.2500 (79.0936)  Acc@5: 100.0000 (95.4313)  time: 0.6671  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.3656  Acc@1: 87.5000 (79.5235)  Acc@5: 100.0000 (95.5456)  time: 0.6670  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1265  Acc@1: 81.2500 (79.6793)  Acc@5: 100.0000 (95.6806)  time: 0.6669  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.3079  Acc@1: 81.2500 (79.8507)  Acc@5: 100.0000 (95.8333)  time: 0.6665  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2202  Acc@1: 87.5000 (80.2133)  Acc@5: 100.0000 (95.9419)  time: 0.6663  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2080  Acc@1: 87.5000 (80.5430)  Acc@5: 100.0000 (96.0973)  time: 0.6662  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.3346  Acc@1: 87.5000 (80.6818)  Acc@5: 100.0000 (96.1310)  time: 0.6663  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.6370  Acc@1: 87.5000 (81.0685)  Acc@5: 100.0000 (96.2915)  time: 0.6658  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.6803  Acc@1: 87.5000 (81.2749)  Acc@5: 100.0000 (96.3645)  time: 0.6658  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0888  Acc@1: 81.2500 (81.4176)  Acc@5: 100.0000 (96.4559)  time: 0.6662  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1920  Acc@1: 81.2500 (81.5959)  Acc@5: 100.0000 (96.4483)  time: 0.6661  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.3807  Acc@1: 81.2500 (81.6726)  Acc@5: 100.0000 (96.5080)  time: 0.6662  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.0057  Acc@1: 81.2500 (81.7869)  Acc@5: 100.0000 (96.5421)  time: 0.6665  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1086  Acc@1: 87.5000 (82.0183)  Acc@5: 100.0000 (96.6362)  time: 0.6666  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3979  Acc@1: 87.5000 (82.1744)  Acc@5: 100.0000 (96.6640)  time: 0.6661  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0176  Acc@1: 87.5000 (82.2400)  Acc@5: 100.0000 (96.6800)  time: 0.6498  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:29 (0.6678 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0176  Acc@1: 87.5000 (82.2400)  Acc@5: 100.0000 (96.6800)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:00  Lr: 0.001875  Loss: 0.0539  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7689  data: 0.1430  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:24  Lr: 0.001875  Loss: -0.0274  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (98.2955)  time: 0.6762  data: 0.0143  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:16  Lr: 0.001875  Loss: 0.1382  Acc@1: 87.5000 (85.1190)  Acc@5: 100.0000 (98.8095)  time: 0.6668  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 0.4259  Acc@1: 87.5000 (85.4839)  Acc@5: 100.0000 (98.7903)  time: 0.6660  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: 0.2121  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (98.9329)  time: 0.6657  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:55  Lr: 0.001875  Loss: 0.0102  Acc@1: 87.5000 (86.2745)  Acc@5: 100.0000 (99.0196)  time: 0.6659  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:48  Lr: 0.001875  Loss: 0.1594  Acc@1: 87.5000 (86.0656)  Acc@5: 100.0000 (98.8730)  time: 0.6662  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.1406  Acc@1: 87.5000 (86.5317)  Acc@5: 100.0000 (98.9437)  time: 0.6668  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.2326  Acc@1: 87.5000 (86.9599)  Acc@5: 100.0000 (98.9198)  time: 0.6665  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:28  Lr: 0.001875  Loss: 0.1404  Acc@1: 87.5000 (86.6758)  Acc@5: 100.0000 (98.8324)  time: 0.6660  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.2429  Acc@1: 81.2500 (86.5718)  Acc@5: 100.0000 (98.7005)  time: 0.6663  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0830  Acc@1: 87.5000 (86.6554)  Acc@5: 100.0000 (98.6486)  time: 0.6659  data: 0.0028  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: 0.2449  Acc@1: 93.7500 (86.9318)  Acc@5: 100.0000 (98.7087)  time: 0.6657  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1129  Acc@1: 87.5000 (86.7844)  Acc@5: 100.0000 (98.6164)  time: 0.6660  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.4387  Acc@1: 87.5000 (86.8794)  Acc@5: 100.0000 (98.6259)  time: 0.6670  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.1869  Acc@1: 87.5000 (86.5480)  Acc@5: 100.0000 (98.5927)  time: 0.6670  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0167  Acc@1: 87.5000 (86.8401)  Acc@5: 100.0000 (98.5637)  time: 0.6665  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.1126  Acc@1: 87.5000 (86.9518)  Acc@5: 100.0000 (98.4649)  time: 0.6664  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.2868  Acc@1: 81.2500 (86.6367)  Acc@5: 100.0000 (98.3425)  time: 0.6664  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.4277  Acc@1: 81.2500 (86.6492)  Acc@5: 100.0000 (98.4293)  time: 0.6667  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.3059  Acc@1: 87.5000 (86.5983)  Acc@5: 100.0000 (98.3831)  time: 0.6662  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: -0.0796  Acc@1: 87.5000 (86.6114)  Acc@5: 100.0000 (98.4005)  time: 0.6662  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.1099  Acc@1: 87.5000 (86.5950)  Acc@5: 100.0000 (98.3597)  time: 0.6662  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2583  Acc@1: 87.5000 (86.5260)  Acc@5: 100.0000 (98.2684)  time: 0.6659  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0861  Acc@1: 87.5000 (86.6701)  Acc@5: 100.0000 (98.3143)  time: 0.6660  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:41  Lr: 0.001875  Loss: -0.0061  Acc@1: 87.5000 (86.8028)  Acc@5: 100.0000 (98.3317)  time: 0.6657  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.4274  Acc@1: 87.5000 (86.7577)  Acc@5: 100.0000 (98.2998)  time: 0.6664  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1099  Acc@1: 87.5000 (86.8542)  Acc@5: 100.0000 (98.3164)  time: 0.6666  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:21  Lr: 0.001875  Loss: 0.4666  Acc@1: 87.5000 (86.8105)  Acc@5: 100.0000 (98.3541)  time: 0.6659  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2973  Acc@1: 81.2500 (86.7268)  Acc@5: 100.0000 (98.3033)  time: 0.6661  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0665  Acc@1: 81.2500 (86.6694)  Acc@5: 100.0000 (98.3389)  time: 0.6659  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:01  Lr: 0.001875  Loss: 0.4741  Acc@1: 81.2500 (86.5957)  Acc@5: 100.0000 (98.3119)  time: 0.6658  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1366  Acc@1: 81.2500 (86.6200)  Acc@5: 100.0000 (98.3200)  time: 0.6497  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:28 (0.6658 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1366  Acc@1: 81.2500 (86.6200)  Acc@5: 100.0000 (98.3200)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:18  Lr: 0.001875  Loss: 0.0143  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.8272  data: 0.2055  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: 0.1646  Acc@1: 93.7500 (88.6364)  Acc@5: 100.0000 (99.4318)  time: 0.6818  data: 0.0195  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.3117  Acc@1: 87.5000 (86.3095)  Acc@5: 100.0000 (98.5119)  time: 0.6673  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: -0.0654  Acc@1: 87.5000 (88.1048)  Acc@5: 100.0000 (98.9919)  time: 0.6661  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.1595  Acc@1: 93.7500 (88.7195)  Acc@5: 100.0000 (99.0854)  time: 0.6661  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: -0.0155  Acc@1: 87.5000 (88.4804)  Acc@5: 100.0000 (98.8971)  time: 0.6668  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.5092  Acc@1: 87.5000 (87.7049)  Acc@5: 100.0000 (98.8730)  time: 0.6667  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: -0.0190  Acc@1: 87.5000 (88.3803)  Acc@5: 100.0000 (98.7676)  time: 0.6667  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.1700  Acc@1: 93.7500 (88.5802)  Acc@5: 100.0000 (98.7654)  time: 0.6668  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.3908  Acc@1: 87.5000 (87.9808)  Acc@5: 100.0000 (98.6264)  time: 0.6676  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0831  Acc@1: 87.5000 (87.7475)  Acc@5: 93.7500 (98.3292)  time: 0.6680  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: -0.0319  Acc@1: 87.5000 (88.1757)  Acc@5: 100.0000 (98.4234)  time: 0.6677  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: -0.0601  Acc@1: 87.5000 (87.9649)  Acc@5: 100.0000 (98.2438)  time: 0.6669  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.6304  Acc@1: 81.2500 (87.4523)  Acc@5: 100.0000 (98.1393)  time: 0.6662  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.2652  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.2270)  time: 0.6668  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.1553  Acc@1: 87.5000 (87.3344)  Acc@5: 100.0000 (98.2202)  time: 0.6674  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.2564  Acc@1: 87.5000 (87.0730)  Acc@5: 100.0000 (98.3307)  time: 0.6668  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.1997  Acc@1: 87.5000 (87.1345)  Acc@5: 100.0000 (98.3553)  time: 0.6672  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.1566  Acc@1: 87.5000 (87.0511)  Acc@5: 100.0000 (98.4116)  time: 0.6669  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0601  Acc@1: 87.5000 (87.0746)  Acc@5: 100.0000 (98.4620)  time: 0.6666  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0329  Acc@1: 87.5000 (87.2823)  Acc@5: 100.0000 (98.5386)  time: 0.6669  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.8766  Acc@1: 87.5000 (87.1149)  Acc@5: 100.0000 (98.4301)  time: 0.6670  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.0116  Acc@1: 87.5000 (87.1606)  Acc@5: 100.0000 (98.4729)  time: 0.6670  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.4102  Acc@1: 81.2500 (86.8777)  Acc@5: 100.0000 (98.4037)  time: 0.6667  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0257  Acc@1: 87.5000 (86.9554)  Acc@5: 100.0000 (98.4180)  time: 0.6667  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2542  Acc@1: 87.5000 (87.0020)  Acc@5: 100.0000 (98.4811)  time: 0.6666  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.1895  Acc@1: 87.5000 (87.0211)  Acc@5: 100.0000 (98.5393)  time: 0.6668  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1292  Acc@1: 87.5000 (87.0618)  Acc@5: 100.0000 (98.5932)  time: 0.6668  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2741  Acc@1: 87.5000 (87.1441)  Acc@5: 100.0000 (98.5988)  time: 0.6668  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1985  Acc@1: 87.5000 (87.1564)  Acc@5: 100.0000 (98.5610)  time: 0.6668  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: -0.0153  Acc@1: 87.5000 (87.2301)  Acc@5: 100.0000 (98.5880)  time: 0.6673  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.0005  Acc@1: 87.5000 (87.2789)  Acc@5: 100.0000 (98.5732)  time: 0.6672  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1655  Acc@1: 87.5000 (87.3400)  Acc@5: 100.0000 (98.5800)  time: 0.6507  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[3/5] Total time: 0:03:28 (0.6666 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1655  Acc@1: 87.5000 (87.3400)  Acc@5: 100.0000 (98.5800)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:03:59  Lr: 0.001875  Loss: -0.0788  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7665  data: 0.1418  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.1653  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (98.8636)  time: 0.6779  data: 0.0140  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.1468  Acc@1: 87.5000 (89.8810)  Acc@5: 100.0000 (99.1071)  time: 0.6677  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 0.1364  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (98.9919)  time: 0.6670  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: 0.1357  Acc@1: 87.5000 (88.7195)  Acc@5: 100.0000 (99.0854)  time: 0.6670  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:55  Lr: 0.001875  Loss: 0.4020  Acc@1: 87.5000 (89.4608)  Acc@5: 100.0000 (99.2647)  time: 0.6663  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.3108  Acc@1: 93.7500 (89.1393)  Acc@5: 100.0000 (99.1803)  time: 0.6664  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.4678  Acc@1: 87.5000 (89.0845)  Acc@5: 100.0000 (99.1197)  time: 0.6666  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.0211  Acc@1: 87.5000 (89.1975)  Acc@5: 100.0000 (98.9969)  time: 0.6682  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.0838  Acc@1: 87.5000 (89.2857)  Acc@5: 100.0000 (99.0385)  time: 0.6693  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0219  Acc@1: 87.5000 (89.1089)  Acc@5: 100.0000 (99.0099)  time: 0.6685  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: -0.0513  Acc@1: 93.7500 (88.9640)  Acc@5: 100.0000 (98.9865)  time: 0.6683  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1625  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (98.8120)  time: 0.6682  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.0879  Acc@1: 87.5000 (88.5496)  Acc@5: 100.0000 (98.8550)  time: 0.6678  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.1189  Acc@1: 87.5000 (88.6082)  Acc@5: 100.0000 (98.9362)  time: 0.6679  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.3656  Acc@1: 87.5000 (88.4934)  Acc@5: 100.0000 (98.9652)  time: 0.6674  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.4277  Acc@1: 87.5000 (88.6646)  Acc@5: 100.0000 (98.9519)  time: 0.6666  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.5158  Acc@1: 87.5000 (88.5965)  Acc@5: 100.0000 (98.9401)  time: 0.6668  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.0178  Acc@1: 87.5000 (88.6050)  Acc@5: 100.0000 (98.8605)  time: 0.6671  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0034  Acc@1: 87.5000 (88.7107)  Acc@5: 100.0000 (98.8874)  time: 0.6673  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2858  Acc@1: 93.7500 (88.7127)  Acc@5: 100.0000 (98.8806)  time: 0.6678  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.6269  Acc@1: 87.5000 (88.6848)  Acc@5: 100.0000 (98.8744)  time: 0.6682  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.0273  Acc@1: 87.5000 (88.6595)  Acc@5: 100.0000 (98.8971)  time: 0.6682  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0233  Acc@1: 87.5000 (88.6093)  Acc@5: 100.0000 (98.8907)  time: 0.6678  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0518  Acc@1: 87.5000 (88.4855)  Acc@5: 100.0000 (98.9108)  time: 0.6679  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2841  Acc@1: 87.5000 (88.5209)  Acc@5: 100.0000 (98.9293)  time: 0.6675  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0473  Acc@1: 93.7500 (88.5776)  Acc@5: 100.0000 (98.9464)  time: 0.6673  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0111  Acc@1: 87.5000 (88.5378)  Acc@5: 100.0000 (98.9161)  time: 0.6676  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2126  Acc@1: 87.5000 (88.5231)  Acc@5: 100.0000 (98.8879)  time: 0.6673  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0030  Acc@1: 87.5000 (88.5095)  Acc@5: 100.0000 (98.8832)  time: 0.6676  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.4178  Acc@1: 87.5000 (88.3513)  Acc@5: 100.0000 (98.8787)  time: 0.6679  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1498  Acc@1: 87.5000 (88.1833)  Acc@5: 100.0000 (98.8746)  time: 0.6685  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0530  Acc@1: 87.5000 (88.1400)  Acc@5: 100.0000 (98.8800)  time: 0.6514  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[4/5] Total time: 0:03:28 (0.6671 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0530  Acc@1: 87.5000 (88.1400)  Acc@5: 100.0000 (98.8800)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:03:57  Lr: 0.001875  Loss: 0.2174  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7602  data: 0.1360  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.1766  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (98.8636)  time: 0.6793  data: 0.0127  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.1508  Acc@1: 93.7500 (90.1786)  Acc@5: 100.0000 (99.4048)  time: 0.6705  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.2379  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (98.9919)  time: 0.6701  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0996  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (98.6280)  time: 0.6704  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: -0.0569  Acc@1: 93.7500 (89.2157)  Acc@5: 100.0000 (98.7745)  time: 0.6703  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.3371  Acc@1: 87.5000 (88.6270)  Acc@5: 100.0000 (98.6680)  time: 0.6700  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.4329  Acc@1: 87.5000 (88.6444)  Acc@5: 100.0000 (98.6796)  time: 0.6699  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.1050  Acc@1: 87.5000 (88.2716)  Acc@5: 100.0000 (98.3796)  time: 0.6699  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.1315  Acc@1: 87.5000 (88.2555)  Acc@5: 100.0000 (98.4203)  time: 0.6697  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0860  Acc@1: 87.5000 (88.2426)  Acc@5: 100.0000 (98.3911)  time: 0.6694  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.1665  Acc@1: 87.5000 (88.1194)  Acc@5: 100.0000 (98.4234)  time: 0.6692  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.2086  Acc@1: 87.5000 (88.1715)  Acc@5: 100.0000 (98.4504)  time: 0.6695  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.3001  Acc@1: 87.5000 (88.2156)  Acc@5: 100.0000 (98.4733)  time: 0.6695  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.3544  Acc@1: 87.5000 (87.9876)  Acc@5: 100.0000 (98.4486)  time: 0.6697  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.2609  Acc@1: 87.5000 (88.0795)  Acc@5: 100.0000 (98.4685)  time: 0.6699  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.0654  Acc@1: 87.5000 (88.0047)  Acc@5: 100.0000 (98.5248)  time: 0.6696  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.0564  Acc@1: 87.5000 (88.1213)  Acc@5: 100.0000 (98.5746)  time: 0.6699  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.0272  Acc@1: 87.5000 (88.0870)  Acc@5: 100.0000 (98.6533)  time: 0.6700  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0633  Acc@1: 87.5000 (88.1545)  Acc@5: 100.0000 (98.6584)  time: 0.6701  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.4091  Acc@1: 87.5000 (88.0597)  Acc@5: 100.0000 (98.6007)  time: 0.6698  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: -0.0636  Acc@1: 87.5000 (87.9739)  Acc@5: 100.0000 (98.5486)  time: 0.6690  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.4735  Acc@1: 87.5000 (88.0090)  Acc@5: 100.0000 (98.5577)  time: 0.6697  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.0280  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (98.5931)  time: 0.6702  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2078  Acc@1: 93.7500 (88.2780)  Acc@5: 100.0000 (98.5996)  time: 0.6697  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0861  Acc@1: 93.7500 (88.4462)  Acc@5: 100.0000 (98.5807)  time: 0.6693  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0493  Acc@1: 93.7500 (88.4579)  Acc@5: 100.0000 (98.5632)  time: 0.6701  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1826  Acc@1: 87.5000 (88.5148)  Acc@5: 100.0000 (98.6162)  time: 0.6704  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.3475  Acc@1: 87.5000 (88.5454)  Acc@5: 100.0000 (98.6210)  time: 0.6696  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0349  Acc@1: 87.5000 (88.4880)  Acc@5: 100.0000 (98.6684)  time: 0.6697  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: -0.0320  Acc@1: 87.5000 (88.5174)  Acc@5: 100.0000 (98.6711)  time: 0.6705  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3123  Acc@1: 87.5000 (88.4847)  Acc@5: 100.0000 (98.6937)  time: 0.6706  data: 0.0004  max mem: 2373\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0260  Acc@1: 87.5000 (88.4800)  Acc@5: 100.0000 (98.7000)  time: 0.6534  data: 0.0004  max mem: 2373\n",
            "Train: Epoch[5/5] Total time: 0:03:29 (0.6694 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0260  Acc@1: 87.5000 (88.4800)  Acc@5: 100.0000 (98.7000)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:38  Loss: 0.5178 (0.5178)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6140  data: 0.2152  max mem: 2373\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5002 (0.4788)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4513  data: 0.0201  max mem: 2373\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.4875 (0.5342)  Acc@1: 93.7500 (89.8810)  Acc@5: 100.0000 (99.7024)  time: 0.4349  data: 0.0012  max mem: 2373\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.4442 (0.5114)  Acc@1: 93.7500 (90.3226)  Acc@5: 100.0000 (99.7984)  time: 0.4346  data: 0.0019  max mem: 2373\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4416 (0.4966)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (99.8476)  time: 0.4349  data: 0.0011  max mem: 2373\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3982 (0.4744)  Acc@1: 93.7500 (91.6667)  Acc@5: 100.0000 (99.7549)  time: 0.4351  data: 0.0010  max mem: 2373\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3946 (0.4658)  Acc@1: 93.7500 (91.7008)  Acc@5: 100.0000 (99.7951)  time: 0.4348  data: 0.0026  max mem: 2373\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3946 (0.4639)  Acc@1: 93.7500 (91.9000)  Acc@5: 100.0000 (99.8000)  time: 0.4239  data: 0.0026  max mem: 2373\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4352 s / it)\n",
            "* Acc@1 91.900 Acc@5 99.800 loss 0.464\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:39  Loss: 0.6183 (0.6183)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6300  data: 0.2311  max mem: 2373\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:24  Loss: 0.5662 (0.6001)  Acc@1: 100.0000 (95.4545)  Acc@5: 100.0000 (98.2955)  time: 0.4530  data: 0.0214  max mem: 2373\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.6122 (0.6841)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4354  data: 0.0014  max mem: 2373\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6694 (0.6667)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (98.3871)  time: 0.4345  data: 0.0018  max mem: 2373\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6524 (0.6528)  Acc@1: 93.7500 (92.5305)  Acc@5: 100.0000 (98.6280)  time: 0.4344  data: 0.0009  max mem: 2373\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5743 (0.6429)  Acc@1: 93.7500 (92.4020)  Acc@5: 100.0000 (98.7745)  time: 0.4348  data: 0.0024  max mem: 2373\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5022 (0.6258)  Acc@1: 93.7500 (92.4180)  Acc@5: 100.0000 (98.9754)  time: 0.4346  data: 0.0027  max mem: 2373\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4910 (0.6172)  Acc@1: 93.7500 (92.5000)  Acc@5: 100.0000 (99.0000)  time: 0.4238  data: 0.0027  max mem: 2373\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4353 s / it)\n",
            "* Acc@1 92.500 Acc@5 99.000 loss 0.617\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:33  Loss: 0.2563 (0.2563)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5335  data: 0.1369  max mem: 2373\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.4239 (0.4755)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4443  data: 0.0129  max mem: 2373\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:18  Loss: 0.4874 (0.5017)  Acc@1: 87.5000 (90.1786)  Acc@5: 100.0000 (98.8095)  time: 0.4357  data: 0.0013  max mem: 2373\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4954 (0.4892)  Acc@1: 93.7500 (90.9274)  Acc@5: 100.0000 (98.9919)  time: 0.4351  data: 0.0013  max mem: 2373\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.3972 (0.4820)  Acc@1: 93.7500 (91.4634)  Acc@5: 100.0000 (99.0854)  time: 0.4344  data: 0.0011  max mem: 2373\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4298 (0.4836)  Acc@1: 93.7500 (91.2990)  Acc@5: 100.0000 (98.8971)  time: 0.4347  data: 0.0035  max mem: 2373\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.4516 (0.4939)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (99.0779)  time: 0.4348  data: 0.0029  max mem: 2373\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5528 (0.4960)  Acc@1: 87.5000 (91.2000)  Acc@5: 100.0000 (99.0000)  time: 0.4241  data: 0.0028  max mem: 2373\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4340 s / it)\n",
            "* Acc@1 91.200 Acc@5 99.000 loss 0.496\n",
            "[Average accuracy till task3]\tAcc@1: 91.8667\tAcc@5: 99.2667\tLoss: 0.5257\tForgetting: 3.7500\tBackward: -3.7500\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:04:34  Lr: 0.001875  Loss: 2.0709  Acc@1: 18.7500 (18.7500)  Acc@5: 37.5000 (37.5000)  time: 0.8764  data: 0.2545  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: 1.7976  Acc@1: 56.2500 (52.2727)  Acc@5: 87.5000 (81.2500)  time: 0.6864  data: 0.0245  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 1.4527  Acc@1: 62.5000 (63.3929)  Acc@5: 93.7500 (87.2024)  time: 0.6680  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.9906  Acc@1: 81.2500 (69.1532)  Acc@5: 100.0000 (90.9274)  time: 0.6689  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.7714  Acc@1: 81.2500 (71.9512)  Acc@5: 100.0000 (92.2256)  time: 0.6692  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.8151  Acc@1: 81.2500 (73.8971)  Acc@5: 100.0000 (93.5049)  time: 0.6694  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.4684  Acc@1: 81.2500 (75.6148)  Acc@5: 100.0000 (94.0574)  time: 0.6708  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.7398  Acc@1: 87.5000 (77.3768)  Acc@5: 100.0000 (94.5423)  time: 0.6716  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.7778  Acc@1: 87.5000 (78.3951)  Acc@5: 100.0000 (95.0617)  time: 0.6723  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: 0.1111  Acc@1: 87.5000 (79.3269)  Acc@5: 100.0000 (95.4670)  time: 0.6734  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.5088  Acc@1: 87.5000 (79.9505)  Acc@5: 100.0000 (95.5446)  time: 0.6735  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.5224  Acc@1: 87.5000 (80.3491)  Acc@5: 100.0000 (95.6644)  time: 0.6727  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.6104  Acc@1: 81.2500 (80.8884)  Acc@5: 100.0000 (95.8678)  time: 0.6713  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:03  Lr: 0.001875  Loss: 0.3457  Acc@1: 81.2500 (81.0592)  Acc@5: 100.0000 (95.9924)  time: 0.6704  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.1584  Acc@1: 87.5000 (81.3830)  Acc@5: 100.0000 (96.1436)  time: 0.6693  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0582  Acc@1: 87.5000 (81.7053)  Acc@5: 100.0000 (96.3576)  time: 0.6692  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.4504  Acc@1: 87.5000 (82.1040)  Acc@5: 100.0000 (96.5839)  time: 0.6691  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.0787  Acc@1: 87.5000 (82.4196)  Acc@5: 100.0000 (96.6740)  time: 0.6683  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.0637  Acc@1: 87.5000 (82.8729)  Acc@5: 100.0000 (96.8232)  time: 0.6672  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1845  Acc@1: 87.5000 (83.1479)  Acc@5: 100.0000 (96.7932)  time: 0.6666  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.4508  Acc@1: 87.5000 (83.4888)  Acc@5: 100.0000 (96.9216)  time: 0.6670  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.1249  Acc@1: 87.5000 (83.7085)  Acc@5: 100.0000 (97.0083)  time: 0.6675  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.0515  Acc@1: 87.5000 (83.9932)  Acc@5: 100.0000 (97.0588)  time: 0.6673  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.1318  Acc@1: 87.5000 (84.0368)  Acc@5: 100.0000 (97.0779)  time: 0.6669  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0410  Acc@1: 87.5000 (84.0249)  Acc@5: 100.0000 (97.1214)  time: 0.6667  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.1777  Acc@1: 87.5000 (84.1135)  Acc@5: 100.0000 (97.1614)  time: 0.6666  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0432  Acc@1: 87.5000 (84.4109)  Acc@5: 100.0000 (97.1983)  time: 0.6663  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2575  Acc@1: 87.5000 (84.4557)  Acc@5: 100.0000 (97.2325)  time: 0.6660  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.4239  Acc@1: 87.5000 (84.6308)  Acc@5: 100.0000 (97.3087)  time: 0.6661  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1672  Acc@1: 87.5000 (84.7509)  Acc@5: 100.0000 (97.3797)  time: 0.6664  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.3128  Acc@1: 87.5000 (84.7799)  Acc@5: 100.0000 (97.3837)  time: 0.6664  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.5661  Acc@1: 81.2500 (84.8875)  Acc@5: 100.0000 (97.4678)  time: 0.6665  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0146  Acc@1: 81.2500 (84.8800)  Acc@5: 100.0000 (97.4800)  time: 0.6505  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:29 (0.6685 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0146  Acc@1: 81.2500 (84.8800)  Acc@5: 100.0000 (97.4800)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:24  Lr: 0.001875  Loss: -0.0924  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8452  data: 0.2220  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: 0.1568  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (98.2955)  time: 0.6860  data: 0.0218  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.3366  Acc@1: 87.5000 (88.6905)  Acc@5: 100.0000 (97.9167)  time: 0.6695  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.6005  Acc@1: 87.5000 (89.5161)  Acc@5: 100.0000 (98.3871)  time: 0.6688  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.4546  Acc@1: 93.7500 (89.4817)  Acc@5: 100.0000 (98.4756)  time: 0.6684  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.1915  Acc@1: 93.7500 (89.2157)  Acc@5: 100.0000 (98.5294)  time: 0.6681  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.1911  Acc@1: 87.5000 (88.7295)  Acc@5: 100.0000 (98.2582)  time: 0.6678  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.1216  Acc@1: 87.5000 (88.3803)  Acc@5: 100.0000 (98.2394)  time: 0.6670  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0230  Acc@1: 93.7500 (88.6574)  Acc@5: 100.0000 (98.2253)  time: 0.6665  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.2558  Acc@1: 93.7500 (88.5989)  Acc@5: 100.0000 (97.9396)  time: 0.6675  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.1041  Acc@1: 87.5000 (88.6757)  Acc@5: 93.7500 (97.8960)  time: 0.6675  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: -0.0626  Acc@1: 87.5000 (88.6824)  Acc@5: 100.0000 (98.0293)  time: 0.6671  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1931  Acc@1: 87.5000 (88.5331)  Acc@5: 100.0000 (98.1921)  time: 0.6677  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.3181  Acc@1: 87.5000 (88.4542)  Acc@5: 100.0000 (98.2824)  time: 0.6680  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.2361  Acc@1: 87.5000 (88.0762)  Acc@5: 100.0000 (98.2713)  time: 0.6679  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: -0.0406  Acc@1: 87.5000 (88.1623)  Acc@5: 100.0000 (98.1788)  time: 0.6685  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.1287  Acc@1: 87.5000 (88.1988)  Acc@5: 100.0000 (98.2143)  time: 0.6685  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3277  Acc@1: 87.5000 (88.2310)  Acc@5: 100.0000 (98.2456)  time: 0.6684  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.1086  Acc@1: 93.7500 (88.6740)  Acc@5: 100.0000 (98.3080)  time: 0.6691  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.2692  Acc@1: 93.7500 (88.5471)  Acc@5: 100.0000 (98.2657)  time: 0.6698  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.3575  Acc@1: 87.5000 (88.4017)  Acc@5: 100.0000 (98.2276)  time: 0.6695  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.4684  Acc@1: 87.5000 (88.5071)  Acc@5: 100.0000 (98.2524)  time: 0.6687  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2033  Acc@1: 87.5000 (88.4898)  Acc@5: 100.0000 (98.2183)  time: 0.6690  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.1938  Acc@1: 87.5000 (88.3117)  Acc@5: 100.0000 (98.2413)  time: 0.6701  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.3427  Acc@1: 87.5000 (88.4336)  Acc@5: 100.0000 (98.3143)  time: 0.6702  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1400  Acc@1: 87.5000 (88.5209)  Acc@5: 100.0000 (98.3566)  time: 0.6698  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.1199  Acc@1: 87.5000 (88.6255)  Acc@5: 100.0000 (98.3238)  time: 0.6696  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2237  Acc@1: 87.5000 (88.6531)  Acc@5: 100.0000 (98.3164)  time: 0.6682  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.0841  Acc@1: 87.5000 (88.6566)  Acc@5: 100.0000 (98.3096)  time: 0.6676  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0980  Acc@1: 87.5000 (88.7887)  Acc@5: 100.0000 (98.3462)  time: 0.6688  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.3268  Acc@1: 87.5000 (88.7666)  Acc@5: 100.0000 (98.2973)  time: 0.6688  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2182  Acc@1: 87.5000 (88.7460)  Acc@5: 100.0000 (98.2717)  time: 0.6685  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.5166  Acc@1: 87.5000 (88.7200)  Acc@5: 100.0000 (98.2800)  time: 0.6519  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6683 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.5166  Acc@1: 87.5000 (88.7200)  Acc@5: 100.0000 (98.2800)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:21  Lr: 0.001875  Loss: 0.0749  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8348  data: 0.2133  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: -0.0319  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (98.8636)  time: 0.6837  data: 0.0209  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.0498  Acc@1: 87.5000 (88.6905)  Acc@5: 100.0000 (97.9167)  time: 0.6688  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: -0.0834  Acc@1: 87.5000 (88.5081)  Acc@5: 100.0000 (97.7823)  time: 0.6683  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.3780  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (97.7134)  time: 0.6684  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.2368  Acc@1: 87.5000 (87.8676)  Acc@5: 100.0000 (97.7941)  time: 0.6690  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.0311  Acc@1: 87.5000 (87.8074)  Acc@5: 100.0000 (98.0533)  time: 0.6688  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.1396  Acc@1: 87.5000 (87.4120)  Acc@5: 100.0000 (97.8873)  time: 0.6689  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.2679  Acc@1: 87.5000 (88.1944)  Acc@5: 100.0000 (98.0710)  time: 0.6688  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.1895  Acc@1: 93.7500 (87.9808)  Acc@5: 100.0000 (98.0769)  time: 0.6690  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.0212  Acc@1: 87.5000 (88.1807)  Acc@5: 100.0000 (98.2054)  time: 0.6698  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.1187  Acc@1: 87.5000 (88.2883)  Acc@5: 100.0000 (98.2545)  time: 0.6691  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3801  Acc@1: 87.5000 (88.2748)  Acc@5: 100.0000 (98.3471)  time: 0.6691  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.0792  Acc@1: 87.5000 (88.5496)  Acc@5: 100.0000 (98.3779)  time: 0.6688  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.3275  Acc@1: 87.5000 (88.2092)  Acc@5: 100.0000 (98.2713)  time: 0.6687  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0003  Acc@1: 87.5000 (88.3278)  Acc@5: 100.0000 (98.2616)  time: 0.6691  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0337  Acc@1: 87.5000 (88.2764)  Acc@5: 100.0000 (98.2143)  time: 0.6678  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.1191  Acc@1: 87.5000 (88.4868)  Acc@5: 100.0000 (98.2822)  time: 0.6668  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.1507  Acc@1: 87.5000 (88.2597)  Acc@5: 100.0000 (98.2390)  time: 0.6673  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0407  Acc@1: 87.5000 (88.1545)  Acc@5: 100.0000 (98.2330)  time: 0.6678  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2607  Acc@1: 87.5000 (88.3085)  Acc@5: 100.0000 (98.2587)  time: 0.6670  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2830  Acc@1: 93.7500 (88.2998)  Acc@5: 100.0000 (98.2227)  time: 0.6682  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.1109  Acc@1: 87.5000 (88.3767)  Acc@5: 100.0000 (98.2466)  time: 0.6691  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.1207  Acc@1: 87.5000 (88.4199)  Acc@5: 100.0000 (98.2955)  time: 0.6690  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.0370  Acc@1: 93.7500 (88.5633)  Acc@5: 100.0000 (98.2884)  time: 0.6698  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0717  Acc@1: 93.7500 (88.5707)  Acc@5: 100.0000 (98.3317)  time: 0.6693  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2427  Acc@1: 87.5000 (88.5536)  Acc@5: 100.0000 (98.3477)  time: 0.6687  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.4785  Acc@1: 87.5000 (88.5839)  Acc@5: 100.0000 (98.3625)  time: 0.6690  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1273  Acc@1: 93.7500 (88.6121)  Acc@5: 100.0000 (98.3541)  time: 0.6690  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.0050  Acc@1: 87.5000 (88.5954)  Acc@5: 100.0000 (98.3247)  time: 0.6695  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0601  Acc@1: 87.5000 (88.4967)  Acc@5: 100.0000 (98.3596)  time: 0.6699  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: -0.1294  Acc@1: 87.5000 (88.4043)  Acc@5: 100.0000 (98.3923)  time: 0.6696  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.4428  Acc@1: 87.5000 (88.4200)  Acc@5: 100.0000 (98.4000)  time: 0.6529  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5] Total time: 0:03:29 (0.6685 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.4428  Acc@1: 87.5000 (88.4200)  Acc@5: 100.0000 (98.4000)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:03:59  Lr: 0.001875  Loss: -0.1698  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7658  data: 0.1423  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: -0.1746  Acc@1: 93.7500 (92.6136)  Acc@5: 100.0000 (98.8636)  time: 0.6785  data: 0.0162  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.1822  Acc@1: 93.7500 (91.3690)  Acc@5: 100.0000 (98.8095)  time: 0.6682  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 0.1069  Acc@1: 93.7500 (89.5161)  Acc@5: 100.0000 (98.9919)  time: 0.6672  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.3463  Acc@1: 87.5000 (88.7195)  Acc@5: 100.0000 (98.9329)  time: 0.6678  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: -0.0994  Acc@1: 87.5000 (88.8480)  Acc@5: 100.0000 (99.1422)  time: 0.6682  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.0934  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (99.0779)  time: 0.6683  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.1752  Acc@1: 87.5000 (88.0282)  Acc@5: 100.0000 (99.1197)  time: 0.6681  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.1290  Acc@1: 93.7500 (88.7346)  Acc@5: 100.0000 (99.2284)  time: 0.6683  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1197  Acc@1: 93.7500 (89.0797)  Acc@5: 100.0000 (99.1071)  time: 0.6683  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0411  Acc@1: 93.7500 (89.1708)  Acc@5: 100.0000 (99.1955)  time: 0.6681  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.3835  Acc@1: 93.7500 (89.2455)  Acc@5: 100.0000 (99.2117)  time: 0.6680  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1536  Acc@1: 93.7500 (89.4628)  Acc@5: 100.0000 (99.1736)  time: 0.6682  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.2730  Acc@1: 87.5000 (89.2176)  Acc@5: 100.0000 (99.0935)  time: 0.6682  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.4028  Acc@1: 87.5000 (89.0957)  Acc@5: 100.0000 (99.1135)  time: 0.6680  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.3210  Acc@1: 87.5000 (88.6589)  Acc@5: 100.0000 (99.0894)  time: 0.6682  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3947  Acc@1: 87.5000 (88.7422)  Acc@5: 100.0000 (99.0683)  time: 0.6691  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.0656  Acc@1: 93.7500 (88.9985)  Acc@5: 100.0000 (99.1228)  time: 0.6694  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.0122  Acc@1: 93.7500 (89.0884)  Acc@5: 100.0000 (99.1367)  time: 0.6695  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0798  Acc@1: 93.7500 (89.2016)  Acc@5: 100.0000 (99.1492)  time: 0.6691  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.1396  Acc@1: 87.5000 (89.2102)  Acc@5: 100.0000 (99.1604)  time: 0.6689  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.1295  Acc@1: 93.7500 (89.3957)  Acc@5: 100.0000 (99.2002)  time: 0.6690  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2847  Acc@1: 87.5000 (89.2251)  Acc@5: 100.0000 (99.1799)  time: 0.6688  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.1307  Acc@1: 87.5000 (89.2857)  Acc@5: 100.0000 (99.1613)  time: 0.6687  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0886  Acc@1: 87.5000 (89.1857)  Acc@5: 100.0000 (99.1442)  time: 0.6688  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.0813  Acc@1: 87.5000 (89.1434)  Acc@5: 100.0000 (99.0787)  time: 0.6681  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0261  Acc@1: 87.5000 (89.1044)  Acc@5: 100.0000 (99.0900)  time: 0.6682  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2446  Acc@1: 87.5000 (89.2297)  Acc@5: 100.0000 (99.1006)  time: 0.6681  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1911  Acc@1: 87.5000 (89.1014)  Acc@5: 100.0000 (99.0881)  time: 0.6679  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2941  Acc@1: 87.5000 (89.1753)  Acc@5: 100.0000 (99.0979)  time: 0.6686  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.3841  Acc@1: 93.7500 (89.1819)  Acc@5: 100.0000 (99.0656)  time: 0.6689  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: -0.0911  Acc@1: 93.7500 (89.3288)  Acc@5: 100.0000 (99.0756)  time: 0.6692  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0932  Acc@1: 93.7500 (89.3400)  Acc@5: 100.0000 (99.0800)  time: 0.6524  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5] Total time: 0:03:29 (0.6680 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0932  Acc@1: 93.7500 (89.3400)  Acc@5: 100.0000 (99.0800)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:04:24  Lr: 0.001875  Loss: 0.0022  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8452  data: 0.2220  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: 0.0142  Acc@1: 93.7500 (89.2045)  Acc@5: 100.0000 (98.2955)  time: 0.6855  data: 0.0212  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.2158  Acc@1: 87.5000 (89.2857)  Acc@5: 100.0000 (97.9167)  time: 0.6691  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.3813  Acc@1: 87.5000 (89.3145)  Acc@5: 100.0000 (98.1855)  time: 0.6681  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.1429  Acc@1: 87.5000 (88.2622)  Acc@5: 100.0000 (98.1707)  time: 0.6679  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.1551  Acc@1: 87.5000 (88.3578)  Acc@5: 100.0000 (98.4069)  time: 0.6690  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: -0.0775  Acc@1: 87.5000 (88.9344)  Acc@5: 100.0000 (98.5656)  time: 0.6689  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: -0.0453  Acc@1: 93.7500 (89.6127)  Acc@5: 100.0000 (98.5035)  time: 0.6684  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0831  Acc@1: 93.7500 (89.6605)  Acc@5: 100.0000 (98.3025)  time: 0.6689  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1886  Acc@1: 93.7500 (90.1786)  Acc@5: 100.0000 (98.4203)  time: 0.6694  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0176  Acc@1: 93.7500 (90.3465)  Acc@5: 100.0000 (98.5149)  time: 0.6702  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.1753  Acc@1: 93.7500 (90.2027)  Acc@5: 100.0000 (98.5360)  time: 0.6702  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.0031  Acc@1: 93.7500 (90.2893)  Acc@5: 100.0000 (98.5021)  time: 0.6696  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0334  Acc@1: 93.7500 (90.2672)  Acc@5: 100.0000 (98.6164)  time: 0.6690  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: -0.0870  Acc@1: 93.7500 (90.6472)  Acc@5: 100.0000 (98.6259)  time: 0.6688  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: -0.0594  Acc@1: 93.7500 (90.8526)  Acc@5: 100.0000 (98.6755)  time: 0.6693  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.2441  Acc@1: 93.7500 (91.0326)  Acc@5: 100.0000 (98.6801)  time: 0.6697  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.1029  Acc@1: 93.7500 (91.3012)  Acc@5: 100.0000 (98.6842)  time: 0.6689  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.3172  Acc@1: 93.7500 (91.4019)  Acc@5: 100.0000 (98.6878)  time: 0.6682  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.4260  Acc@1: 93.7500 (91.2304)  Acc@5: 100.0000 (98.7238)  time: 0.6686  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0473  Acc@1: 93.7500 (91.2002)  Acc@5: 100.0000 (98.7873)  time: 0.6688  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.4216  Acc@1: 87.5000 (91.1434)  Acc@5: 100.0000 (98.8152)  time: 0.6684  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.1139  Acc@1: 93.7500 (91.1765)  Acc@5: 100.0000 (98.8122)  time: 0.6681  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.0548  Acc@1: 93.7500 (91.0173)  Acc@5: 100.0000 (98.7554)  time: 0.6688  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.3699  Acc@1: 87.5000 (90.9492)  Acc@5: 100.0000 (98.7552)  time: 0.6688  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1902  Acc@1: 93.7500 (91.0359)  Acc@5: 100.0000 (98.7799)  time: 0.6685  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0274  Acc@1: 93.7500 (91.0680)  Acc@5: 100.0000 (98.7787)  time: 0.6690  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: -0.1573  Acc@1: 93.7500 (90.9594)  Acc@5: 100.0000 (98.7546)  time: 0.6695  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0034  Acc@1: 87.5000 (90.9920)  Acc@5: 100.0000 (98.7767)  time: 0.6689  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.5614  Acc@1: 93.7500 (90.9794)  Acc@5: 100.0000 (98.7973)  time: 0.6689  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1673  Acc@1: 87.5000 (90.8846)  Acc@5: 100.0000 (98.7334)  time: 0.6688  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2762  Acc@1: 87.5000 (90.8159)  Acc@5: 93.7500 (98.6736)  time: 0.6684  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0397  Acc@1: 87.5000 (90.8400)  Acc@5: 93.7500 (98.6800)  time: 0.6520  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5] Total time: 0:03:29 (0.6687 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0397  Acc@1: 87.5000 (90.8400)  Acc@5: 93.7500 (98.6800)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:39  Loss: 0.5862 (0.5862)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6287  data: 0.2305  max mem: 2373\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5155 (0.4946)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4514  data: 0.0213  max mem: 2373\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.5155 (0.5422)  Acc@1: 93.7500 (89.2857)  Acc@5: 100.0000 (99.4048)  time: 0.4336  data: 0.0013  max mem: 2373\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5011 (0.5325)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (99.3952)  time: 0.4338  data: 0.0013  max mem: 2373\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4808 (0.5196)  Acc@1: 87.5000 (89.1768)  Acc@5: 100.0000 (99.5427)  time: 0.4344  data: 0.0005  max mem: 2373\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4410 (0.5057)  Acc@1: 93.7500 (89.8284)  Acc@5: 100.0000 (99.5098)  time: 0.4347  data: 0.0024  max mem: 2373\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4410 (0.4955)  Acc@1: 93.7500 (90.1639)  Acc@5: 100.0000 (99.3852)  time: 0.4351  data: 0.0023  max mem: 2373\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4410 (0.4961)  Acc@1: 93.7500 (90.4000)  Acc@5: 100.0000 (99.4000)  time: 0.4244  data: 0.0012  max mem: 2373\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4349 s / it)\n",
            "* Acc@1 90.400 Acc@5 99.400 loss 0.496\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:37  Loss: 0.7167 (0.7167)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5969  data: 0.1986  max mem: 2373\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.6513 (0.6737)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (98.8636)  time: 0.4503  data: 0.0198  max mem: 2373\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.6781 (0.7447)  Acc@1: 87.5000 (87.7976)  Acc@5: 100.0000 (98.5119)  time: 0.4355  data: 0.0020  max mem: 2373\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6909 (0.7304)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (97.7823)  time: 0.4348  data: 0.0012  max mem: 2373\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6816 (0.7167)  Acc@1: 87.5000 (88.8720)  Acc@5: 100.0000 (98.1707)  time: 0.4343  data: 0.0010  max mem: 2373\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5950 (0.7098)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (98.4069)  time: 0.4347  data: 0.0017  max mem: 2373\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5703 (0.6861)  Acc@1: 87.5000 (88.8320)  Acc@5: 100.0000 (98.5656)  time: 0.4351  data: 0.0012  max mem: 2373\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5563 (0.6762)  Acc@1: 87.5000 (88.9000)  Acc@5: 100.0000 (98.6000)  time: 0.4243  data: 0.0006  max mem: 2373\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4350 s / it)\n",
            "* Acc@1 88.900 Acc@5 98.600 loss 0.676\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:37  Loss: 0.3755 (0.3755)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5922  data: 0.1955  max mem: 2373\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5362 (0.5234)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4477  data: 0.0187  max mem: 2373\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:18  Loss: 0.5362 (0.5304)  Acc@1: 87.5000 (88.9881)  Acc@5: 100.0000 (98.2143)  time: 0.4337  data: 0.0008  max mem: 2373\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4622 (0.5213)  Acc@1: 87.5000 (89.3145)  Acc@5: 100.0000 (98.5887)  time: 0.4337  data: 0.0006  max mem: 2373\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4136 (0.5136)  Acc@1: 87.5000 (89.4817)  Acc@5: 100.0000 (98.9329)  time: 0.4336  data: 0.0017  max mem: 2373\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4773 (0.5184)  Acc@1: 93.7500 (89.7059)  Acc@5: 100.0000 (98.6520)  time: 0.4338  data: 0.0023  max mem: 2373\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5373 (0.5309)  Acc@1: 87.5000 (89.3443)  Acc@5: 100.0000 (98.7705)  time: 0.4344  data: 0.0011  max mem: 2373\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5929 (0.5299)  Acc@1: 87.5000 (89.2000)  Acc@5: 100.0000 (98.7000)  time: 0.4238  data: 0.0004  max mem: 2373\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4338 s / it)\n",
            "* Acc@1 89.200 Acc@5 98.700 loss 0.530\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:38  Loss: 0.7318 (0.7318)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6147  data: 0.2138  max mem: 2373\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5270 (0.5498)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4518  data: 0.0228  max mem: 2373\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:19  Loss: 0.5078 (0.5582)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.2143)  time: 0.4356  data: 0.0021  max mem: 2373\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4446 (0.5321)  Acc@1: 87.5000 (88.7097)  Acc@5: 100.0000 (98.3871)  time: 0.4351  data: 0.0005  max mem: 2373\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3201 (0.4888)  Acc@1: 93.7500 (89.7866)  Acc@5: 100.0000 (98.6280)  time: 0.4346  data: 0.0024  max mem: 2373\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.3877 (0.5000)  Acc@1: 93.7500 (89.9510)  Acc@5: 100.0000 (98.6520)  time: 0.4347  data: 0.0024  max mem: 2373\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.4762 (0.5150)  Acc@1: 87.5000 (89.2418)  Acc@5: 100.0000 (98.4631)  time: 0.4348  data: 0.0004  max mem: 2373\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.4607 (0.5098)  Acc@1: 87.5000 (89.4000)  Acc@5: 100.0000 (98.5000)  time: 0.4239  data: 0.0004  max mem: 2373\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4352 s / it)\n",
            "* Acc@1 89.400 Acc@5 98.500 loss 0.510\n",
            "[Average accuracy till task4]\tAcc@1: 89.4750\tAcc@5: 98.8000\tLoss: 0.5530\tForgetting: 4.8667\tBackward: -4.8667\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:05:07  Lr: 0.001875  Loss: 2.0795  Acc@1: 6.2500 (6.2500)  Acc@5: 43.7500 (43.7500)  time: 0.9825  data: 0.3591  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:31  Lr: 0.001875  Loss: 1.7121  Acc@1: 56.2500 (51.7045)  Acc@5: 81.2500 (82.3864)  time: 0.6967  data: 0.0340  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:20  Lr: 0.001875  Loss: 1.5720  Acc@1: 62.5000 (61.3095)  Acc@5: 93.7500 (87.7976)  time: 0.6682  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:12  Lr: 0.001875  Loss: 1.2678  Acc@1: 75.0000 (67.3387)  Acc@5: 93.7500 (90.3226)  time: 0.6687  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: 0.9115  Acc@1: 81.2500 (71.3415)  Acc@5: 100.0000 (92.2256)  time: 0.6692  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.7893  Acc@1: 81.2500 (73.1618)  Acc@5: 100.0000 (93.2598)  time: 0.6705  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.5260  Acc@1: 81.2500 (75.1025)  Acc@5: 100.0000 (93.9549)  time: 0.6715  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.7245  Acc@1: 81.2500 (75.8803)  Acc@5: 93.7500 (94.1901)  time: 0.6713  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:37  Lr: 0.001875  Loss: 0.6408  Acc@1: 81.2500 (76.4660)  Acc@5: 93.7500 (94.5988)  time: 0.6717  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: 0.6229  Acc@1: 81.2500 (77.4725)  Acc@5: 100.0000 (94.9863)  time: 0.6719  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.5517  Acc@1: 87.5000 (78.4653)  Acc@5: 100.0000 (95.2351)  time: 0.6716  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.2369  Acc@1: 87.5000 (78.9977)  Acc@5: 100.0000 (95.4955)  time: 0.6728  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:10  Lr: 0.001875  Loss: 0.4829  Acc@1: 87.5000 (79.6488)  Acc@5: 100.0000 (95.7645)  time: 0.6743  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:03  Lr: 0.001875  Loss: 0.7472  Acc@1: 87.5000 (79.9618)  Acc@5: 100.0000 (95.8492)  time: 0.6734  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.2763  Acc@1: 87.5000 (80.5408)  Acc@5: 100.0000 (95.9220)  time: 0.6719  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.5780  Acc@1: 87.5000 (80.9603)  Acc@5: 100.0000 (96.1093)  time: 0.6723  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:43  Lr: 0.001875  Loss: 0.3955  Acc@1: 87.5000 (81.2500)  Acc@5: 100.0000 (96.3509)  time: 0.6727  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.2109  Acc@1: 87.5000 (81.6520)  Acc@5: 100.0000 (96.5643)  time: 0.6711  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.4302  Acc@1: 87.5000 (81.8025)  Acc@5: 100.0000 (96.7196)  time: 0.6699  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.3461  Acc@1: 87.5000 (82.1990)  Acc@5: 100.0000 (96.8586)  time: 0.6703  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:16  Lr: 0.001875  Loss: 0.0092  Acc@1: 87.5000 (82.0274)  Acc@5: 100.0000 (96.9216)  time: 0.6702  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.2147  Acc@1: 81.2500 (82.1979)  Acc@5: 100.0000 (96.9491)  time: 0.6708  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.5959  Acc@1: 87.5000 (82.3529)  Acc@5: 100.0000 (97.0588)  time: 0.6711  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.4634  Acc@1: 87.5000 (82.4675)  Acc@5: 100.0000 (97.0779)  time: 0.6704  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:49  Lr: 0.001875  Loss: 0.1906  Acc@1: 87.5000 (82.6763)  Acc@5: 100.0000 (97.1992)  time: 0.6707  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2489  Acc@1: 87.5000 (82.8685)  Acc@5: 100.0000 (97.2610)  time: 0.6709  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.1346  Acc@1: 87.5000 (82.9502)  Acc@5: 100.0000 (97.2701)  time: 0.6702  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.3099  Acc@1: 81.2500 (82.9336)  Acc@5: 100.0000 (97.3017)  time: 0.6696  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1093  Acc@1: 81.2500 (82.9626)  Acc@5: 100.0000 (97.3532)  time: 0.6694  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.3262  Acc@1: 87.5000 (83.1186)  Acc@5: 100.0000 (97.4442)  time: 0.6691  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.2403  Acc@1: 87.5000 (83.2226)  Acc@5: 100.0000 (97.5083)  time: 0.6693  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.4370  Acc@1: 87.5000 (83.2596)  Acc@5: 100.0000 (97.5482)  time: 0.6699  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.7305  Acc@1: 87.5000 (83.2400)  Acc@5: 100.0000 (97.5400)  time: 0.6535  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:30 (0.6710 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.7305  Acc@1: 87.5000 (83.2400)  Acc@5: 100.0000 (97.5400)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:05:14  Lr: 0.001875  Loss: 0.1966  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 1.0063  data: 0.3766  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:32  Lr: 0.001875  Loss: 0.3166  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (97.7273)  time: 0.7009  data: 0.0349  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:20  Lr: 0.001875  Loss: 0.2762  Acc@1: 87.5000 (86.3095)  Acc@5: 100.0000 (98.8095)  time: 0.6692  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:12  Lr: 0.001875  Loss: 0.0900  Acc@1: 87.5000 (87.2984)  Acc@5: 100.0000 (98.7903)  time: 0.6692  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:05  Lr: 0.001875  Loss: 0.1267  Acc@1: 87.5000 (87.1951)  Acc@5: 100.0000 (99.0854)  time: 0.6699  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.2103  Acc@1: 87.5000 (87.6225)  Acc@5: 100.0000 (99.1422)  time: 0.6703  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.2665  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (99.2828)  time: 0.6710  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.0834  Acc@1: 87.5000 (87.4120)  Acc@5: 100.0000 (99.1197)  time: 0.6704  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:37  Lr: 0.001875  Loss: 0.1502  Acc@1: 87.5000 (87.6543)  Acc@5: 100.0000 (99.0741)  time: 0.6699  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: -0.1042  Acc@1: 87.5000 (87.0192)  Acc@5: 100.0000 (98.9011)  time: 0.6700  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: -0.1001  Acc@1: 87.5000 (87.3144)  Acc@5: 100.0000 (98.7005)  time: 0.6692  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.0365  Acc@1: 87.5000 (87.3874)  Acc@5: 100.0000 (98.8176)  time: 0.6693  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1147  Acc@1: 87.5000 (86.9835)  Acc@5: 100.0000 (98.7603)  time: 0.6690  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:03  Lr: 0.001875  Loss: 0.3339  Acc@1: 87.5000 (86.9275)  Acc@5: 100.0000 (98.5687)  time: 0.6677  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.1890  Acc@1: 87.5000 (86.9238)  Acc@5: 100.0000 (98.6259)  time: 0.6679  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.1228  Acc@1: 81.2500 (86.7136)  Acc@5: 100.0000 (98.5099)  time: 0.6683  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3544  Acc@1: 87.5000 (86.8012)  Acc@5: 100.0000 (98.5637)  time: 0.6691  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.3983  Acc@1: 93.7500 (86.8056)  Acc@5: 100.0000 (98.6111)  time: 0.6696  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.1876  Acc@1: 93.7500 (87.0856)  Acc@5: 100.0000 (98.6188)  time: 0.6699  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0610  Acc@1: 93.7500 (87.1728)  Acc@5: 100.0000 (98.5929)  time: 0.6697  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.3462  Acc@1: 87.5000 (87.0647)  Acc@5: 100.0000 (98.6318)  time: 0.6692  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.0613  Acc@1: 87.5000 (87.0261)  Acc@5: 100.0000 (98.6967)  time: 0.6692  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.1661  Acc@1: 87.5000 (87.1324)  Acc@5: 100.0000 (98.7557)  time: 0.6693  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.6474  Acc@1: 87.5000 (87.0400)  Acc@5: 100.0000 (98.7013)  time: 0.6695  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0770  Acc@1: 87.5000 (87.1369)  Acc@5: 100.0000 (98.6774)  time: 0.6695  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0951  Acc@1: 87.5000 (87.1265)  Acc@5: 100.0000 (98.6554)  time: 0.6685  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.1290  Acc@1: 87.5000 (87.2126)  Acc@5: 100.0000 (98.6590)  time: 0.6689  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0865  Acc@1: 87.5000 (87.2232)  Acc@5: 100.0000 (98.6624)  time: 0.6700  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.0481  Acc@1: 87.5000 (87.4110)  Acc@5: 100.0000 (98.7100)  time: 0.6693  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2369  Acc@1: 93.7500 (87.4785)  Acc@5: 100.0000 (98.6899)  time: 0.6687  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1104  Acc@1: 93.7500 (87.6869)  Acc@5: 100.0000 (98.7334)  time: 0.6693  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.0519  Acc@1: 93.7500 (87.5201)  Acc@5: 100.0000 (98.7741)  time: 0.6696  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.2851  Acc@1: 87.5000 (87.5400)  Acc@5: 100.0000 (98.7800)  time: 0.6528  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6697 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.2851  Acc@1: 87.5000 (87.5400)  Acc@5: 100.0000 (98.7800)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:53  Lr: 0.001875  Loss: 0.2644  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.9374  data: 0.3119  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:30  Lr: 0.001875  Loss: -0.0463  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (99.4318)  time: 0.6937  data: 0.0295  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:19  Lr: 0.001875  Loss: -0.0866  Acc@1: 87.5000 (88.3929)  Acc@5: 100.0000 (98.8095)  time: 0.6685  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.2308  Acc@1: 87.5000 (88.1048)  Acc@5: 100.0000 (98.5887)  time: 0.6679  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: 0.1709  Acc@1: 87.5000 (88.1098)  Acc@5: 100.0000 (98.3232)  time: 0.6682  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: -0.0302  Acc@1: 87.5000 (87.8676)  Acc@5: 100.0000 (98.5294)  time: 0.6682  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.2852  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (98.2582)  time: 0.6686  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.1281  Acc@1: 87.5000 (88.1162)  Acc@5: 100.0000 (98.3275)  time: 0.6688  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.4508  Acc@1: 87.5000 (88.1173)  Acc@5: 100.0000 (98.3796)  time: 0.6691  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1179  Acc@1: 87.5000 (88.3929)  Acc@5: 100.0000 (98.4890)  time: 0.6697  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: -0.0697  Acc@1: 93.7500 (88.6757)  Acc@5: 100.0000 (98.6386)  time: 0.6698  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.4741  Acc@1: 87.5000 (87.9505)  Acc@5: 100.0000 (98.5360)  time: 0.6692  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1698  Acc@1: 87.5000 (87.9649)  Acc@5: 100.0000 (98.6054)  time: 0.6684  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.0606  Acc@1: 87.5000 (88.0725)  Acc@5: 100.0000 (98.7118)  time: 0.6689  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.0875  Acc@1: 87.5000 (87.9433)  Acc@5: 100.0000 (98.7145)  time: 0.6694  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.4218  Acc@1: 87.5000 (87.7897)  Acc@5: 100.0000 (98.5513)  time: 0.6692  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0992  Acc@1: 87.5000 (88.0435)  Acc@5: 100.0000 (98.6025)  time: 0.6691  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.6875  Acc@1: 87.5000 (88.0117)  Acc@5: 100.0000 (98.5746)  time: 0.6693  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.2427  Acc@1: 87.5000 (87.8108)  Acc@5: 100.0000 (98.6533)  time: 0.6695  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0173  Acc@1: 81.2500 (87.6309)  Acc@5: 100.0000 (98.6584)  time: 0.6697  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2566  Acc@1: 87.5000 (87.6555)  Acc@5: 100.0000 (98.6629)  time: 0.6700  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: -0.1395  Acc@1: 87.5000 (87.7370)  Acc@5: 100.0000 (98.6967)  time: 0.6695  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: -0.0671  Acc@1: 87.5000 (87.8676)  Acc@5: 100.0000 (98.7274)  time: 0.6705  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.0455  Acc@1: 87.5000 (87.9329)  Acc@5: 100.0000 (98.7554)  time: 0.6713  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.5474  Acc@1: 87.5000 (87.8890)  Acc@5: 100.0000 (98.7552)  time: 0.6708  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2230  Acc@1: 87.5000 (87.8984)  Acc@5: 100.0000 (98.8048)  time: 0.6712  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0242  Acc@1: 87.5000 (87.9071)  Acc@5: 100.0000 (98.8506)  time: 0.6712  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1541  Acc@1: 87.5000 (87.8459)  Acc@5: 100.0000 (98.8699)  time: 0.6700  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0863  Acc@1: 87.5000 (87.8114)  Acc@5: 100.0000 (98.7989)  time: 0.6689  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.0867  Acc@1: 87.5000 (87.8007)  Acc@5: 100.0000 (98.7973)  time: 0.6694  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0215  Acc@1: 87.5000 (87.9776)  Acc@5: 100.0000 (98.8164)  time: 0.6697  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2492  Acc@1: 87.5000 (87.9622)  Acc@5: 100.0000 (98.7942)  time: 0.6690  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1578  Acc@1: 87.5000 (87.9200)  Acc@5: 100.0000 (98.7800)  time: 0.6526  data: 0.0004  max mem: 2373\n",
            "Train: Epoch[3/5] Total time: 0:03:29 (0.6695 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1578  Acc@1: 87.5000 (87.9200)  Acc@5: 100.0000 (98.7800)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:04:54  Lr: 0.001875  Loss: 0.1019  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.9397  data: 0.3110  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:31  Lr: 0.001875  Loss: 0.0685  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (99.4318)  time: 0.6966  data: 0.0285  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:20  Lr: 0.001875  Loss: 0.0972  Acc@1: 87.5000 (88.6905)  Acc@5: 100.0000 (99.4048)  time: 0.6708  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:12  Lr: 0.001875  Loss: 0.1900  Acc@1: 87.5000 (89.1129)  Acc@5: 100.0000 (99.1935)  time: 0.6695  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: -0.0171  Acc@1: 87.5000 (87.9573)  Acc@5: 100.0000 (99.0854)  time: 0.6691  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.1541  Acc@1: 87.5000 (87.6225)  Acc@5: 100.0000 (99.1422)  time: 0.6690  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.3264  Acc@1: 87.5000 (88.0123)  Acc@5: 100.0000 (99.2828)  time: 0.6694  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: -0.1357  Acc@1: 93.7500 (88.3803)  Acc@5: 100.0000 (99.2077)  time: 0.6697  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.4526  Acc@1: 93.7500 (88.4259)  Acc@5: 100.0000 (99.1512)  time: 0.6698  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: -0.0730  Acc@1: 87.5000 (88.3242)  Acc@5: 100.0000 (99.1071)  time: 0.6695  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.1197  Acc@1: 87.5000 (88.6757)  Acc@5: 100.0000 (99.1337)  time: 0.6692  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: -0.1538  Acc@1: 87.5000 (88.7950)  Acc@5: 100.0000 (99.0991)  time: 0.6700  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: -0.0160  Acc@1: 87.5000 (88.8430)  Acc@5: 100.0000 (99.0186)  time: 0.6707  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1547  Acc@1: 87.5000 (88.7882)  Acc@5: 100.0000 (98.9027)  time: 0.6701  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.2105  Acc@1: 87.5000 (88.7411)  Acc@5: 100.0000 (98.8918)  time: 0.6700  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.3080  Acc@1: 87.5000 (88.8659)  Acc@5: 100.0000 (98.9652)  time: 0.6699  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1593  Acc@1: 93.7500 (89.0140)  Acc@5: 100.0000 (98.9130)  time: 0.6700  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.0479  Acc@1: 93.7500 (88.8523)  Acc@5: 100.0000 (98.9401)  time: 0.6702  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.1786  Acc@1: 87.5000 (88.5014)  Acc@5: 100.0000 (98.8950)  time: 0.6699  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0270  Acc@1: 87.5000 (88.5798)  Acc@5: 100.0000 (98.8547)  time: 0.6704  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2449  Acc@1: 87.5000 (88.5261)  Acc@5: 100.0000 (98.8184)  time: 0.6715  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: -0.0364  Acc@1: 87.5000 (88.5664)  Acc@5: 100.0000 (98.8152)  time: 0.6716  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2420  Acc@1: 87.5000 (88.6595)  Acc@5: 100.0000 (98.7839)  time: 0.6715  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0919  Acc@1: 93.7500 (88.7987)  Acc@5: 100.0000 (98.8366)  time: 0.6716  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:49  Lr: 0.001875  Loss: 0.0526  Acc@1: 87.5000 (88.8226)  Acc@5: 100.0000 (98.8589)  time: 0.6709  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1395  Acc@1: 93.7500 (88.9442)  Acc@5: 100.0000 (98.8546)  time: 0.6704  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.2169  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (98.8745)  time: 0.6706  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: -0.1477  Acc@1: 93.7500 (89.0683)  Acc@5: 100.0000 (98.8699)  time: 0.6712  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0892  Acc@1: 93.7500 (89.1459)  Acc@5: 100.0000 (98.9101)  time: 0.6713  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0980  Acc@1: 93.7500 (89.3041)  Acc@5: 100.0000 (98.9046)  time: 0.6705  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0715  Acc@1: 93.7500 (89.1819)  Acc@5: 100.0000 (98.8580)  time: 0.6709  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1669  Acc@1: 87.5000 (89.1479)  Acc@5: 100.0000 (98.8947)  time: 0.6715  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.3481  Acc@1: 87.5000 (89.1200)  Acc@5: 100.0000 (98.8800)  time: 0.6541  data: 0.0004  max mem: 2373\n",
            "Train: Epoch[4/5] Total time: 0:03:29 (0.6705 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.3481  Acc@1: 87.5000 (89.1200)  Acc@5: 100.0000 (98.8800)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:04:09  Lr: 0.001875  Loss: -0.0631  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7958  data: 0.1705  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: -0.1917  Acc@1: 100.0000 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6843  data: 0.0159  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: -0.0632  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6719  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.1850  Acc@1: 93.7500 (91.7339)  Acc@5: 100.0000 (99.1935)  time: 0.6705  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: -0.1263  Acc@1: 87.5000 (91.3110)  Acc@5: 100.0000 (99.0854)  time: 0.6701  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.1076  Acc@1: 87.5000 (91.0539)  Acc@5: 100.0000 (99.1422)  time: 0.6701  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.0063  Acc@1: 87.5000 (90.7787)  Acc@5: 100.0000 (99.1803)  time: 0.6709  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: -0.1239  Acc@1: 93.7500 (91.0211)  Acc@5: 100.0000 (99.2077)  time: 0.6708  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0289  Acc@1: 93.7500 (91.0494)  Acc@5: 100.0000 (99.2284)  time: 0.6705  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1010  Acc@1: 87.5000 (90.9341)  Acc@5: 100.0000 (99.2445)  time: 0.6712  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.1398  Acc@1: 87.5000 (90.5941)  Acc@5: 100.0000 (99.1955)  time: 0.6706  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.0466  Acc@1: 93.7500 (90.7658)  Acc@5: 100.0000 (99.0991)  time: 0.6705  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.0256  Acc@1: 93.7500 (90.5992)  Acc@5: 100.0000 (99.1219)  time: 0.6709  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1501  Acc@1: 87.5000 (90.4580)  Acc@5: 100.0000 (99.0935)  time: 0.6710  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.1539  Acc@1: 87.5000 (90.3369)  Acc@5: 100.0000 (99.0691)  time: 0.6708  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0129  Acc@1: 87.5000 (90.3146)  Acc@5: 100.0000 (99.0066)  time: 0.6709  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0990  Acc@1: 87.5000 (90.3339)  Acc@5: 100.0000 (98.9907)  time: 0.6714  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.0290  Acc@1: 87.5000 (90.3143)  Acc@5: 100.0000 (99.0132)  time: 0.6706  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: -0.0720  Acc@1: 93.7500 (90.2624)  Acc@5: 100.0000 (99.0677)  time: 0.6700  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0259  Acc@1: 93.7500 (90.3796)  Acc@5: 100.0000 (99.0838)  time: 0.6700  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.1070  Acc@1: 93.7500 (90.3607)  Acc@5: 100.0000 (99.0672)  time: 0.6693  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.2498  Acc@1: 87.5000 (90.1955)  Acc@5: 100.0000 (99.0818)  time: 0.6689  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.0773  Acc@1: 93.7500 (90.2715)  Acc@5: 100.0000 (99.0102)  time: 0.6692  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.5098  Acc@1: 87.5000 (90.0703)  Acc@5: 100.0000 (99.0260)  time: 0.6694  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.5179  Acc@1: 87.5000 (90.1452)  Acc@5: 100.0000 (99.0145)  time: 0.6691  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2418  Acc@1: 87.5000 (90.0896)  Acc@5: 100.0000 (98.9293)  time: 0.6685  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.5163  Acc@1: 87.5000 (89.9186)  Acc@5: 100.0000 (98.9703)  time: 0.6691  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0387  Acc@1: 87.5000 (89.9446)  Acc@5: 100.0000 (98.9161)  time: 0.6697  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.1601  Acc@1: 93.7500 (89.9689)  Acc@5: 100.0000 (98.9101)  time: 0.6695  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0333  Acc@1: 87.5000 (89.9055)  Acc@5: 100.0000 (98.9046)  time: 0.6691  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: -0.0570  Acc@1: 87.5000 (89.8463)  Acc@5: 100.0000 (98.9203)  time: 0.6694  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1706  Acc@1: 93.7500 (89.8513)  Acc@5: 100.0000 (98.9550)  time: 0.6700  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1507  Acc@1: 93.7500 (89.8600)  Acc@5: 100.0000 (98.9600)  time: 0.6536  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[5/5] Total time: 0:03:29 (0.6698 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1507  Acc@1: 93.7500 (89.8600)  Acc@5: 100.0000 (98.9600)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:37  Loss: 0.6559 (0.6559)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5936  data: 0.1944  max mem: 2373\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5713 (0.5606)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.4318)  time: 0.4497  data: 0.0180  max mem: 2373\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.5713 (0.6167)  Acc@1: 87.5000 (85.7143)  Acc@5: 100.0000 (99.1071)  time: 0.4354  data: 0.0003  max mem: 2373\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5675 (0.5979)  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (99.1935)  time: 0.4346  data: 0.0025  max mem: 2373\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5467 (0.5848)  Acc@1: 87.5000 (86.4329)  Acc@5: 100.0000 (99.2378)  time: 0.4340  data: 0.0025  max mem: 2373\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4638 (0.5604)  Acc@1: 87.5000 (87.2549)  Acc@5: 100.0000 (99.1422)  time: 0.4346  data: 0.0004  max mem: 2373\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4566 (0.5459)  Acc@1: 87.5000 (87.7049)  Acc@5: 100.0000 (99.0779)  time: 0.4349  data: 0.0021  max mem: 2373\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4566 (0.5441)  Acc@1: 93.7500 (87.9000)  Acc@5: 100.0000 (99.1000)  time: 0.4240  data: 0.0020  max mem: 2373\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4347 s / it)\n",
            "* Acc@1 87.900 Acc@5 99.100 loss 0.544\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:40  Loss: 0.7879 (0.7879)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6371  data: 0.2407  max mem: 2373\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:24  Loss: 0.6982 (0.7158)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (97.7273)  time: 0.4537  data: 0.0223  max mem: 2373\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7017 (0.7687)  Acc@1: 87.5000 (87.2024)  Acc@5: 100.0000 (97.0238)  time: 0.4353  data: 0.0012  max mem: 2373\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7485 (0.7506)  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (96.5726)  time: 0.4348  data: 0.0027  max mem: 2373\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6587 (0.7283)  Acc@1: 87.5000 (88.2622)  Acc@5: 100.0000 (96.9512)  time: 0.4346  data: 0.0021  max mem: 2373\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6215 (0.7243)  Acc@1: 87.5000 (87.6225)  Acc@5: 100.0000 (97.1814)  time: 0.4351  data: 0.0011  max mem: 2373\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5902 (0.6996)  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (97.4385)  time: 0.4351  data: 0.0024  max mem: 2373\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5863 (0.6921)  Acc@1: 87.5000 (87.9000)  Acc@5: 100.0000 (97.5000)  time: 0.4242  data: 0.0022  max mem: 2373\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4356 s / it)\n",
            "* Acc@1 87.900 Acc@5 97.500 loss 0.692\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:33  Loss: 0.4664 (0.4664)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5316  data: 0.1339  max mem: 2373\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5764 (0.5878)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (98.8636)  time: 0.4429  data: 0.0131  max mem: 2373\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:18  Loss: 0.5947 (0.5967)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (97.9167)  time: 0.4342  data: 0.0022  max mem: 2373\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.5763 (0.5853)  Acc@1: 87.5000 (84.6774)  Acc@5: 100.0000 (98.3871)  time: 0.4345  data: 0.0028  max mem: 2373\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4424 (0.5776)  Acc@1: 87.5000 (85.2134)  Acc@5: 100.0000 (98.6280)  time: 0.4344  data: 0.0014  max mem: 2373\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.5344 (0.5813)  Acc@1: 87.5000 (85.6618)  Acc@5: 100.0000 (98.4069)  time: 0.4343  data: 0.0018  max mem: 2373\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5605 (0.5866)  Acc@1: 87.5000 (85.5533)  Acc@5: 100.0000 (98.3607)  time: 0.4339  data: 0.0026  max mem: 2373\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5692 (0.5848)  Acc@1: 81.2500 (85.5000)  Acc@5: 100.0000 (98.4000)  time: 0.4229  data: 0.0026  max mem: 2373\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4332 s / it)\n",
            "* Acc@1 85.500 Acc@5 98.400 loss 0.585\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:36  Loss: 0.7242 (0.7242)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5773  data: 0.1767  max mem: 2373\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5598 (0.5694)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4472  data: 0.0164  max mem: 2373\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.5505 (0.5830)  Acc@1: 87.5000 (84.8214)  Acc@5: 100.0000 (97.9167)  time: 0.4349  data: 0.0012  max mem: 2373\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4813 (0.5666)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (97.5806)  time: 0.4347  data: 0.0013  max mem: 2373\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3869 (0.5228)  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (98.0183)  time: 0.4338  data: 0.0006  max mem: 2373\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4259 (0.5375)  Acc@1: 87.5000 (87.7451)  Acc@5: 100.0000 (98.0392)  time: 0.4342  data: 0.0017  max mem: 2373\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5104 (0.5581)  Acc@1: 87.5000 (87.3975)  Acc@5: 100.0000 (97.7459)  time: 0.4341  data: 0.0016  max mem: 2373\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5064 (0.5543)  Acc@1: 87.5000 (87.6000)  Acc@5: 100.0000 (97.8000)  time: 0.4228  data: 0.0016  max mem: 2373\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4339 s / it)\n",
            "* Acc@1 87.600 Acc@5 97.800 loss 0.554\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:38  Loss: 0.2359 (0.2359)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6071  data: 0.2095  max mem: 2373\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:23  Loss: 0.4583 (0.5871)  Acc@1: 93.7500 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4501  data: 0.0195  max mem: 2373\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:19  Loss: 0.4583 (0.5282)  Acc@1: 93.7500 (90.7738)  Acc@5: 100.0000 (98.5119)  time: 0.4346  data: 0.0028  max mem: 2373\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.3980 (0.5103)  Acc@1: 93.7500 (90.1210)  Acc@5: 100.0000 (98.7903)  time: 0.4344  data: 0.0029  max mem: 2373\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.3946 (0.4858)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (98.4756)  time: 0.4342  data: 0.0006  max mem: 2373\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4199 (0.4817)  Acc@1: 93.7500 (91.5441)  Acc@5: 100.0000 (98.6520)  time: 0.4341  data: 0.0026  max mem: 2373\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4222 (0.4839)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (98.6680)  time: 0.4345  data: 0.0025  max mem: 2373\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4345 (0.5008)  Acc@1: 93.7500 (90.8000)  Acc@5: 100.0000 (98.6000)  time: 0.4235  data: 0.0019  max mem: 2373\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4345 s / it)\n",
            "* Acc@1 90.800 Acc@5 98.600 loss 0.501\n",
            "[Average accuracy till task5]\tAcc@1: 87.9400\tAcc@5: 98.2800\tLoss: 0.5752\tForgetting: 5.9000\tBackward: -5.9000\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:04:35  Lr: 0.001875  Loss: 2.1444  Acc@1: 0.0000 (0.0000)  Acc@5: 31.2500 (31.2500)  time: 0.8788  data: 0.2542  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: 1.7115  Acc@1: 56.2500 (47.7273)  Acc@5: 87.5000 (77.8409)  time: 0.6870  data: 0.0239  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 1.3664  Acc@1: 68.7500 (62.2024)  Acc@5: 93.7500 (86.3095)  time: 0.6675  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 1.0073  Acc@1: 81.2500 (67.5403)  Acc@5: 100.0000 (88.9113)  time: 0.6676  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.9846  Acc@1: 81.2500 (71.4939)  Acc@5: 100.0000 (91.1585)  time: 0.6692  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.6738  Acc@1: 81.2500 (74.3873)  Acc@5: 100.0000 (92.7696)  time: 0.6696  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.4599  Acc@1: 87.5000 (76.0246)  Acc@5: 100.0000 (93.7500)  time: 0.6703  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.3997  Acc@1: 87.5000 (77.4648)  Acc@5: 100.0000 (94.5423)  time: 0.6714  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.6697  Acc@1: 87.5000 (78.8580)  Acc@5: 100.0000 (95.0617)  time: 0.6709  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.4629  Acc@1: 87.5000 (79.7390)  Acc@5: 100.0000 (95.6044)  time: 0.6721  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.4515  Acc@1: 87.5000 (80.1980)  Acc@5: 100.0000 (95.9158)  time: 0.6722  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.3498  Acc@1: 87.5000 (80.6869)  Acc@5: 100.0000 (96.2275)  time: 0.6712  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3785  Acc@1: 87.5000 (80.8884)  Acc@5: 100.0000 (96.4360)  time: 0.6702  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1047  Acc@1: 87.5000 (81.3931)  Acc@5: 100.0000 (96.6126)  time: 0.6689  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.1551  Acc@1: 87.5000 (81.7376)  Acc@5: 100.0000 (96.7642)  time: 0.6686  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0173  Acc@1: 87.5000 (82.1192)  Acc@5: 100.0000 (96.9785)  time: 0.6674  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3108  Acc@1: 87.5000 (82.3370)  Acc@5: 100.0000 (97.0885)  time: 0.6665  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.2708  Acc@1: 87.5000 (82.4927)  Acc@5: 100.0000 (97.2588)  time: 0.6662  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.0075  Acc@1: 87.5000 (82.9420)  Acc@5: 100.0000 (97.3757)  time: 0.6651  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.2291  Acc@1: 87.5000 (83.2461)  Acc@5: 100.0000 (97.5131)  time: 0.6645  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2475  Acc@1: 87.5000 (83.3333)  Acc@5: 100.0000 (97.6368)  time: 0.6651  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2621  Acc@1: 81.2500 (83.2642)  Acc@5: 100.0000 (97.6303)  time: 0.6645  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.0716  Acc@1: 81.2500 (83.1165)  Acc@5: 100.0000 (97.6810)  time: 0.6642  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0932  Acc@1: 81.2500 (83.1710)  Acc@5: 100.0000 (97.7273)  time: 0.6645  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2315  Acc@1: 87.5000 (83.5322)  Acc@5: 100.0000 (97.7438)  time: 0.6637  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.0884  Acc@1: 87.5000 (83.6902)  Acc@5: 100.0000 (97.7341)  time: 0.6635  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2366  Acc@1: 87.5000 (83.9080)  Acc@5: 100.0000 (97.7730)  time: 0.6639  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0440  Acc@1: 87.5000 (84.0406)  Acc@5: 100.0000 (97.8321)  time: 0.6649  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2820  Acc@1: 87.5000 (84.1415)  Acc@5: 100.0000 (97.8425)  time: 0.6650  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.3658  Acc@1: 87.5000 (84.1495)  Acc@5: 100.0000 (97.8952)  time: 0.6654  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1330  Acc@1: 81.2500 (84.1777)  Acc@5: 100.0000 (97.9236)  time: 0.6663  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2189  Acc@1: 81.2500 (84.1841)  Acc@5: 100.0000 (97.8899)  time: 0.6657  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.3663  Acc@1: 87.5000 (84.2200)  Acc@5: 100.0000 (97.9000)  time: 0.6494  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:28 (0.6670 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.3663  Acc@1: 87.5000 (84.2200)  Acc@5: 100.0000 (97.9000)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:16  Lr: 0.001875  Loss: 0.3452  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.8179  data: 0.1960  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.0052  Acc@1: 93.7500 (92.6136)  Acc@5: 100.0000 (100.0000)  time: 0.6799  data: 0.0210  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 0.0107  Acc@1: 93.7500 (91.3690)  Acc@5: 100.0000 (99.7024)  time: 0.6656  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 0.0919  Acc@1: 87.5000 (89.7177)  Acc@5: 100.0000 (99.5968)  time: 0.6656  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: 0.0340  Acc@1: 87.5000 (88.4146)  Acc@5: 100.0000 (99.3902)  time: 0.6666  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.1198  Acc@1: 87.5000 (87.3775)  Acc@5: 100.0000 (99.1422)  time: 0.6682  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.3466  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.0779)  time: 0.6685  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.5948  Acc@1: 87.5000 (87.0599)  Acc@5: 100.0000 (98.8556)  time: 0.6670  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: -0.1273  Acc@1: 87.5000 (87.8086)  Acc@5: 100.0000 (98.9969)  time: 0.6670  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.0469  Acc@1: 93.7500 (87.9121)  Acc@5: 100.0000 (99.0385)  time: 0.6682  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.1809  Acc@1: 87.5000 (87.7475)  Acc@5: 100.0000 (99.1337)  time: 0.6673  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0930  Acc@1: 87.5000 (87.8378)  Acc@5: 100.0000 (99.2117)  time: 0.6669  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: 0.1160  Acc@1: 87.5000 (87.7066)  Acc@5: 100.0000 (99.1736)  time: 0.6673  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.2041  Acc@1: 87.5000 (87.7385)  Acc@5: 100.0000 (99.1889)  time: 0.6674  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.3720  Acc@1: 87.5000 (87.7216)  Acc@5: 100.0000 (99.2465)  time: 0.6680  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.2588  Acc@1: 87.5000 (87.6242)  Acc@5: 100.0000 (99.2550)  time: 0.6669  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.6068  Acc@1: 87.5000 (87.4612)  Acc@5: 100.0000 (99.2624)  time: 0.6667  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.1386  Acc@1: 87.5000 (87.4635)  Acc@5: 100.0000 (99.1959)  time: 0.6676  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.0225  Acc@1: 93.7500 (87.6727)  Acc@5: 100.0000 (99.1713)  time: 0.6678  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.0652  Acc@1: 93.7500 (87.7618)  Acc@5: 100.0000 (99.1819)  time: 0.6684  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0539  Acc@1: 87.5000 (87.9042)  Acc@5: 100.0000 (99.1294)  time: 0.6684  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.0725  Acc@1: 87.5000 (87.7370)  Acc@5: 100.0000 (99.1114)  time: 0.6680  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.0448  Acc@1: 87.5000 (87.6131)  Acc@5: 100.0000 (99.1233)  time: 0.6683  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2713  Acc@1: 87.5000 (87.6623)  Acc@5: 100.0000 (99.0801)  time: 0.6686  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.0761  Acc@1: 87.5000 (87.7334)  Acc@5: 100.0000 (99.0664)  time: 0.6687  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.1948  Acc@1: 87.5000 (87.7490)  Acc@5: 100.0000 (99.1036)  time: 0.6693  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.1220  Acc@1: 87.5000 (87.8592)  Acc@5: 100.0000 (99.1140)  time: 0.6692  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.0153  Acc@1: 87.5000 (87.8921)  Acc@5: 100.0000 (99.1006)  time: 0.6692  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1387  Acc@1: 93.7500 (88.0116)  Acc@5: 100.0000 (99.1103)  time: 0.6701  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1393  Acc@1: 87.5000 (88.0155)  Acc@5: 100.0000 (99.1194)  time: 0.6716  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1967  Acc@1: 87.5000 (88.1437)  Acc@5: 100.0000 (99.1487)  time: 0.6724  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.1775  Acc@1: 87.5000 (88.1431)  Acc@5: 100.0000 (99.1158)  time: 0.6725  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.2286  Acc@1: 87.5000 (88.1000)  Acc@5: 100.0000 (99.1200)  time: 0.6552  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6679 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.2286  Acc@1: 87.5000 (88.1000)  Acc@5: 100.0000 (99.1200)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:34  Lr: 0.001875  Loss: -0.1165  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8771  data: 0.2499  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: -0.0556  Acc@1: 87.5000 (88.0682)  Acc@5: 100.0000 (98.2955)  time: 0.6886  data: 0.0252  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.0085  Acc@1: 87.5000 (86.9048)  Acc@5: 100.0000 (98.5119)  time: 0.6690  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: -0.0356  Acc@1: 87.5000 (86.6935)  Acc@5: 100.0000 (98.9919)  time: 0.6688  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: 0.0605  Acc@1: 87.5000 (87.0427)  Acc@5: 100.0000 (99.0854)  time: 0.6691  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.3055  Acc@1: 87.5000 (87.2549)  Acc@5: 100.0000 (99.2647)  time: 0.6679  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: -0.0060  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.9754)  time: 0.6669  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.1513  Acc@1: 87.5000 (87.8521)  Acc@5: 100.0000 (99.1197)  time: 0.6665  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0535  Acc@1: 93.7500 (88.5031)  Acc@5: 100.0000 (99.1512)  time: 0.6665  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.0982  Acc@1: 93.7500 (88.7363)  Acc@5: 100.0000 (99.1758)  time: 0.6661  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.3710  Acc@1: 93.7500 (88.7995)  Acc@5: 100.0000 (99.2574)  time: 0.6659  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.3056  Acc@1: 93.7500 (88.7950)  Acc@5: 100.0000 (99.2680)  time: 0.6667  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: -0.0517  Acc@1: 87.5000 (88.8430)  Acc@5: 100.0000 (99.2769)  time: 0.6660  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0229  Acc@1: 87.5000 (88.9790)  Acc@5: 100.0000 (99.3321)  time: 0.6647  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.3890  Acc@1: 87.5000 (88.7411)  Acc@5: 100.0000 (99.3351)  time: 0.6645  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.1623  Acc@1: 87.5000 (88.9487)  Acc@5: 100.0000 (99.3377)  time: 0.6639  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.0653  Acc@1: 93.7500 (89.0528)  Acc@5: 100.0000 (99.3789)  time: 0.6627  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.0548  Acc@1: 93.7500 (88.9254)  Acc@5: 100.0000 (99.3787)  time: 0.6632  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.2713  Acc@1: 87.5000 (88.7086)  Acc@5: 100.0000 (99.3785)  time: 0.6643  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0072  Acc@1: 87.5000 (88.7107)  Acc@5: 100.0000 (99.3783)  time: 0.6641  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0284  Acc@1: 87.5000 (88.4639)  Acc@5: 100.0000 (99.3470)  time: 0.6630  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: -0.0709  Acc@1: 87.5000 (88.4479)  Acc@5: 100.0000 (99.2891)  time: 0.6620  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:01  Lr: 0.001875  Loss: 0.2211  Acc@1: 87.5000 (88.2070)  Acc@5: 100.0000 (99.2081)  time: 0.6623  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2245  Acc@1: 87.5000 (88.2305)  Acc@5: 100.0000 (99.2424)  time: 0.6620  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.0713  Acc@1: 87.5000 (88.3299)  Acc@5: 100.0000 (99.2479)  time: 0.6616  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:41  Lr: 0.001875  Loss: 0.1483  Acc@1: 93.7500 (88.4711)  Acc@5: 100.0000 (99.2779)  time: 0.6626  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2694  Acc@1: 87.5000 (88.3381)  Acc@5: 100.0000 (99.2577)  time: 0.6640  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2176  Acc@1: 87.5000 (88.1688)  Acc@5: 100.0000 (99.2159)  time: 0.6651  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:21  Lr: 0.001875  Loss: 0.3551  Acc@1: 87.5000 (88.2340)  Acc@5: 100.0000 (99.2438)  time: 0.6649  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.3067  Acc@1: 87.5000 (88.1658)  Acc@5: 100.0000 (99.2698)  time: 0.6647  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0152  Acc@1: 87.5000 (88.1645)  Acc@5: 100.0000 (99.2525)  time: 0.6656  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:01  Lr: 0.001875  Loss: -0.0153  Acc@1: 87.5000 (88.2235)  Acc@5: 100.0000 (99.2765)  time: 0.6668  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1826  Acc@1: 87.5000 (88.2800)  Acc@5: 100.0000 (99.2800)  time: 0.6504  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5] Total time: 0:03:28 (0.6650 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1826  Acc@1: 87.5000 (88.2800)  Acc@5: 100.0000 (99.2800)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:04:01  Lr: 0.001875  Loss: 0.2361  Acc@1: 87.5000 (87.5000)  Acc@5: 93.7500 (93.7500)  time: 0.7716  data: 0.1445  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: -0.0108  Acc@1: 93.7500 (92.6136)  Acc@5: 100.0000 (98.8636)  time: 0.6773  data: 0.0154  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:16  Lr: 0.001875  Loss: 0.0371  Acc@1: 93.7500 (90.4762)  Acc@5: 100.0000 (99.4048)  time: 0.6673  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: -0.0031  Acc@1: 87.5000 (90.5242)  Acc@5: 100.0000 (99.3952)  time: 0.6669  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: 0.1065  Acc@1: 87.5000 (90.5488)  Acc@5: 100.0000 (99.3902)  time: 0.6681  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.1167  Acc@1: 87.5000 (89.8284)  Acc@5: 100.0000 (99.2647)  time: 0.6687  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: -0.0382  Acc@1: 87.5000 (89.6516)  Acc@5: 100.0000 (99.1803)  time: 0.6681  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.0501  Acc@1: 81.2500 (88.7324)  Acc@5: 100.0000 (99.1197)  time: 0.6689  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.0119  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (99.1512)  time: 0.6689  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1374  Acc@1: 87.5000 (89.0110)  Acc@5: 100.0000 (99.1071)  time: 0.6683  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.1773  Acc@1: 87.5000 (88.8614)  Acc@5: 100.0000 (99.1337)  time: 0.6690  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.0324  Acc@1: 87.5000 (88.5135)  Acc@5: 100.0000 (99.1554)  time: 0.6701  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: -0.0095  Acc@1: 87.5000 (88.5331)  Acc@5: 100.0000 (99.0702)  time: 0.6709  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0150  Acc@1: 87.5000 (88.4542)  Acc@5: 100.0000 (99.0458)  time: 0.6711  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: -0.0155  Acc@1: 93.7500 (88.6082)  Acc@5: 100.0000 (99.1135)  time: 0.6713  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: -0.0822  Acc@1: 93.7500 (88.8659)  Acc@5: 100.0000 (99.1308)  time: 0.6715  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.1907  Acc@1: 93.7500 (89.0528)  Acc@5: 100.0000 (99.1071)  time: 0.6715  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3362  Acc@1: 93.7500 (88.9620)  Acc@5: 100.0000 (99.1228)  time: 0.6723  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.3476  Acc@1: 87.5000 (88.8122)  Acc@5: 100.0000 (99.1022)  time: 0.6726  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0664  Acc@1: 87.5000 (88.8743)  Acc@5: 100.0000 (99.1165)  time: 0.6709  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.0781  Acc@1: 87.5000 (88.8060)  Acc@5: 100.0000 (99.1294)  time: 0.6698  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: -0.0052  Acc@1: 87.5000 (88.8626)  Acc@5: 100.0000 (99.0818)  time: 0.6706  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.1668  Acc@1: 87.5000 (88.9140)  Acc@5: 100.0000 (99.0950)  time: 0.6710  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2234  Acc@1: 93.7500 (89.0422)  Acc@5: 100.0000 (99.1342)  time: 0.6710  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.1548  Acc@1: 87.5000 (88.9782)  Acc@5: 100.0000 (99.1183)  time: 0.6709  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1510  Acc@1: 87.5000 (89.0189)  Acc@5: 100.0000 (99.1285)  time: 0.6703  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0786  Acc@1: 93.7500 (89.1762)  Acc@5: 100.0000 (99.1619)  time: 0.6711  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.3294  Acc@1: 93.7500 (89.2066)  Acc@5: 100.0000 (99.1467)  time: 0.6718  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0351  Acc@1: 87.5000 (89.1681)  Acc@5: 100.0000 (99.1548)  time: 0.6712  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2578  Acc@1: 87.5000 (89.0464)  Acc@5: 100.0000 (99.1838)  time: 0.6707  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.0707  Acc@1: 87.5000 (88.9535)  Acc@5: 100.0000 (99.1487)  time: 0.6714  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.0497  Acc@1: 87.5000 (88.9670)  Acc@5: 100.0000 (99.1559)  time: 0.6715  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1850  Acc@1: 87.5000 (89.0200)  Acc@5: 100.0000 (99.1600)  time: 0.6551  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5] Total time: 0:03:29 (0.6697 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1850  Acc@1: 87.5000 (89.0200)  Acc@5: 100.0000 (99.1600)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:04:04  Lr: 0.001875  Loss: 0.5114  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.7800  data: 0.1525  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: -0.0303  Acc@1: 93.7500 (88.6364)  Acc@5: 100.0000 (99.4318)  time: 0.6820  data: 0.0153  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.2132  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (99.4048)  time: 0.6717  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: 0.0628  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (99.3952)  time: 0.6710  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0531  Acc@1: 87.5000 (89.3293)  Acc@5: 100.0000 (99.5427)  time: 0.6703  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0924  Acc@1: 87.5000 (89.4608)  Acc@5: 100.0000 (99.6324)  time: 0.6691  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.0162  Acc@1: 93.7500 (89.3443)  Acc@5: 100.0000 (99.3852)  time: 0.6686  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.2006  Acc@1: 87.5000 (88.7324)  Acc@5: 100.0000 (99.2958)  time: 0.6680  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.1387  Acc@1: 87.5000 (88.8117)  Acc@5: 100.0000 (99.3056)  time: 0.6676  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.0308  Acc@1: 93.7500 (89.0110)  Acc@5: 100.0000 (99.3819)  time: 0.6687  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.0521  Acc@1: 87.5000 (88.7376)  Acc@5: 100.0000 (99.4431)  time: 0.6681  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.7002  Acc@1: 93.7500 (88.6261)  Acc@5: 100.0000 (99.3243)  time: 0.6671  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: -0.1303  Acc@1: 93.7500 (88.6364)  Acc@5: 100.0000 (99.3285)  time: 0.6672  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.0531  Acc@1: 87.5000 (88.7405)  Acc@5: 100.0000 (99.3798)  time: 0.6668  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0805  Acc@1: 87.5000 (88.6525)  Acc@5: 100.0000 (99.2908)  time: 0.6664  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0494  Acc@1: 87.5000 (88.7003)  Acc@5: 100.0000 (99.3377)  time: 0.6662  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.3013  Acc@1: 87.5000 (88.8199)  Acc@5: 100.0000 (99.3401)  time: 0.6660  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.1790  Acc@1: 87.5000 (88.8523)  Acc@5: 100.0000 (99.3787)  time: 0.6658  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.1127  Acc@1: 87.5000 (88.7431)  Acc@5: 100.0000 (99.3785)  time: 0.6663  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.4791  Acc@1: 87.5000 (88.7435)  Acc@5: 100.0000 (99.3783)  time: 0.6659  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.1004  Acc@1: 93.7500 (88.7438)  Acc@5: 100.0000 (99.4092)  time: 0.6654  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: -0.0737  Acc@1: 93.7500 (88.9514)  Acc@5: 100.0000 (99.4372)  time: 0.6650  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.1408  Acc@1: 93.7500 (89.0554)  Acc@5: 100.0000 (99.4627)  time: 0.6644  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2155  Acc@1: 93.7500 (88.9881)  Acc@5: 100.0000 (99.4048)  time: 0.6647  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.1517  Acc@1: 87.5000 (88.8745)  Acc@5: 100.0000 (99.3257)  time: 0.6648  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: -0.1036  Acc@1: 87.5000 (88.9442)  Acc@5: 100.0000 (99.3277)  time: 0.6649  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0391  Acc@1: 87.5000 (88.9128)  Acc@5: 100.0000 (99.3534)  time: 0.6642  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2727  Acc@1: 87.5000 (88.9991)  Acc@5: 100.0000 (99.3542)  time: 0.6642  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0079  Acc@1: 93.7500 (89.0792)  Acc@5: 100.0000 (99.3772)  time: 0.6643  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1905  Acc@1: 93.7500 (89.0679)  Acc@5: 100.0000 (99.3986)  time: 0.6632  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.3121  Acc@1: 87.5000 (88.9950)  Acc@5: 100.0000 (99.4186)  time: 0.6632  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2499  Acc@1: 87.5000 (89.0273)  Acc@5: 100.0000 (99.4172)  time: 0.6630  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.3591  Acc@1: 93.7500 (89.0200)  Acc@5: 100.0000 (99.4200)  time: 0.6463  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[5/5] Total time: 0:03:28 (0.6660 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.3591  Acc@1: 93.7500 (89.0200)  Acc@5: 100.0000 (99.4200)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:34  Loss: 0.8557 (0.8557)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5470  data: 0.1516  max mem: 2373\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.6478 (0.5963)  Acc@1: 81.2500 (85.7955)  Acc@5: 100.0000 (99.4318)  time: 0.4406  data: 0.0150  max mem: 2373\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.6478 (0.6494)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (98.8095)  time: 0.4306  data: 0.0023  max mem: 2373\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5810 (0.6218)  Acc@1: 81.2500 (84.2742)  Acc@5: 100.0000 (99.1935)  time: 0.4300  data: 0.0019  max mem: 2373\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.4946 (0.5961)  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (99.2378)  time: 0.4297  data: 0.0004  max mem: 2373\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4587 (0.5723)  Acc@1: 87.5000 (86.5196)  Acc@5: 100.0000 (99.2647)  time: 0.4296  data: 0.0025  max mem: 2373\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4569 (0.5571)  Acc@1: 93.7500 (87.2951)  Acc@5: 100.0000 (99.1803)  time: 0.4295  data: 0.0029  max mem: 2373\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4524 (0.5526)  Acc@1: 93.7500 (87.6000)  Acc@5: 100.0000 (99.2000)  time: 0.4189  data: 0.0029  max mem: 2373\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4292 s / it)\n",
            "* Acc@1 87.600 Acc@5 99.200 loss 0.553\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:39  Loss: 0.8404 (0.8404)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.6232  data: 0.2265  max mem: 2373\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.7207 (0.7189)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (97.1591)  time: 0.4463  data: 0.0209  max mem: 2373\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.7110 (0.7700)  Acc@1: 87.5000 (84.2262)  Acc@5: 100.0000 (97.3214)  time: 0.4290  data: 0.0015  max mem: 2373\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7110 (0.7498)  Acc@1: 81.2500 (85.0806)  Acc@5: 100.0000 (96.7742)  time: 0.4298  data: 0.0016  max mem: 2373\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:09  Loss: 0.6228 (0.7282)  Acc@1: 87.5000 (85.3659)  Acc@5: 100.0000 (96.9512)  time: 0.4308  data: 0.0005  max mem: 2373\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6228 (0.7280)  Acc@1: 81.2500 (84.1912)  Acc@5: 100.0000 (97.0588)  time: 0.4311  data: 0.0017  max mem: 2373\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.6032 (0.7084)  Acc@1: 81.2500 (85.1434)  Acc@5: 100.0000 (97.3361)  time: 0.4315  data: 0.0017  max mem: 2373\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5957 (0.7002)  Acc@1: 87.5000 (85.2000)  Acc@5: 100.0000 (97.4000)  time: 0.4209  data: 0.0017  max mem: 2373\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4310 s / it)\n",
            "* Acc@1 85.200 Acc@5 97.400 loss 0.700\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:34  Loss: 0.3739 (0.3739)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5513  data: 0.1519  max mem: 2373\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5623 (0.6029)  Acc@1: 87.5000 (83.5227)  Acc@5: 100.0000 (97.1591)  time: 0.4433  data: 0.0148  max mem: 2373\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:18  Loss: 0.6745 (0.6095)  Acc@1: 87.5000 (84.2262)  Acc@5: 93.7500 (97.0238)  time: 0.4330  data: 0.0023  max mem: 2373\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.6438 (0.6101)  Acc@1: 81.2500 (83.8710)  Acc@5: 100.0000 (97.7823)  time: 0.4333  data: 0.0022  max mem: 2373\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.5547 (0.6059)  Acc@1: 81.2500 (84.2988)  Acc@5: 100.0000 (98.1707)  time: 0.4334  data: 0.0010  max mem: 2373\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.6387 (0.6123)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4343  data: 0.0024  max mem: 2373\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.6387 (0.6199)  Acc@1: 87.5000 (85.1434)  Acc@5: 100.0000 (97.9508)  time: 0.4352  data: 0.0020  max mem: 2373\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.6869 (0.6203)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.9000)  time: 0.4244  data: 0.0015  max mem: 2373\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4331 s / it)\n",
            "* Acc@1 84.900 Acc@5 97.900 loss 0.620\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:39  Loss: 0.7500 (0.7500)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.6292  data: 0.2279  max mem: 2373\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:24  Loss: 0.6926 (0.6195)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (97.7273)  time: 0.4531  data: 0.0216  max mem: 2373\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:19  Loss: 0.5853 (0.6083)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (96.7262)  time: 0.4357  data: 0.0018  max mem: 2373\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4736 (0.5783)  Acc@1: 87.5000 (86.8952)  Acc@5: 100.0000 (97.1774)  time: 0.4360  data: 0.0016  max mem: 2373\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3626 (0.5328)  Acc@1: 93.7500 (88.1098)  Acc@5: 100.0000 (97.7134)  time: 0.4359  data: 0.0019  max mem: 2373\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4589 (0.5475)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (97.7941)  time: 0.4355  data: 0.0029  max mem: 2373\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5247 (0.5634)  Acc@1: 87.5000 (87.6025)  Acc@5: 100.0000 (97.3361)  time: 0.4350  data: 0.0014  max mem: 2373\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5247 (0.5618)  Acc@1: 87.5000 (87.7000)  Acc@5: 100.0000 (97.4000)  time: 0.4242  data: 0.0007  max mem: 2373\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4360 s / it)\n",
            "* Acc@1 87.700 Acc@5 97.400 loss 0.562\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:37  Loss: 0.2667 (0.2667)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5898  data: 0.1936  max mem: 2373\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:23  Loss: 0.4531 (0.5351)  Acc@1: 93.7500 (91.4773)  Acc@5: 100.0000 (98.2955)  time: 0.4483  data: 0.0193  max mem: 2373\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:19  Loss: 0.4531 (0.4820)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4348  data: 0.0011  max mem: 2373\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.4307 (0.4756)  Acc@1: 93.7500 (92.1371)  Acc@5: 100.0000 (98.5887)  time: 0.4345  data: 0.0004  max mem: 2373\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.4049 (0.4612)  Acc@1: 93.7500 (92.6829)  Acc@5: 100.0000 (98.4756)  time: 0.4334  data: 0.0020  max mem: 2373\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4049 (0.4702)  Acc@1: 93.7500 (93.0147)  Acc@5: 100.0000 (98.4069)  time: 0.4337  data: 0.0024  max mem: 2373\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4179 (0.4735)  Acc@1: 93.7500 (92.7254)  Acc@5: 100.0000 (98.3607)  time: 0.4339  data: 0.0008  max mem: 2373\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4667 (0.4906)  Acc@1: 93.7500 (92.3000)  Acc@5: 100.0000 (98.3000)  time: 0.4228  data: 0.0005  max mem: 2373\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4338 s / it)\n",
            "* Acc@1 92.300 Acc@5 98.300 loss 0.491\n",
            "Test: [Task 6]  [ 0/63]  eta: 0:00:41  Loss: 0.4410 (0.4410)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6515  data: 0.2504  max mem: 2373\n",
            "Test: [Task 6]  [10/63]  eta: 0:00:23  Loss: 0.6429 (0.5867)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (99.4318)  time: 0.4521  data: 0.0256  max mem: 2373\n",
            "Test: [Task 6]  [20/63]  eta: 0:00:19  Loss: 0.6429 (0.6360)  Acc@1: 81.2500 (83.0357)  Acc@5: 100.0000 (99.1071)  time: 0.4329  data: 0.0019  max mem: 2373\n",
            "Test: [Task 6]  [30/63]  eta: 0:00:14  Loss: 0.5972 (0.6252)  Acc@1: 81.2500 (82.2581)  Acc@5: 100.0000 (99.3952)  time: 0.4326  data: 0.0006  max mem: 2373\n",
            "Test: [Task 6]  [40/63]  eta: 0:00:10  Loss: 0.6066 (0.6568)  Acc@1: 75.0000 (81.0976)  Acc@5: 100.0000 (98.9329)  time: 0.4316  data: 0.0023  max mem: 2373\n",
            "Test: [Task 6]  [50/63]  eta: 0:00:05  Loss: 0.6548 (0.6397)  Acc@1: 81.2500 (81.6176)  Acc@5: 100.0000 (99.1422)  time: 0.4321  data: 0.0025  max mem: 2373\n",
            "Test: [Task 6]  [60/63]  eta: 0:00:01  Loss: 0.5738 (0.6575)  Acc@1: 81.2500 (81.7623)  Acc@5: 100.0000 (98.9754)  time: 0.4322  data: 0.0005  max mem: 2373\n",
            "Test: [Task 6]  [62/63]  eta: 0:00:00  Loss: 0.5738 (0.6536)  Acc@1: 81.2500 (82.1000)  Acc@5: 100.0000 (99.0000)  time: 0.4211  data: 0.0005  max mem: 2373\n",
            "Test: [Task 6] Total time: 0:00:27 (0.4331 s / it)\n",
            "* Acc@1 82.100 Acc@5 99.000 loss 0.654\n",
            "[Average accuracy till task6]\tAcc@1: 86.6333\tAcc@5: 98.2000\tLoss: 0.5965\tForgetting: 5.4200\tBackward: -5.1200\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:04:30  Lr: 0.001875  Loss: 2.1074  Acc@1: 0.0000 (0.0000)  Acc@5: 31.2500 (31.2500)  time: 0.8638  data: 0.2220  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: 1.8096  Acc@1: 56.2500 (50.0000)  Acc@5: 81.2500 (79.5455)  time: 0.6802  data: 0.0235  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:17  Lr: 0.001875  Loss: 1.3881  Acc@1: 75.0000 (62.7976)  Acc@5: 93.7500 (87.2024)  time: 0.6635  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 1.0879  Acc@1: 81.2500 (68.9516)  Acc@5: 93.7500 (90.1210)  time: 0.6659  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: 0.8301  Acc@1: 81.2500 (72.2561)  Acc@5: 100.0000 (92.2256)  time: 0.6668  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.9180  Acc@1: 81.2500 (73.8971)  Acc@5: 100.0000 (93.1373)  time: 0.6670  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.5503  Acc@1: 81.2500 (75.4098)  Acc@5: 100.0000 (93.7500)  time: 0.6679  data: 0.0028  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.6614  Acc@1: 81.2500 (76.8486)  Acc@5: 93.7500 (94.0141)  time: 0.6687  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.6817  Acc@1: 81.2500 (77.3148)  Acc@5: 93.7500 (94.1358)  time: 0.6691  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.4558  Acc@1: 81.2500 (78.3654)  Acc@5: 100.0000 (94.6429)  time: 0.6693  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.8787  Acc@1: 87.5000 (79.2698)  Acc@5: 100.0000 (94.9257)  time: 0.6689  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.2236  Acc@1: 87.5000 (79.8986)  Acc@5: 100.0000 (95.3829)  time: 0.6679  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.4994  Acc@1: 87.5000 (80.3202)  Acc@5: 100.0000 (95.5579)  time: 0.6666  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.4675  Acc@1: 87.5000 (80.6298)  Acc@5: 100.0000 (95.8015)  time: 0.6661  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.5278  Acc@1: 87.5000 (81.0727)  Acc@5: 100.0000 (95.8777)  time: 0.6664  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.1473  Acc@1: 87.5000 (81.4156)  Acc@5: 100.0000 (96.1507)  time: 0.6662  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.2288  Acc@1: 87.5000 (81.5217)  Acc@5: 100.0000 (96.3509)  time: 0.6643  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.2042  Acc@1: 87.5000 (82.1637)  Acc@5: 100.0000 (96.4912)  time: 0.6626  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.0127  Acc@1: 87.5000 (82.5276)  Acc@5: 100.0000 (96.6160)  time: 0.6636  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1410  Acc@1: 87.5000 (82.8207)  Acc@5: 100.0000 (96.6950)  time: 0.6628  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.4157  Acc@1: 87.5000 (82.8669)  Acc@5: 100.0000 (96.7351)  time: 0.6617  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.2601  Acc@1: 87.5000 (82.9976)  Acc@5: 100.0000 (96.8009)  time: 0.6627  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:01  Lr: 0.001875  Loss: 0.1216  Acc@1: 87.5000 (83.1448)  Acc@5: 100.0000 (96.8891)  time: 0.6635  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2687  Acc@1: 93.7500 (83.4145)  Acc@5: 100.0000 (96.9968)  time: 0.6637  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.5435  Acc@1: 87.5000 (83.5322)  Acc@5: 100.0000 (96.9658)  time: 0.6637  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:41  Lr: 0.001875  Loss: 0.1152  Acc@1: 87.5000 (83.6155)  Acc@5: 100.0000 (97.0867)  time: 0.6642  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.2645  Acc@1: 87.5000 (83.6446)  Acc@5: 100.0000 (97.1264)  time: 0.6651  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.7514  Acc@1: 87.5000 (83.7177)  Acc@5: 100.0000 (97.1633)  time: 0.6664  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:21  Lr: 0.001875  Loss: 0.3679  Acc@1: 81.2500 (83.7189)  Acc@5: 100.0000 (97.2642)  time: 0.6667  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2100  Acc@1: 81.2500 (83.7199)  Acc@5: 100.0000 (97.2723)  time: 0.6666  data: 0.0033  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1428  Acc@1: 87.5000 (83.7209)  Acc@5: 100.0000 (97.2799)  time: 0.6669  data: 0.0031  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:01  Lr: 0.001875  Loss: 0.3149  Acc@1: 87.5000 (83.8223)  Acc@5: 100.0000 (97.3473)  time: 0.6668  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.1844  Acc@1: 87.5000 (83.8600)  Acc@5: 100.0000 (97.3600)  time: 0.6501  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:28 (0.6655 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.1844  Acc@1: 87.5000 (83.8600)  Acc@5: 100.0000 (97.3600)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:18  Lr: 0.001875  Loss: 0.1305  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8244  data: 0.2000  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:27  Lr: 0.001875  Loss: -0.0684  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.8636)  time: 0.6836  data: 0.0215  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: -0.0308  Acc@1: 87.5000 (85.1190)  Acc@5: 100.0000 (99.1071)  time: 0.6693  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: -0.0396  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (98.5887)  time: 0.6689  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.0528  Acc@1: 87.5000 (86.4329)  Acc@5: 100.0000 (98.4756)  time: 0.6691  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.0115  Acc@1: 87.5000 (87.1324)  Acc@5: 100.0000 (98.7745)  time: 0.6693  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.2803  Acc@1: 87.5000 (86.7828)  Acc@5: 100.0000 (98.8730)  time: 0.6692  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: 0.6999  Acc@1: 87.5000 (86.7958)  Acc@5: 100.0000 (98.7676)  time: 0.6700  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0867  Acc@1: 87.5000 (87.1142)  Acc@5: 100.0000 (98.7654)  time: 0.6708  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.1140  Acc@1: 87.5000 (87.1566)  Acc@5: 100.0000 (98.6951)  time: 0.6701  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.1552  Acc@1: 87.5000 (87.2525)  Acc@5: 100.0000 (98.7005)  time: 0.6701  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.1630  Acc@1: 87.5000 (87.3311)  Acc@5: 100.0000 (98.7050)  time: 0.6697  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.1036  Acc@1: 87.5000 (87.2934)  Acc@5: 100.0000 (98.6570)  time: 0.6684  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.1135  Acc@1: 87.5000 (87.2137)  Acc@5: 100.0000 (98.6641)  time: 0.6683  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.2485  Acc@1: 87.5000 (87.1011)  Acc@5: 100.0000 (98.7589)  time: 0.6671  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.2451  Acc@1: 87.5000 (87.1689)  Acc@5: 100.0000 (98.8411)  time: 0.6667  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.0454  Acc@1: 87.5000 (87.3059)  Acc@5: 100.0000 (98.8742)  time: 0.6669  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.0947  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.9035)  time: 0.6667  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.0764  Acc@1: 87.5000 (87.4309)  Acc@5: 100.0000 (98.9296)  time: 0.6663  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.1424  Acc@1: 87.5000 (87.6963)  Acc@5: 100.0000 (98.9202)  time: 0.6654  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.0488  Acc@1: 87.5000 (87.7177)  Acc@5: 100.0000 (98.8806)  time: 0.6659  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.3025  Acc@1: 87.5000 (87.7073)  Acc@5: 100.0000 (98.8448)  time: 0.6659  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2171  Acc@1: 87.5000 (87.6414)  Acc@5: 100.0000 (98.8405)  time: 0.6653  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: -0.0153  Acc@1: 87.5000 (87.6082)  Acc@5: 100.0000 (98.8636)  time: 0.6644  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 1.1332  Acc@1: 87.5000 (87.5778)  Acc@5: 100.0000 (98.8071)  time: 0.6642  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2340  Acc@1: 87.5000 (87.5498)  Acc@5: 100.0000 (98.8546)  time: 0.6646  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0214  Acc@1: 93.7500 (87.6676)  Acc@5: 100.0000 (98.8985)  time: 0.6648  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1491  Acc@1: 87.5000 (87.7306)  Acc@5: 100.0000 (98.9161)  time: 0.6641  data: 0.0040  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.2005  Acc@1: 87.5000 (87.7002)  Acc@5: 100.0000 (98.9101)  time: 0.6643  data: 0.0029  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0433  Acc@1: 87.5000 (87.7363)  Acc@5: 100.0000 (98.8617)  time: 0.6649  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1579  Acc@1: 87.5000 (87.6453)  Acc@5: 100.0000 (98.8787)  time: 0.6651  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2657  Acc@1: 87.5000 (87.6206)  Acc@5: 100.0000 (98.8947)  time: 0.6653  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0693  Acc@1: 87.5000 (87.6600)  Acc@5: 100.0000 (98.9000)  time: 0.6489  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:28 (0.6667 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0693  Acc@1: 87.5000 (87.6600)  Acc@5: 100.0000 (98.9000)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:04:16  Lr: 0.001875  Loss: -0.0454  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8183  data: 0.1983  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:25  Lr: 0.001875  Loss: 0.2760  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (100.0000)  time: 0.6786  data: 0.0205  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:16  Lr: 0.001875  Loss: 0.2484  Acc@1: 87.5000 (90.1786)  Acc@5: 100.0000 (99.7024)  time: 0.6640  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:09  Lr: 0.001875  Loss: 0.1940  Acc@1: 87.5000 (89.1129)  Acc@5: 100.0000 (99.5968)  time: 0.6636  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:02  Lr: 0.001875  Loss: -0.1004  Acc@1: 87.5000 (89.0244)  Acc@5: 100.0000 (98.9329)  time: 0.6635  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:55  Lr: 0.001875  Loss: 0.1155  Acc@1: 93.7500 (89.7059)  Acc@5: 100.0000 (99.0196)  time: 0.6632  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:48  Lr: 0.001875  Loss: 0.2121  Acc@1: 93.7500 (89.4467)  Acc@5: 100.0000 (99.0779)  time: 0.6633  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:41  Lr: 0.001875  Loss: -0.0185  Acc@1: 81.2500 (88.3803)  Acc@5: 100.0000 (98.9437)  time: 0.6638  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: -0.1427  Acc@1: 87.5000 (88.7346)  Acc@5: 100.0000 (98.9969)  time: 0.6646  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:28  Lr: 0.001875  Loss: 0.3520  Acc@1: 93.7500 (88.5989)  Acc@5: 100.0000 (99.1071)  time: 0.6650  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:21  Lr: 0.001875  Loss: 0.1275  Acc@1: 87.5000 (88.6139)  Acc@5: 100.0000 (99.1337)  time: 0.6657  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.5375  Acc@1: 93.7500 (88.7387)  Acc@5: 100.0000 (99.0991)  time: 0.6665  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: -0.2045  Acc@1: 87.5000 (88.2748)  Acc@5: 100.0000 (98.9669)  time: 0.6665  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:01  Lr: 0.001875  Loss: 0.0871  Acc@1: 81.2500 (88.0248)  Acc@5: 100.0000 (98.8073)  time: 0.6662  data: 0.0037  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.3988  Acc@1: 81.2500 (87.8546)  Acc@5: 100.0000 (98.8475)  time: 0.6666  data: 0.0045  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.0231  Acc@1: 87.5000 (87.7897)  Acc@5: 100.0000 (98.5927)  time: 0.6670  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:41  Lr: 0.001875  Loss: 0.0131  Acc@1: 87.5000 (87.9658)  Acc@5: 100.0000 (98.6025)  time: 0.6677  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.3662  Acc@1: 87.5000 (88.0482)  Acc@5: 100.0000 (98.6842)  time: 0.6682  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.1440  Acc@1: 87.5000 (88.0180)  Acc@5: 100.0000 (98.6533)  time: 0.6679  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:21  Lr: 0.001875  Loss: 0.2226  Acc@1: 87.5000 (88.1545)  Acc@5: 100.0000 (98.6584)  time: 0.6679  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: 0.2088  Acc@1: 87.5000 (87.9975)  Acc@5: 100.0000 (98.6318)  time: 0.6684  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.1109  Acc@1: 87.5000 (88.0924)  Acc@5: 100.0000 (98.6374)  time: 0.6697  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.1938  Acc@1: 87.5000 (87.8676)  Acc@5: 100.0000 (98.5577)  time: 0.6707  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2594  Acc@1: 87.5000 (87.9058)  Acc@5: 100.0000 (98.5660)  time: 0.6710  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[3/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.0064  Acc@1: 87.5000 (87.8890)  Acc@5: 100.0000 (98.5737)  time: 0.6712  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.0333  Acc@1: 87.5000 (87.8735)  Acc@5: 100.0000 (98.6305)  time: 0.6704  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0149  Acc@1: 93.7500 (88.0987)  Acc@5: 100.0000 (98.6590)  time: 0.6700  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.3902  Acc@1: 87.5000 (88.0304)  Acc@5: 100.0000 (98.6854)  time: 0.6705  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: -0.1123  Acc@1: 87.5000 (88.2117)  Acc@5: 100.0000 (98.7100)  time: 0.6701  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[3/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.0781  Acc@1: 93.7500 (88.3806)  Acc@5: 100.0000 (98.6899)  time: 0.6692  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1305  Acc@1: 87.5000 (88.3098)  Acc@5: 100.0000 (98.7126)  time: 0.6689  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3479  Acc@1: 87.5000 (88.2838)  Acc@5: 100.0000 (98.7540)  time: 0.6685  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[3/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.1430  Acc@1: 87.5000 (88.3200)  Acc@5: 100.0000 (98.7600)  time: 0.6520  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[3/5] Total time: 0:03:28 (0.6670 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.1430  Acc@1: 87.5000 (88.3200)  Acc@5: 100.0000 (98.7600)\n",
            "Train: Epoch[4/5]  [  0/313]  eta: 0:04:34  Lr: 0.001875  Loss: 0.0316  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.8764  data: 0.2535  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 10/313]  eta: 0:03:28  Lr: 0.001875  Loss: -0.0517  Acc@1: 93.7500 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.6877  data: 0.0235  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.1272  Acc@1: 93.7500 (89.5833)  Acc@5: 100.0000 (99.1071)  time: 0.6688  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 30/313]  eta: 0:03:11  Lr: 0.001875  Loss: -0.0323  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (99.1935)  time: 0.6689  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.4639  Acc@1: 87.5000 (88.1098)  Acc@5: 100.0000 (98.9329)  time: 0.6676  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.2122  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (98.4069)  time: 0.6668  data: 0.0011  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: -0.0477  Acc@1: 87.5000 (88.2172)  Acc@5: 100.0000 (98.2582)  time: 0.6671  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: -0.1668  Acc@1: 87.5000 (88.4683)  Acc@5: 100.0000 (98.3275)  time: 0.6662  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.0004  Acc@1: 87.5000 (88.9660)  Acc@5: 100.0000 (98.3796)  time: 0.6657  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.3722  Acc@1: 87.5000 (88.6676)  Acc@5: 100.0000 (98.4203)  time: 0.6662  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[4/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: -0.1289  Acc@1: 87.5000 (88.9851)  Acc@5: 100.0000 (98.5149)  time: 0.6666  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.2682  Acc@1: 87.5000 (88.9640)  Acc@5: 100.0000 (98.5360)  time: 0.6667  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[4/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.2729  Acc@1: 87.5000 (88.7397)  Acc@5: 100.0000 (98.5537)  time: 0.6672  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0596  Acc@1: 87.5000 (88.9790)  Acc@5: 100.0000 (98.6164)  time: 0.6670  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: -0.1490  Acc@1: 93.7500 (89.1401)  Acc@5: 100.0000 (98.6702)  time: 0.6660  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: 0.3285  Acc@1: 93.7500 (89.1556)  Acc@5: 100.0000 (98.7169)  time: 0.6659  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[4/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.0219  Acc@1: 87.5000 (89.0528)  Acc@5: 100.0000 (98.6801)  time: 0.6661  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.1580  Acc@1: 93.7500 (89.3275)  Acc@5: 100.0000 (98.7573)  time: 0.6662  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: -0.1454  Acc@1: 93.7500 (89.1229)  Acc@5: 100.0000 (98.7569)  time: 0.6663  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: -0.1827  Acc@1: 87.5000 (89.1034)  Acc@5: 100.0000 (98.7893)  time: 0.6663  data: 0.0032  max mem: 2373\n",
            "Train: Epoch[4/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0869  Acc@1: 87.5000 (89.1480)  Acc@5: 100.0000 (98.8495)  time: 0.6662  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[4/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.0173  Acc@1: 93.7500 (89.3069)  Acc@5: 100.0000 (98.8744)  time: 0.6663  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[4/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2744  Acc@1: 87.5000 (89.1686)  Acc@5: 100.0000 (98.8405)  time: 0.6648  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[4/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0687  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (98.8095)  time: 0.6639  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[4/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.1191  Acc@1: 87.5000 (89.3413)  Acc@5: 100.0000 (98.8589)  time: 0.6655  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[4/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.1322  Acc@1: 87.5000 (89.2928)  Acc@5: 100.0000 (98.8795)  time: 0.6663  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0347  Acc@1: 87.5000 (89.4157)  Acc@5: 100.0000 (98.8745)  time: 0.6661  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[4/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: -0.1338  Acc@1: 93.7500 (89.5987)  Acc@5: 100.0000 (98.8699)  time: 0.6649  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.1800  Acc@1: 93.7500 (89.6352)  Acc@5: 100.0000 (98.8657)  time: 0.6644  data: 0.0037  max mem: 2373\n",
            "Train: Epoch[4/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: -0.1623  Acc@1: 93.7500 (89.7122)  Acc@5: 100.0000 (98.8617)  time: 0.6643  data: 0.0032  max mem: 2373\n",
            "Train: Epoch[4/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.3450  Acc@1: 87.5000 (89.5972)  Acc@5: 100.0000 (98.8787)  time: 0.6638  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[4/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3037  Acc@1: 87.5000 (89.5498)  Acc@5: 100.0000 (98.8746)  time: 0.6638  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[4/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.0023  Acc@1: 87.5000 (89.5600)  Acc@5: 100.0000 (98.8800)  time: 0.6474  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[4/5] Total time: 0:03:28 (0.6660 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.0023  Acc@1: 87.5000 (89.5600)  Acc@5: 100.0000 (98.8800)\n",
            "Train: Epoch[5/5]  [  0/313]  eta: 0:05:00  Lr: 0.001875  Loss: -0.1369  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.9586  data: 0.3372  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 10/313]  eta: 0:03:29  Lr: 0.001875  Loss: -0.0314  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (98.2955)  time: 0.6898  data: 0.0318  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.1264  Acc@1: 93.7500 (91.9643)  Acc@5: 100.0000 (98.8095)  time: 0.6628  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.0616  Acc@1: 93.7500 (91.1290)  Acc@5: 100.0000 (98.9919)  time: 0.6642  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: -0.0990  Acc@1: 87.5000 (90.8537)  Acc@5: 100.0000 (98.9329)  time: 0.6660  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.6021  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.1422)  time: 0.6662  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 60/313]  eta: 0:02:49  Lr: 0.001875  Loss: 0.1884  Acc@1: 87.5000 (89.7541)  Acc@5: 100.0000 (99.1803)  time: 0.6664  data: 0.0028  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 70/313]  eta: 0:02:42  Lr: 0.001875  Loss: 0.5297  Acc@1: 87.5000 (89.5246)  Acc@5: 100.0000 (99.1197)  time: 0.6664  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 80/313]  eta: 0:02:35  Lr: 0.001875  Loss: 0.2114  Acc@1: 93.7500 (89.9691)  Acc@5: 100.0000 (99.1512)  time: 0.6663  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[5/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: 0.2172  Acc@1: 87.5000 (89.5604)  Acc@5: 100.0000 (99.1758)  time: 0.6666  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[5/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.0364  Acc@1: 87.5000 (89.4802)  Acc@5: 100.0000 (99.1955)  time: 0.6666  data: 0.0024  max mem: 2373\n",
            "Train: Epoch[5/5]  [110/313]  eta: 0:02:15  Lr: 0.001875  Loss: 0.3037  Acc@1: 87.5000 (89.1329)  Acc@5: 100.0000 (99.0428)  time: 0.6668  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [120/313]  eta: 0:02:08  Lr: 0.001875  Loss: -0.0066  Acc@1: 87.5000 (89.1529)  Acc@5: 100.0000 (99.0702)  time: 0.6672  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[5/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: -0.0176  Acc@1: 87.5000 (89.1221)  Acc@5: 100.0000 (98.9981)  time: 0.6674  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[5/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0677  Acc@1: 87.5000 (89.0514)  Acc@5: 100.0000 (98.9805)  time: 0.6682  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [150/313]  eta: 0:01:48  Lr: 0.001875  Loss: -0.0589  Acc@1: 87.5000 (89.1556)  Acc@5: 100.0000 (98.9652)  time: 0.6692  data: 0.0033  max mem: 2373\n",
            "Train: Epoch[5/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.2152  Acc@1: 93.7500 (89.2857)  Acc@5: 100.0000 (98.9519)  time: 0.6688  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[5/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: -0.2005  Acc@1: 93.7500 (89.2909)  Acc@5: 100.0000 (99.0132)  time: 0.6686  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[5/5]  [180/313]  eta: 0:01:28  Lr: 0.001875  Loss: 0.4630  Acc@1: 93.7500 (89.2610)  Acc@5: 100.0000 (98.9296)  time: 0.6688  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1763  Acc@1: 87.5000 (89.2670)  Acc@5: 100.0000 (98.9856)  time: 0.6689  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[5/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.1011  Acc@1: 93.7500 (89.5522)  Acc@5: 100.0000 (99.0361)  time: 0.6689  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[5/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: -0.1025  Acc@1: 93.7500 (89.6623)  Acc@5: 100.0000 (99.0818)  time: 0.6691  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.2340  Acc@1: 93.7500 (89.7624)  Acc@5: 100.0000 (99.0667)  time: 0.6695  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[5/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.2512  Acc@1: 87.5000 (89.8810)  Acc@5: 100.0000 (99.1071)  time: 0.6705  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: -0.0187  Acc@1: 87.5000 (89.9118)  Acc@5: 100.0000 (99.1183)  time: 0.6708  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[5/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.1385  Acc@1: 87.5000 (89.7410)  Acc@5: 100.0000 (99.1036)  time: 0.6697  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[5/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0487  Acc@1: 87.5000 (89.7989)  Acc@5: 100.0000 (99.0900)  time: 0.6699  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[5/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.1185  Acc@1: 87.5000 (89.8063)  Acc@5: 100.0000 (99.0544)  time: 0.6703  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[5/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0315  Acc@1: 93.7500 (89.9689)  Acc@5: 100.0000 (99.0658)  time: 0.6693  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.0977  Acc@1: 87.5000 (89.5619)  Acc@5: 100.0000 (99.0335)  time: 0.6694  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[5/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.5841  Acc@1: 81.2500 (89.5556)  Acc@5: 100.0000 (99.0656)  time: 0.6693  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[5/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.2300  Acc@1: 87.5000 (89.4895)  Acc@5: 100.0000 (99.0555)  time: 0.6701  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[5/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: -0.0338  Acc@1: 93.7500 (89.5400)  Acc@5: 100.0000 (99.0600)  time: 0.6537  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[5/5] Total time: 0:03:29 (0.6681 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: -0.0338  Acc@1: 93.7500 (89.5400)  Acc@5: 100.0000 (99.0600)\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:44  Loss: 0.8063 (0.8063)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.7036  data: 0.2937  max mem: 2373\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:24  Loss: 0.6840 (0.6192)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4590  data: 0.0327  max mem: 2373\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.6964 (0.6710)  Acc@1: 81.2500 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4346  data: 0.0037  max mem: 2373\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5941 (0.6455)  Acc@1: 81.2500 (85.6855)  Acc@5: 100.0000 (98.1855)  time: 0.4350  data: 0.0017  max mem: 2373\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5753 (0.6239)  Acc@1: 87.5000 (85.9756)  Acc@5: 100.0000 (98.4756)  time: 0.4347  data: 0.0028  max mem: 2373\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4881 (0.6033)  Acc@1: 87.5000 (86.2745)  Acc@5: 100.0000 (98.4069)  time: 0.4352  data: 0.0017  max mem: 2373\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4766 (0.5865)  Acc@1: 87.5000 (87.0902)  Acc@5: 100.0000 (98.3607)  time: 0.4360  data: 0.0003  max mem: 2373\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4712 (0.5856)  Acc@1: 93.7500 (87.3000)  Acc@5: 100.0000 (98.4000)  time: 0.4251  data: 0.0003  max mem: 2373\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4372 s / it)\n",
            "* Acc@1 87.300 Acc@5 98.400 loss 0.586\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:42  Loss: 0.8421 (0.8421)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6746  data: 0.2572  max mem: 2373\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:24  Loss: 0.7135 (0.7431)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.1591)  time: 0.4552  data: 0.0240  max mem: 2373\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7636 (0.8059)  Acc@1: 87.5000 (83.0357)  Acc@5: 100.0000 (96.4286)  time: 0.4343  data: 0.0006  max mem: 2373\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.8425 (0.7955)  Acc@1: 81.2500 (83.6694)  Acc@5: 93.7500 (95.9677)  time: 0.4353  data: 0.0012  max mem: 2373\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.7209 (0.7759)  Acc@1: 81.2500 (83.6890)  Acc@5: 100.0000 (96.3415)  time: 0.4353  data: 0.0022  max mem: 2373\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6508 (0.7737)  Acc@1: 81.2500 (83.4559)  Acc@5: 100.0000 (96.4461)  time: 0.4355  data: 0.0014  max mem: 2373\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.6478 (0.7536)  Acc@1: 81.2500 (84.0164)  Acc@5: 100.0000 (96.8238)  time: 0.4356  data: 0.0003  max mem: 2373\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.6170 (0.7457)  Acc@1: 87.5000 (84.1000)  Acc@5: 100.0000 (96.9000)  time: 0.4247  data: 0.0003  max mem: 2373\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4366 s / it)\n",
            "* Acc@1 84.100 Acc@5 96.900 loss 0.746\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:44  Loss: 0.5316 (0.5316)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.6988  data: 0.2941  max mem: 2373\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:24  Loss: 0.6729 (0.6522)  Acc@1: 81.2500 (82.9545)  Acc@5: 100.0000 (96.5909)  time: 0.4580  data: 0.0272  max mem: 2373\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.6896 (0.6666)  Acc@1: 81.2500 (83.6310)  Acc@5: 100.0000 (96.4286)  time: 0.4350  data: 0.0006  max mem: 2373\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.6718 (0.6589)  Acc@1: 81.2500 (83.2661)  Acc@5: 100.0000 (96.7742)  time: 0.4360  data: 0.0021  max mem: 2373\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.5163 (0.6440)  Acc@1: 87.5000 (83.9939)  Acc@5: 100.0000 (97.4085)  time: 0.4355  data: 0.0046  max mem: 2373\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.6786 (0.6586)  Acc@1: 87.5000 (84.0686)  Acc@5: 100.0000 (96.9363)  time: 0.4352  data: 0.0031  max mem: 2373\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.7117 (0.6738)  Acc@1: 81.2500 (83.5041)  Acc@5: 93.7500 (96.7213)  time: 0.4355  data: 0.0005  max mem: 2373\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.7117 (0.6726)  Acc@1: 81.2500 (83.4000)  Acc@5: 93.7500 (96.8000)  time: 0.4247  data: 0.0005  max mem: 2373\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4370 s / it)\n",
            "* Acc@1 83.400 Acc@5 96.800 loss 0.673\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:34  Loss: 0.6884 (0.6884)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.5402  data: 0.1405  max mem: 2373\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.6690 (0.6358)  Acc@1: 87.5000 (85.2273)  Acc@5: 93.7500 (96.5909)  time: 0.4459  data: 0.0131  max mem: 2373\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.6480 (0.6452)  Acc@1: 87.5000 (83.6310)  Acc@5: 93.7500 (96.4286)  time: 0.4364  data: 0.0004  max mem: 2373\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.5037 (0.6278)  Acc@1: 81.2500 (84.6774)  Acc@5: 93.7500 (96.3710)  time: 0.4359  data: 0.0022  max mem: 2373\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3855 (0.5743)  Acc@1: 87.5000 (86.7378)  Acc@5: 100.0000 (97.1037)  time: 0.4354  data: 0.0025  max mem: 2373\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4109 (0.5928)  Acc@1: 93.7500 (87.2549)  Acc@5: 100.0000 (96.8137)  time: 0.4359  data: 0.0006  max mem: 2373\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5688 (0.6009)  Acc@1: 87.5000 (86.7828)  Acc@5: 93.7500 (96.7213)  time: 0.4360  data: 0.0007  max mem: 2373\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5655 (0.5972)  Acc@1: 87.5000 (86.9000)  Acc@5: 100.0000 (96.8000)  time: 0.4252  data: 0.0006  max mem: 2373\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4350 s / it)\n",
            "* Acc@1 86.900 Acc@5 96.800 loss 0.597\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:33  Loss: 0.2541 (0.2541)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5339  data: 0.1337  max mem: 2373\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:23  Loss: 0.5212 (0.5698)  Acc@1: 93.7500 (89.7727)  Acc@5: 100.0000 (97.7273)  time: 0.4453  data: 0.0127  max mem: 2373\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:18  Loss: 0.4897 (0.5155)  Acc@1: 93.7500 (90.7738)  Acc@5: 100.0000 (97.9167)  time: 0.4363  data: 0.0009  max mem: 2373\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.4647 (0.5120)  Acc@1: 87.5000 (90.1210)  Acc@5: 100.0000 (97.9839)  time: 0.4352  data: 0.0028  max mem: 2373\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.4416 (0.4901)  Acc@1: 87.5000 (91.0061)  Acc@5: 100.0000 (98.0183)  time: 0.4350  data: 0.0026  max mem: 2373\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4811 (0.4922)  Acc@1: 93.7500 (91.1765)  Acc@5: 100.0000 (98.2843)  time: 0.4356  data: 0.0023  max mem: 2373\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4351 (0.4926)  Acc@1: 93.7500 (91.0861)  Acc@5: 100.0000 (98.3607)  time: 0.4355  data: 0.0033  max mem: 2373\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4811 (0.5082)  Acc@1: 93.7500 (90.6000)  Acc@5: 100.0000 (98.2000)  time: 0.4246  data: 0.0033  max mem: 2373\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4345 s / it)\n",
            "* Acc@1 90.600 Acc@5 98.200 loss 0.508\n",
            "Test: [Task 6]  [ 0/63]  eta: 0:00:36  Loss: 0.5022 (0.5022)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5758  data: 0.1773  max mem: 2373\n",
            "Test: [Task 6]  [10/63]  eta: 0:00:23  Loss: 0.6730 (0.6493)  Acc@1: 87.5000 (82.9545)  Acc@5: 100.0000 (97.7273)  time: 0.4484  data: 0.0166  max mem: 2373\n",
            "Test: [Task 6]  [20/63]  eta: 0:00:19  Loss: 0.6871 (0.6999)  Acc@1: 81.2500 (80.6548)  Acc@5: 100.0000 (97.9167)  time: 0.4356  data: 0.0009  max mem: 2373\n",
            "Test: [Task 6]  [30/63]  eta: 0:00:14  Loss: 0.6298 (0.6869)  Acc@1: 81.2500 (81.0484)  Acc@5: 100.0000 (98.3871)  time: 0.4355  data: 0.0022  max mem: 2373\n",
            "Test: [Task 6]  [40/63]  eta: 0:00:10  Loss: 0.6846 (0.7260)  Acc@1: 81.2500 (79.5732)  Acc@5: 100.0000 (97.8659)  time: 0.4359  data: 0.0018  max mem: 2373\n",
            "Test: [Task 6]  [50/63]  eta: 0:00:05  Loss: 0.7718 (0.7102)  Acc@1: 75.0000 (79.9020)  Acc@5: 100.0000 (98.1618)  time: 0.4359  data: 0.0009  max mem: 2373\n",
            "Test: [Task 6]  [60/63]  eta: 0:00:01  Loss: 0.7224 (0.7291)  Acc@1: 81.2500 (79.7131)  Acc@5: 100.0000 (97.9508)  time: 0.4358  data: 0.0019  max mem: 2373\n",
            "Test: [Task 6]  [62/63]  eta: 0:00:00  Loss: 0.7224 (0.7265)  Acc@1: 81.2500 (79.9000)  Acc@5: 100.0000 (98.0000)  time: 0.4250  data: 0.0019  max mem: 2373\n",
            "Test: [Task 6] Total time: 0:00:27 (0.4353 s / it)\n",
            "* Acc@1 79.900 Acc@5 98.000 loss 0.727\n",
            "Test: [Task 7]  [ 0/63]  eta: 0:00:35  Loss: 0.6204 (0.6204)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5567  data: 0.1583  max mem: 2373\n",
            "Test: [Task 7]  [10/63]  eta: 0:00:23  Loss: 0.5060 (0.5341)  Acc@1: 87.5000 (85.7955)  Acc@5: 100.0000 (98.2955)  time: 0.4459  data: 0.0156  max mem: 2373\n",
            "Test: [Task 7]  [20/63]  eta: 0:00:18  Loss: 0.5060 (0.5862)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (96.7262)  time: 0.4352  data: 0.0012  max mem: 2373\n",
            "Test: [Task 7]  [30/63]  eta: 0:00:14  Loss: 0.5764 (0.5865)  Acc@1: 87.5000 (86.0887)  Acc@5: 100.0000 (96.9758)  time: 0.4354  data: 0.0014  max mem: 2373\n",
            "Test: [Task 7]  [40/63]  eta: 0:00:10  Loss: 0.4901 (0.5748)  Acc@1: 93.7500 (87.5000)  Acc@5: 100.0000 (97.1037)  time: 0.4353  data: 0.0012  max mem: 2373\n",
            "Test: [Task 7]  [50/63]  eta: 0:00:05  Loss: 0.5489 (0.5906)  Acc@1: 87.5000 (86.6422)  Acc@5: 100.0000 (97.0588)  time: 0.4355  data: 0.0023  max mem: 2373\n",
            "Test: [Task 7]  [60/63]  eta: 0:00:01  Loss: 0.5489 (0.5796)  Acc@1: 87.5000 (87.2951)  Acc@5: 100.0000 (97.0287)  time: 0.4360  data: 0.0027  max mem: 2373\n",
            "Test: [Task 7]  [62/63]  eta: 0:00:00  Loss: 0.5489 (0.5754)  Acc@1: 87.5000 (87.4000)  Acc@5: 100.0000 (97.1000)  time: 0.4252  data: 0.0026  max mem: 2373\n",
            "Test: [Task 7] Total time: 0:00:27 (0.4352 s / it)\n",
            "* Acc@1 87.400 Acc@5 97.100 loss 0.575\n",
            "[Average accuracy till task7]\tAcc@1: 85.6571\tAcc@5: 97.4571\tLoss: 0.6302\tForgetting: 5.7833\tBackward: -5.5333\n",
            "Train: Epoch[1/5]  [  0/313]  eta: 0:05:08  Lr: 0.001875  Loss: 2.0588  Acc@1: 18.7500 (18.7500)  Acc@5: 75.0000 (75.0000)  time: 0.9857  data: 0.3570  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 10/313]  eta: 0:03:31  Lr: 0.001875  Loss: 1.5934  Acc@1: 62.5000 (56.2500)  Acc@5: 87.5000 (84.6591)  time: 0.6981  data: 0.0350  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 20/313]  eta: 0:03:20  Lr: 0.001875  Loss: 1.1902  Acc@1: 68.7500 (66.3690)  Acc@5: 93.7500 (89.5833)  time: 0.6699  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 30/313]  eta: 0:03:12  Lr: 0.001875  Loss: 1.0014  Acc@1: 81.2500 (70.7661)  Acc@5: 100.0000 (91.5323)  time: 0.6706  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 40/313]  eta: 0:03:05  Lr: 0.001875  Loss: 0.9882  Acc@1: 81.2500 (72.5610)  Acc@5: 93.7500 (92.2256)  time: 0.6714  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 50/313]  eta: 0:02:58  Lr: 0.001875  Loss: 0.5321  Acc@1: 81.2500 (75.1225)  Acc@5: 93.7500 (93.3824)  time: 0.6723  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 60/313]  eta: 0:02:51  Lr: 0.001875  Loss: 0.3362  Acc@1: 87.5000 (76.3320)  Acc@5: 100.0000 (94.1598)  time: 0.6720  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 70/313]  eta: 0:02:44  Lr: 0.001875  Loss: 0.3697  Acc@1: 81.2500 (77.3768)  Acc@5: 100.0000 (94.6303)  time: 0.6719  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 80/313]  eta: 0:02:37  Lr: 0.001875  Loss: 0.5064  Acc@1: 81.2500 (78.4722)  Acc@5: 100.0000 (94.9846)  time: 0.6723  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: 0.2184  Acc@1: 81.2500 (78.9835)  Acc@5: 100.0000 (95.2610)  time: 0.6721  data: 0.0025  max mem: 2373\n",
            "Train: Epoch[1/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.3407  Acc@1: 87.5000 (80.0743)  Acc@5: 100.0000 (95.6064)  time: 0.6714  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[1/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.3824  Acc@1: 87.5000 (80.7432)  Acc@5: 100.0000 (95.8896)  time: 0.6712  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [120/313]  eta: 0:02:10  Lr: 0.001875  Loss: 0.3052  Acc@1: 87.5000 (81.0950)  Acc@5: 100.0000 (96.0744)  time: 0.6710  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [130/313]  eta: 0:02:03  Lr: 0.001875  Loss: 0.4621  Acc@1: 81.2500 (81.3454)  Acc@5: 100.0000 (96.1832)  time: 0.6705  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[1/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: 0.3742  Acc@1: 81.2500 (81.3830)  Acc@5: 100.0000 (96.3209)  time: 0.6706  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.2577  Acc@1: 81.2500 (81.9123)  Acc@5: 100.0000 (96.4404)  time: 0.6711  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.1462  Acc@1: 87.5000 (82.3370)  Acc@5: 100.0000 (96.6227)  time: 0.6713  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.2819  Acc@1: 87.5000 (82.6023)  Acc@5: 100.0000 (96.7471)  time: 0.6708  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[1/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.2314  Acc@1: 87.5000 (82.8729)  Acc@5: 100.0000 (96.7887)  time: 0.6701  data: 0.0032  max mem: 2373\n",
            "Train: Epoch[1/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0366  Acc@1: 87.5000 (83.3770)  Acc@5: 100.0000 (96.8914)  time: 0.6707  data: 0.0038  max mem: 2373\n",
            "Train: Epoch[1/5]  [200/313]  eta: 0:01:16  Lr: 0.001875  Loss: 0.6060  Acc@1: 87.5000 (83.6443)  Acc@5: 100.0000 (96.9527)  time: 0.6711  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[1/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: 0.1278  Acc@1: 87.5000 (83.6197)  Acc@5: 100.0000 (96.9787)  time: 0.6706  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.3314  Acc@1: 87.5000 (83.6538)  Acc@5: 100.0000 (97.0023)  time: 0.6708  data: 0.0015  max mem: 2373\n",
            "Train: Epoch[1/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.3092  Acc@1: 81.2500 (83.7392)  Acc@5: 100.0000 (97.0509)  time: 0.6708  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[1/5]  [240/313]  eta: 0:00:49  Lr: 0.001875  Loss: -0.0509  Acc@1: 87.5000 (83.9730)  Acc@5: 100.0000 (97.1473)  time: 0.6712  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.0466  Acc@1: 87.5000 (84.0139)  Acc@5: 100.0000 (97.2112)  time: 0.6708  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[1/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: -0.0087  Acc@1: 87.5000 (84.1954)  Acc@5: 100.0000 (97.1983)  time: 0.6703  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[1/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: 0.2951  Acc@1: 87.5000 (84.2943)  Acc@5: 100.0000 (97.2325)  time: 0.6698  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[1/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0535  Acc@1: 87.5000 (84.4084)  Acc@5: 100.0000 (97.2642)  time: 0.6695  data: 0.0036  max mem: 2373\n",
            "Train: Epoch[1/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.2148  Acc@1: 87.5000 (84.5361)  Acc@5: 100.0000 (97.3368)  time: 0.6700  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[1/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: -0.0680  Acc@1: 93.7500 (84.8214)  Acc@5: 100.0000 (97.3837)  time: 0.6703  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[1/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: 0.3943  Acc@1: 93.7500 (84.8875)  Acc@5: 100.0000 (97.4277)  time: 0.6704  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[1/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.4420  Acc@1: 87.5000 (84.9200)  Acc@5: 100.0000 (97.4400)  time: 0.6539  data: 0.0003  max mem: 2373\n",
            "Train: Epoch[1/5] Total time: 0:03:30 (0.6710 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.4420  Acc@1: 87.5000 (84.9200)  Acc@5: 100.0000 (97.4400)\n",
            "Train: Epoch[2/5]  [  0/313]  eta: 0:04:02  Lr: 0.001875  Loss: 0.4405  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.7741  data: 0.1462  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 10/313]  eta: 0:03:26  Lr: 0.001875  Loss: 0.0993  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (97.7273)  time: 0.6812  data: 0.0137  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 20/313]  eta: 0:03:18  Lr: 0.001875  Loss: 0.3808  Acc@1: 87.5000 (88.3929)  Acc@5: 100.0000 (98.5119)  time: 0.6719  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 30/313]  eta: 0:03:10  Lr: 0.001875  Loss: 0.2913  Acc@1: 93.7500 (88.7097)  Acc@5: 100.0000 (98.3871)  time: 0.6712  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 40/313]  eta: 0:03:03  Lr: 0.001875  Loss: 0.1613  Acc@1: 87.5000 (88.8720)  Acc@5: 100.0000 (98.6280)  time: 0.6702  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 50/313]  eta: 0:02:56  Lr: 0.001875  Loss: 0.1116  Acc@1: 87.5000 (88.9706)  Acc@5: 100.0000 (98.6520)  time: 0.6698  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: -0.0309  Acc@1: 93.7500 (89.5492)  Acc@5: 100.0000 (98.6680)  time: 0.6696  data: 0.0036  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: -0.0483  Acc@1: 93.7500 (89.3486)  Acc@5: 100.0000 (98.5915)  time: 0.6692  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: -0.1011  Acc@1: 87.5000 (89.6605)  Acc@5: 100.0000 (98.6883)  time: 0.6689  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [ 90/313]  eta: 0:02:29  Lr: 0.001875  Loss: -0.0605  Acc@1: 87.5000 (89.0797)  Acc@5: 100.0000 (98.6951)  time: 0.6688  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [100/313]  eta: 0:02:22  Lr: 0.001875  Loss: 0.3584  Acc@1: 87.5000 (88.9851)  Acc@5: 100.0000 (98.8243)  time: 0.6680  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[2/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.0350  Acc@1: 87.5000 (89.0203)  Acc@5: 100.0000 (98.7613)  time: 0.6684  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[2/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.0499  Acc@1: 93.7500 (89.1529)  Acc@5: 100.0000 (98.8636)  time: 0.6695  data: 0.0007  max mem: 2373\n",
            "Train: Epoch[2/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.3357  Acc@1: 93.7500 (89.0267)  Acc@5: 100.0000 (98.9027)  time: 0.6690  data: 0.0008  max mem: 2373\n",
            "Train: Epoch[2/5]  [140/313]  eta: 0:01:55  Lr: 0.001875  Loss: 0.0094  Acc@1: 87.5000 (89.1844)  Acc@5: 100.0000 (98.8918)  time: 0.6686  data: 0.0031  max mem: 2373\n",
            "Train: Epoch[2/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.2931  Acc@1: 87.5000 (89.0315)  Acc@5: 100.0000 (98.8411)  time: 0.6690  data: 0.0030  max mem: 2373\n",
            "Train: Epoch[2/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: -0.0890  Acc@1: 87.5000 (88.9363)  Acc@5: 100.0000 (98.8354)  time: 0.6685  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [170/313]  eta: 0:01:35  Lr: 0.001875  Loss: 0.0048  Acc@1: 87.5000 (89.0351)  Acc@5: 100.0000 (98.7939)  time: 0.6686  data: 0.0019  max mem: 2373\n",
            "Train: Epoch[2/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.0987  Acc@1: 93.7500 (89.0884)  Acc@5: 100.0000 (98.7569)  time: 0.6687  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[2/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.0694  Acc@1: 93.7500 (89.2016)  Acc@5: 100.0000 (98.7565)  time: 0.6683  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[2/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0962  Acc@1: 87.5000 (89.1791)  Acc@5: 100.0000 (98.7562)  time: 0.6686  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[2/5]  [210/313]  eta: 0:01:08  Lr: 0.001875  Loss: 0.4577  Acc@1: 87.5000 (89.0995)  Acc@5: 100.0000 (98.7263)  time: 0.6689  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [220/313]  eta: 0:01:02  Lr: 0.001875  Loss: 0.5247  Acc@1: 87.5000 (89.1968)  Acc@5: 100.0000 (98.7274)  time: 0.6693  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [230/313]  eta: 0:00:55  Lr: 0.001875  Loss: 0.0070  Acc@1: 87.5000 (88.9881)  Acc@5: 100.0000 (98.7284)  time: 0.6691  data: 0.0029  max mem: 2373\n",
            "Train: Epoch[2/5]  [240/313]  eta: 0:00:48  Lr: 0.001875  Loss: 0.2162  Acc@1: 87.5000 (88.9782)  Acc@5: 100.0000 (98.7293)  time: 0.6686  data: 0.0033  max mem: 2373\n",
            "Train: Epoch[2/5]  [250/313]  eta: 0:00:42  Lr: 0.001875  Loss: 0.2572  Acc@1: 87.5000 (88.8197)  Acc@5: 100.0000 (98.7799)  time: 0.6689  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[2/5]  [260/313]  eta: 0:00:35  Lr: 0.001875  Loss: 0.0031  Acc@1: 87.5000 (88.9128)  Acc@5: 100.0000 (98.8027)  time: 0.6697  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[2/5]  [270/313]  eta: 0:00:28  Lr: 0.001875  Loss: -0.0954  Acc@1: 93.7500 (89.0683)  Acc@5: 100.0000 (98.7546)  time: 0.6699  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[2/5]  [280/313]  eta: 0:00:22  Lr: 0.001875  Loss: 0.0047  Acc@1: 93.7500 (88.9902)  Acc@5: 100.0000 (98.7544)  time: 0.6694  data: 0.0009  max mem: 2373\n",
            "Train: Epoch[2/5]  [290/313]  eta: 0:00:15  Lr: 0.001875  Loss: 0.1366  Acc@1: 87.5000 (88.9820)  Acc@5: 100.0000 (98.7113)  time: 0.6699  data: 0.0012  max mem: 2373\n",
            "Train: Epoch[2/5]  [300/313]  eta: 0:00:08  Lr: 0.001875  Loss: 0.1741  Acc@1: 93.7500 (89.2027)  Acc@5: 100.0000 (98.7542)  time: 0.6701  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[2/5]  [310/313]  eta: 0:00:02  Lr: 0.001875  Loss: -0.1091  Acc@1: 93.7500 (89.1881)  Acc@5: 100.0000 (98.7540)  time: 0.6700  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[2/5]  [312/313]  eta: 0:00:00  Lr: 0.001875  Loss: 0.1660  Acc@1: 93.7500 (89.1600)  Acc@5: 100.0000 (98.7600)  time: 0.6535  data: 0.0006  max mem: 2373\n",
            "Train: Epoch[2/5] Total time: 0:03:29 (0.6690 s / it)\n",
            "Averaged stats: Lr: 0.001875  Loss: 0.1660  Acc@1: 93.7500 (89.1600)  Acc@5: 100.0000 (98.7600)\n",
            "Train: Epoch[3/5]  [  0/313]  eta: 0:05:06  Lr: 0.001875  Loss: 0.2007  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9789  data: 0.3501  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 10/313]  eta: 0:03:31  Lr: 0.001875  Loss: 0.2508  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (99.4318)  time: 0.6966  data: 0.0346  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 20/313]  eta: 0:03:20  Lr: 0.001875  Loss: 0.2400  Acc@1: 81.2500 (86.0119)  Acc@5: 100.0000 (98.5119)  time: 0.6689  data: 0.0023  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 30/313]  eta: 0:03:12  Lr: 0.001875  Loss: 0.2308  Acc@1: 87.5000 (88.5081)  Acc@5: 100.0000 (98.5887)  time: 0.6689  data: 0.0010  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 40/313]  eta: 0:03:04  Lr: 0.001875  Loss: 0.2167  Acc@1: 93.7500 (88.7195)  Acc@5: 100.0000 (98.7805)  time: 0.6690  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 50/313]  eta: 0:02:57  Lr: 0.001875  Loss: 0.0610  Acc@1: 93.7500 (89.4608)  Acc@5: 100.0000 (98.8971)  time: 0.6695  data: 0.0022  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 60/313]  eta: 0:02:50  Lr: 0.001875  Loss: 0.1484  Acc@1: 93.7500 (89.3443)  Acc@5: 100.0000 (98.7705)  time: 0.6693  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 70/313]  eta: 0:02:43  Lr: 0.001875  Loss: -0.0734  Acc@1: 87.5000 (88.9965)  Acc@5: 100.0000 (98.6796)  time: 0.6702  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 80/313]  eta: 0:02:36  Lr: 0.001875  Loss: 0.3725  Acc@1: 87.5000 (89.1975)  Acc@5: 100.0000 (98.5340)  time: 0.6704  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [ 90/313]  eta: 0:02:30  Lr: 0.001875  Loss: 0.1769  Acc@1: 87.5000 (89.1484)  Acc@5: 100.0000 (98.5577)  time: 0.6698  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [100/313]  eta: 0:02:23  Lr: 0.001875  Loss: 0.0897  Acc@1: 87.5000 (89.1089)  Acc@5: 100.0000 (98.6386)  time: 0.6696  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [110/313]  eta: 0:02:16  Lr: 0.001875  Loss: 0.3190  Acc@1: 87.5000 (88.8514)  Acc@5: 100.0000 (98.7050)  time: 0.6704  data: 0.0021  max mem: 2373\n",
            "Train: Epoch[3/5]  [120/313]  eta: 0:02:09  Lr: 0.001875  Loss: 0.3608  Acc@1: 87.5000 (88.5847)  Acc@5: 100.0000 (98.6570)  time: 0.6706  data: 0.0017  max mem: 2373\n",
            "Train: Epoch[3/5]  [130/313]  eta: 0:02:02  Lr: 0.001875  Loss: 0.1910  Acc@1: 87.5000 (88.5496)  Acc@5: 100.0000 (98.6641)  time: 0.6702  data: 0.0020  max mem: 2373\n",
            "Train: Epoch[3/5]  [140/313]  eta: 0:01:56  Lr: 0.001875  Loss: -0.0926  Acc@1: 87.5000 (88.4309)  Acc@5: 100.0000 (98.6259)  time: 0.6713  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [150/313]  eta: 0:01:49  Lr: 0.001875  Loss: 0.0060  Acc@1: 87.5000 (88.3278)  Acc@5: 100.0000 (98.6755)  time: 0.6713  data: 0.0013  max mem: 2373\n",
            "Train: Epoch[3/5]  [160/313]  eta: 0:01:42  Lr: 0.001875  Loss: 0.6042  Acc@1: 87.5000 (88.2764)  Acc@5: 100.0000 (98.6413)  time: 0.6700  data: 0.0016  max mem: 2373\n",
            "Train: Epoch[3/5]  [170/313]  eta: 0:01:36  Lr: 0.001875  Loss: 0.0319  Acc@1: 87.5000 (88.3772)  Acc@5: 100.0000 (98.6842)  time: 0.6700  data: 0.0014  max mem: 2373\n",
            "Train: Epoch[3/5]  [180/313]  eta: 0:01:29  Lr: 0.001875  Loss: 0.1967  Acc@1: 87.5000 (88.3633)  Acc@5: 100.0000 (98.6533)  time: 0.6708  data: 0.0018  max mem: 2373\n",
            "Train: Epoch[3/5]  [190/313]  eta: 0:01:22  Lr: 0.001875  Loss: 0.1135  Acc@1: 87.5000 (88.1545)  Acc@5: 100.0000 (98.6911)  time: 0.6706  data: 0.0027  max mem: 2373\n",
            "Train: Epoch[3/5]  [200/313]  eta: 0:01:15  Lr: 0.001875  Loss: -0.0948  Acc@1: 87.5000 (88.4328)  Acc@5: 100.0000 (98.6940)  time: 0.6710  data: 0.0026  max mem: 2373\n",
            "Train: Epoch[3/5]  [210/313]  eta: 0:01:09  Lr: 0.001875  Loss: -0.0949  Acc@1: 93.7500 (88.4182)  Acc@5: 100.0000 (98.6671)  time: 0.6713  data: 0.0016  max mem: 2373\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2322 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 650, in format\n",
            "    def format(self, record):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2322 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    log.warning(f\"Received {e.sigval} death signal, shutting down workers\")\n",
            "Message: 'Received 2 death signal, shutting down workers'\n",
            "Arguments: ()\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2337 closing signal SIGINT\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fd654614550>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\", line 1424, in _shutdown_workers\n",
            "    self._worker_result_queue.put((None, None))\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 94, in put\n",
            "    self._start_thread()\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 177, in _start_thread\n",
            "    self._thread.start()\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 904, in start\n",
            "    self._started.wait()\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 581, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.9/threading.py\", line 312, in wait\n",
            "    waiter.acquire()\n",
            "KeyboardInterrupt: \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/l2p-pytorch/main.py\", line 165, in <module>\n",
            "    main(args)\n",
            "  File \"/content/l2p-pytorch/main.py\", line 135, in main\n",
            "    train_and_evaluate(model, model_without_ddp, original_model,\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 232, in train_and_evaluate\n",
            "    train_stats = train_one_epoch(model=model, original_model=original_model, criterion=criterion, \n",
            "  File \"/content/l2p-pytorch/engine.py\", line 51, in train_one_epoch\n",
            "    output = original_model(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 514, in forward\n",
            "    res = self.forward_features(x, task_id=task_id, cls_features=cls_features, train=train)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 483, in forward_features\n",
            "    x = self.blocks(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\", line 217, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 251, in forward\n",
            "    x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 218, in forward\n",
            "    x = self.proj(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 2322 got signal: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=1 --use_env /content/l2p-pytorch/main.py cifar100_l2p --eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbiUfIHyQ02t",
        "outputId": "77031b1c-eb51-49a0-aa61-3381abe60b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  warnings.warn(\n",
            "| distributed init (rank 0): env://\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Creating original model: vit_base_patch16_224\n",
            "Creating model: vit_base_patch16_224\n",
            "Namespace(subparser_name='cifar100_l2p', batch_size=16, epochs=5, model='vit_base_patch16_224', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='/local_datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output', device='cuda', seed=42, eval=True, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, prompt_pool=True, size=10, length=5, top_k=5, initializer='uniform', prompt_key=True, prompt_key_init='uniform', use_prompt_mask=False, shared_prompt_pool=False, shared_prompt_key=False, batchwise_prompt=True, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=0.1, global_pool='token', head_type='prompt', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], print_freq=10, rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "Loading checkpoint from: ./output/checkpoint/task1_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:01:35  Loss: 0.4389 (0.4389)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 1.5233  data: 0.3560  max mem: 1156\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:25  Loss: 0.4181 (0.4133)  Acc@1: 100.0000 (97.1591)  Acc@5: 100.0000 (99.4318)  time: 0.4889  data: 0.0344  max mem: 1157\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4181 (0.4811)  Acc@1: 93.7500 (96.4286)  Acc@5: 100.0000 (99.7024)  time: 0.3854  data: 0.0014  max mem: 1157\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3209 (0.4284)  Acc@1: 100.0000 (97.3790)  Acc@5: 100.0000 (99.7984)  time: 0.3891  data: 0.0005  max mem: 1157\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.2965 (0.4261)  Acc@1: 100.0000 (97.5610)  Acc@5: 100.0000 (99.8476)  time: 0.3957  data: 0.0020  max mem: 1157\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3595 (0.4154)  Acc@1: 100.0000 (97.7941)  Acc@5: 100.0000 (99.7549)  time: 0.4019  data: 0.0041  max mem: 1157\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4022 (0.4095)  Acc@1: 100.0000 (98.0533)  Acc@5: 100.0000 (99.7951)  time: 0.4085  data: 0.0025  max mem: 1157\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4030 (0.4104)  Acc@1: 100.0000 (98.1000)  Acc@5: 100.0000 (99.8000)  time: 0.4006  data: 0.0018  max mem: 1157\n",
            "Test: [Task 1] Total time: 0:00:26 (0.4133 s / it)\n",
            "* Acc@1 98.100 Acc@5 99.800 loss 0.410\n",
            "[Average accuracy till task1]\tAcc@1: 98.1000\tAcc@5: 99.8000\tLoss: 0.4104\n",
            "Loading checkpoint from: ./output/checkpoint/task2_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:48  Loss: 0.4949 (0.4949)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7649  data: 0.3692  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.4524 (0.4691)  Acc@1: 93.7500 (93.1818)  Acc@5: 100.0000 (99.4318)  time: 0.4494  data: 0.0365  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4427 (0.5171)  Acc@1: 93.7500 (92.2619)  Acc@5: 100.0000 (99.7024)  time: 0.4181  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.3931 (0.4850)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (99.7984)  time: 0.4209  data: 0.0011  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.4260 (0.4748)  Acc@1: 93.7500 (92.8354)  Acc@5: 100.0000 (99.8476)  time: 0.4268  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3854 (0.4554)  Acc@1: 93.7500 (93.6275)  Acc@5: 100.0000 (99.7549)  time: 0.4343  data: 0.0025  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3854 (0.4521)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.7951)  time: 0.4427  data: 0.0008  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3854 (0.4503)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.8000)  time: 0.4332  data: 0.0004  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4332 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.800 loss 0.450\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:48  Loss: 0.5508 (0.5508)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7630  data: 0.3423  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:25  Loss: 0.5508 (0.5424)  Acc@1: 93.7500 (95.4545)  Acc@5: 100.0000 (99.4318)  time: 0.4818  data: 0.0327  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:20  Loss: 0.5631 (0.6228)  Acc@1: 93.7500 (94.0476)  Acc@5: 100.0000 (99.4048)  time: 0.4555  data: 0.0011  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:15  Loss: 0.6539 (0.6270)  Acc@1: 93.7500 (93.1452)  Acc@5: 100.0000 (98.9919)  time: 0.4562  data: 0.0018  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6045 (0.6057)  Acc@1: 93.7500 (93.5976)  Acc@5: 100.0000 (99.0854)  time: 0.4503  data: 0.0018  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5034 (0.5978)  Acc@1: 93.7500 (93.2598)  Acc@5: 100.0000 (99.1422)  time: 0.4418  data: 0.0006  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.4732 (0.5745)  Acc@1: 93.7500 (93.6475)  Acc@5: 100.0000 (99.2828)  time: 0.4357  data: 0.0017  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4657 (0.5666)  Acc@1: 93.7500 (93.8000)  Acc@5: 100.0000 (99.3000)  time: 0.4236  data: 0.0017  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:28 (0.4497 s / it)\n",
            "* Acc@1 93.800 Acc@5 99.300 loss 0.567\n",
            "[Average accuracy till task2]\tAcc@1: 93.8000\tAcc@5: 99.5500\tLoss: 0.5085\tForgetting: 4.3000\tBackward: -4.3000\n",
            "Loading checkpoint from: ./output/checkpoint/task3_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:45  Loss: 0.5178 (0.5178)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7250  data: 0.3305  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:24  Loss: 0.5002 (0.4788)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4529  data: 0.0307  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.4875 (0.5342)  Acc@1: 93.7500 (89.8810)  Acc@5: 100.0000 (99.7024)  time: 0.4255  data: 0.0005  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.4442 (0.5114)  Acc@1: 93.7500 (90.3226)  Acc@5: 100.0000 (99.7984)  time: 0.4243  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:09  Loss: 0.4416 (0.4966)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (99.8476)  time: 0.4226  data: 0.0026  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.3982 (0.4744)  Acc@1: 93.7500 (91.6667)  Acc@5: 100.0000 (99.7549)  time: 0.4217  data: 0.0010  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.3946 (0.4658)  Acc@1: 93.7500 (91.7008)  Acc@5: 100.0000 (99.7951)  time: 0.4226  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.3946 (0.4639)  Acc@1: 93.7500 (91.9000)  Acc@5: 100.0000 (99.8000)  time: 0.4122  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:26 (0.4265 s / it)\n",
            "* Acc@1 91.900 Acc@5 99.800 loss 0.464\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:43  Loss: 0.6183 (0.6183)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6958  data: 0.3065  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.5662 (0.6001)  Acc@1: 100.0000 (95.4545)  Acc@5: 100.0000 (98.2955)  time: 0.4476  data: 0.0290  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.6122 (0.6841)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4248  data: 0.0008  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6694 (0.6667)  Acc@1: 93.7500 (92.5403)  Acc@5: 100.0000 (98.3871)  time: 0.4280  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:09  Loss: 0.6524 (0.6528)  Acc@1: 93.7500 (92.5305)  Acc@5: 100.0000 (98.6280)  time: 0.4308  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5743 (0.6429)  Acc@1: 93.7500 (92.4020)  Acc@5: 100.0000 (98.7745)  time: 0.4338  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5022 (0.6258)  Acc@1: 93.7500 (92.4180)  Acc@5: 100.0000 (98.9754)  time: 0.4363  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.4910 (0.6172)  Acc@1: 93.7500 (92.5000)  Acc@5: 100.0000 (99.0000)  time: 0.4258  data: 0.0003  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4327 s / it)\n",
            "* Acc@1 92.500 Acc@5 99.000 loss 0.617\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:45  Loss: 0.2563 (0.2563)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7235  data: 0.3205  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:24  Loss: 0.4239 (0.4755)  Acc@1: 93.7500 (90.9091)  Acc@5: 100.0000 (99.4318)  time: 0.4632  data: 0.0313  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.4874 (0.5017)  Acc@1: 87.5000 (90.1786)  Acc@5: 100.0000 (98.8095)  time: 0.4387  data: 0.0016  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4954 (0.4892)  Acc@1: 93.7500 (90.9274)  Acc@5: 100.0000 (98.9919)  time: 0.4399  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.3972 (0.4820)  Acc@1: 93.7500 (91.4634)  Acc@5: 100.0000 (99.0854)  time: 0.4398  data: 0.0023  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4298 (0.4836)  Acc@1: 93.7500 (91.2990)  Acc@5: 100.0000 (98.8971)  time: 0.4395  data: 0.0006  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.4516 (0.4939)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (99.0779)  time: 0.4387  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5528 (0.4960)  Acc@1: 87.5000 (91.2000)  Acc@5: 100.0000 (99.0000)  time: 0.4275  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4412 s / it)\n",
            "* Acc@1 91.200 Acc@5 99.000 loss 0.496\n",
            "[Average accuracy till task3]\tAcc@1: 91.8667\tAcc@5: 99.2667\tLoss: 0.5257\tForgetting: 3.7500\tBackward: -3.7500\n",
            "Loading checkpoint from: ./output/checkpoint/task4_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:39  Loss: 0.5862 (0.5862)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6191  data: 0.2208  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5155 (0.4946)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4503  data: 0.0209  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:19  Loss: 0.5155 (0.5422)  Acc@1: 93.7500 (89.2857)  Acc@5: 100.0000 (99.4048)  time: 0.4335  data: 0.0010  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5011 (0.5325)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (99.3952)  time: 0.4323  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4808 (0.5196)  Acc@1: 87.5000 (89.1768)  Acc@5: 100.0000 (99.5427)  time: 0.4314  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4410 (0.5057)  Acc@1: 93.7500 (89.8284)  Acc@5: 100.0000 (99.5098)  time: 0.4304  data: 0.0007  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4410 (0.4955)  Acc@1: 93.7500 (90.1639)  Acc@5: 100.0000 (99.3852)  time: 0.4292  data: 0.0023  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4410 (0.4961)  Acc@1: 93.7500 (90.4000)  Acc@5: 100.0000 (99.4000)  time: 0.4183  data: 0.0023  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4319 s / it)\n",
            "* Acc@1 90.400 Acc@5 99.400 loss 0.496\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:37  Loss: 0.7167 (0.7167)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6018  data: 0.2092  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.6513 (0.6737)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (98.8636)  time: 0.4448  data: 0.0194  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.6781 (0.7447)  Acc@1: 87.5000 (87.7976)  Acc@5: 100.0000 (98.5119)  time: 0.4296  data: 0.0023  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.6909 (0.7304)  Acc@1: 87.5000 (88.9113)  Acc@5: 100.0000 (97.7823)  time: 0.4302  data: 0.0033  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:09  Loss: 0.6816 (0.7167)  Acc@1: 87.5000 (88.8720)  Acc@5: 100.0000 (98.1707)  time: 0.4309  data: 0.0014  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.5950 (0.7098)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (98.4069)  time: 0.4321  data: 0.0012  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5703 (0.6861)  Acc@1: 87.5000 (88.8320)  Acc@5: 100.0000 (98.5656)  time: 0.4333  data: 0.0024  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5563 (0.6762)  Acc@1: 87.5000 (88.9000)  Acc@5: 100.0000 (98.6000)  time: 0.4225  data: 0.0024  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4317 s / it)\n",
            "* Acc@1 88.900 Acc@5 98.600 loss 0.676\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:36  Loss: 0.3755 (0.3755)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5740  data: 0.1795  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5362 (0.5234)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4480  data: 0.0169  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.5362 (0.5304)  Acc@1: 87.5000 (88.9881)  Acc@5: 100.0000 (98.2143)  time: 0.4356  data: 0.0019  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.4622 (0.5213)  Acc@1: 87.5000 (89.3145)  Acc@5: 100.0000 (98.5887)  time: 0.4359  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4136 (0.5136)  Acc@1: 87.5000 (89.4817)  Acc@5: 100.0000 (98.9329)  time: 0.4368  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.4773 (0.5184)  Acc@1: 93.7500 (89.7059)  Acc@5: 100.0000 (98.6520)  time: 0.4364  data: 0.0013  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5373 (0.5309)  Acc@1: 87.5000 (89.3443)  Acc@5: 100.0000 (98.7705)  time: 0.4355  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5929 (0.5299)  Acc@1: 87.5000 (89.2000)  Acc@5: 100.0000 (98.7000)  time: 0.4246  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4357 s / it)\n",
            "* Acc@1 89.200 Acc@5 98.700 loss 0.530\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:34  Loss: 0.7318 (0.7318)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5507  data: 0.1541  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5270 (0.5498)  Acc@1: 87.5000 (90.3409)  Acc@5: 100.0000 (99.4318)  time: 0.4455  data: 0.0157  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.5078 (0.5582)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.2143)  time: 0.4342  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4446 (0.5321)  Acc@1: 87.5000 (88.7097)  Acc@5: 100.0000 (98.3871)  time: 0.4334  data: 0.0039  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3201 (0.4888)  Acc@1: 93.7500 (89.7866)  Acc@5: 100.0000 (98.6280)  time: 0.4336  data: 0.0030  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.3877 (0.5000)  Acc@1: 93.7500 (89.9510)  Acc@5: 100.0000 (98.6520)  time: 0.4337  data: 0.0014  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.4762 (0.5150)  Acc@1: 87.5000 (89.2418)  Acc@5: 100.0000 (98.4631)  time: 0.4336  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.4607 (0.5098)  Acc@1: 87.5000 (89.4000)  Acc@5: 100.0000 (98.5000)  time: 0.4230  data: 0.0020  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4335 s / it)\n",
            "* Acc@1 89.400 Acc@5 98.500 loss 0.510\n",
            "[Average accuracy till task4]\tAcc@1: 89.4750\tAcc@5: 98.8000\tLoss: 0.5530\tForgetting: 4.8667\tBackward: -4.8667\n",
            "Loading checkpoint from: ./output/checkpoint/task5_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:35  Loss: 0.6559 (0.6559)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5563  data: 0.1535  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.5713 (0.5606)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.4318)  time: 0.4428  data: 0.0144  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.5713 (0.6167)  Acc@1: 87.5000 (85.7143)  Acc@5: 100.0000 (99.1071)  time: 0.4324  data: 0.0014  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5675 (0.5979)  Acc@1: 87.5000 (86.2903)  Acc@5: 100.0000 (99.1935)  time: 0.4327  data: 0.0013  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5467 (0.5848)  Acc@1: 87.5000 (86.4329)  Acc@5: 100.0000 (99.2378)  time: 0.4318  data: 0.0004  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4638 (0.5604)  Acc@1: 87.5000 (87.2549)  Acc@5: 100.0000 (99.1422)  time: 0.4313  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4566 (0.5459)  Acc@1: 87.5000 (87.7049)  Acc@5: 100.0000 (99.0779)  time: 0.4318  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4566 (0.5441)  Acc@1: 93.7500 (87.9000)  Acc@5: 100.0000 (99.1000)  time: 0.4213  data: 0.0022  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4316 s / it)\n",
            "* Acc@1 87.900 Acc@5 99.100 loss 0.544\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:35  Loss: 0.7879 (0.7879)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.5643  data: 0.1623  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.6982 (0.7158)  Acc@1: 87.5000 (89.2045)  Acc@5: 100.0000 (97.7273)  time: 0.4427  data: 0.0152  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:18  Loss: 0.7017 (0.7687)  Acc@1: 87.5000 (87.2024)  Acc@5: 100.0000 (97.0238)  time: 0.4314  data: 0.0015  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7485 (0.7506)  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (96.5726)  time: 0.4327  data: 0.0015  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6587 (0.7283)  Acc@1: 87.5000 (88.2622)  Acc@5: 100.0000 (96.9512)  time: 0.4327  data: 0.0022  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6215 (0.7243)  Acc@1: 87.5000 (87.6225)  Acc@5: 100.0000 (97.1814)  time: 0.4319  data: 0.0048  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.5902 (0.6996)  Acc@1: 87.5000 (87.9098)  Acc@5: 100.0000 (97.4385)  time: 0.4325  data: 0.0029  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5863 (0.6921)  Acc@1: 87.5000 (87.9000)  Acc@5: 100.0000 (97.5000)  time: 0.4216  data: 0.0028  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4319 s / it)\n",
            "* Acc@1 87.900 Acc@5 97.500 loss 0.692\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:40  Loss: 0.4664 (0.4664)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6422  data: 0.2450  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:23  Loss: 0.5764 (0.5878)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (98.8636)  time: 0.4514  data: 0.0226  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.5947 (0.5967)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (97.9167)  time: 0.4327  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.5763 (0.5853)  Acc@1: 87.5000 (84.6774)  Acc@5: 100.0000 (98.3871)  time: 0.4330  data: 0.0027  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.4424 (0.5776)  Acc@1: 87.5000 (85.2134)  Acc@5: 100.0000 (98.6280)  time: 0.4334  data: 0.0009  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.5344 (0.5813)  Acc@1: 87.5000 (85.6618)  Acc@5: 100.0000 (98.4069)  time: 0.4340  data: 0.0030  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.5605 (0.5866)  Acc@1: 87.5000 (85.5533)  Acc@5: 100.0000 (98.3607)  time: 0.4337  data: 0.0024  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.5692 (0.5848)  Acc@1: 81.2500 (85.5000)  Acc@5: 100.0000 (98.4000)  time: 0.4229  data: 0.0017  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4342 s / it)\n",
            "* Acc@1 85.500 Acc@5 98.400 loss 0.585\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:34  Loss: 0.7242 (0.7242)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5513  data: 0.1487  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:23  Loss: 0.5598 (0.5694)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4444  data: 0.0141  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:18  Loss: 0.5505 (0.5830)  Acc@1: 87.5000 (84.8214)  Acc@5: 100.0000 (97.9167)  time: 0.4345  data: 0.0012  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4813 (0.5666)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (97.5806)  time: 0.4350  data: 0.0012  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3869 (0.5228)  Acc@1: 87.5000 (87.8049)  Acc@5: 100.0000 (98.0183)  time: 0.4346  data: 0.0013  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4259 (0.5375)  Acc@1: 87.5000 (87.7451)  Acc@5: 100.0000 (98.0392)  time: 0.4345  data: 0.0045  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5104 (0.5581)  Acc@1: 87.5000 (87.3975)  Acc@5: 100.0000 (97.7459)  time: 0.4345  data: 0.0042  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5064 (0.5543)  Acc@1: 87.5000 (87.6000)  Acc@5: 100.0000 (97.8000)  time: 0.4238  data: 0.0033  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4339 s / it)\n",
            "* Acc@1 87.600 Acc@5 97.800 loss 0.554\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:35  Loss: 0.2359 (0.2359)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5569  data: 0.1499  max mem: 1323\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:23  Loss: 0.4583 (0.5871)  Acc@1: 93.7500 (89.2045)  Acc@5: 100.0000 (98.8636)  time: 0.4460  data: 0.0167  max mem: 1323\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:18  Loss: 0.4583 (0.5282)  Acc@1: 93.7500 (90.7738)  Acc@5: 100.0000 (98.5119)  time: 0.4354  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.3980 (0.5103)  Acc@1: 93.7500 (90.1210)  Acc@5: 100.0000 (98.7903)  time: 0.4353  data: 0.0013  max mem: 1323\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.3946 (0.4858)  Acc@1: 93.7500 (91.0061)  Acc@5: 100.0000 (98.4756)  time: 0.4346  data: 0.0018  max mem: 1323\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4199 (0.4817)  Acc@1: 93.7500 (91.5441)  Acc@5: 100.0000 (98.6520)  time: 0.4351  data: 0.0028  max mem: 1323\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4222 (0.4839)  Acc@1: 93.7500 (91.1885)  Acc@5: 100.0000 (98.6680)  time: 0.4359  data: 0.0014  max mem: 1323\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4345 (0.5008)  Acc@1: 93.7500 (90.8000)  Acc@5: 100.0000 (98.6000)  time: 0.4249  data: 0.0010  max mem: 1323\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4348 s / it)\n",
            "* Acc@1 90.800 Acc@5 98.600 loss 0.501\n",
            "[Average accuracy till task5]\tAcc@1: 87.9400\tAcc@5: 98.2800\tLoss: 0.5752\tForgetting: 5.9000\tBackward: -5.9000\n",
            "Loading checkpoint from: ./output/checkpoint/task6_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:34  Loss: 0.8557 (0.8557)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.5509  data: 0.1553  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.6478 (0.5963)  Acc@1: 81.2500 (85.7955)  Acc@5: 100.0000 (99.4318)  time: 0.4440  data: 0.0196  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.6478 (0.6494)  Acc@1: 81.2500 (83.3333)  Acc@5: 100.0000 (98.8095)  time: 0.4344  data: 0.0032  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5810 (0.6218)  Acc@1: 81.2500 (84.2742)  Acc@5: 100.0000 (99.1935)  time: 0.4350  data: 0.0005  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.4946 (0.5961)  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (99.2378)  time: 0.4351  data: 0.0020  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4587 (0.5723)  Acc@1: 87.5000 (86.5196)  Acc@5: 100.0000 (99.2647)  time: 0.4358  data: 0.0024  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4569 (0.5571)  Acc@1: 93.7500 (87.2951)  Acc@5: 100.0000 (99.1803)  time: 0.4363  data: 0.0009  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4524 (0.5526)  Acc@1: 93.7500 (87.6000)  Acc@5: 100.0000 (99.2000)  time: 0.4253  data: 0.0009  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4351 s / it)\n",
            "* Acc@1 87.600 Acc@5 99.200 loss 0.553\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:52  Loss: 0.8404 (0.8404)  Acc@1: 81.2500 (81.2500)  Acc@5: 93.7500 (93.7500)  time: 0.8274  data: 0.4218  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:24  Loss: 0.7207 (0.7189)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (97.1591)  time: 0.4694  data: 0.0415  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7110 (0.7700)  Acc@1: 87.5000 (84.2262)  Acc@5: 100.0000 (97.3214)  time: 0.4348  data: 0.0020  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.7110 (0.7498)  Acc@1: 81.2500 (85.0806)  Acc@5: 100.0000 (96.7742)  time: 0.4358  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.6228 (0.7282)  Acc@1: 87.5000 (85.3659)  Acc@5: 100.0000 (96.9512)  time: 0.4353  data: 0.0026  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6228 (0.7280)  Acc@1: 81.2500 (84.1912)  Acc@5: 100.0000 (97.0588)  time: 0.4358  data: 0.0026  max mem: 1323\n",
            "Test: [Task 2]  [60/63]  eta: 0:00:01  Loss: 0.6032 (0.7084)  Acc@1: 81.2500 (85.1434)  Acc@5: 100.0000 (97.3361)  time: 0.4365  data: 0.0004  max mem: 1323\n",
            "Test: [Task 2]  [62/63]  eta: 0:00:00  Loss: 0.5957 (0.7002)  Acc@1: 87.5000 (85.2000)  Acc@5: 100.0000 (97.4000)  time: 0.4255  data: 0.0003  max mem: 1323\n",
            "Test: [Task 2] Total time: 0:00:27 (0.4402 s / it)\n",
            "* Acc@1 85.200 Acc@5 97.400 loss 0.700\n",
            "Test: [Task 3]  [ 0/63]  eta: 0:00:46  Loss: 0.3739 (0.3739)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.7366  data: 0.3371  max mem: 1323\n",
            "Test: [Task 3]  [10/63]  eta: 0:00:24  Loss: 0.5623 (0.6029)  Acc@1: 87.5000 (83.5227)  Acc@5: 100.0000 (97.1591)  time: 0.4626  data: 0.0322  max mem: 1323\n",
            "Test: [Task 3]  [20/63]  eta: 0:00:19  Loss: 0.6745 (0.6095)  Acc@1: 87.5000 (84.2262)  Acc@5: 93.7500 (97.0238)  time: 0.4360  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [30/63]  eta: 0:00:14  Loss: 0.6438 (0.6101)  Acc@1: 81.2500 (83.8710)  Acc@5: 100.0000 (97.7823)  time: 0.4358  data: 0.0013  max mem: 1323\n",
            "Test: [Task 3]  [40/63]  eta: 0:00:10  Loss: 0.5547 (0.6059)  Acc@1: 81.2500 (84.2988)  Acc@5: 100.0000 (98.1707)  time: 0.4350  data: 0.0018  max mem: 1323\n",
            "Test: [Task 3]  [50/63]  eta: 0:00:05  Loss: 0.6387 (0.6123)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4353  data: 0.0012  max mem: 1323\n",
            "Test: [Task 3]  [60/63]  eta: 0:00:01  Loss: 0.6387 (0.6199)  Acc@1: 87.5000 (85.1434)  Acc@5: 100.0000 (97.9508)  time: 0.4355  data: 0.0006  max mem: 1323\n",
            "Test: [Task 3]  [62/63]  eta: 0:00:00  Loss: 0.6869 (0.6203)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.9000)  time: 0.4248  data: 0.0005  max mem: 1323\n",
            "Test: [Task 3] Total time: 0:00:27 (0.4386 s / it)\n",
            "* Acc@1 84.900 Acc@5 97.900 loss 0.620\n",
            "Test: [Task 4]  [ 0/63]  eta: 0:00:45  Loss: 0.7500 (0.7500)  Acc@1: 75.0000 (75.0000)  Acc@5: 93.7500 (93.7500)  time: 0.7204  data: 0.3159  max mem: 1323\n",
            "Test: [Task 4]  [10/63]  eta: 0:00:24  Loss: 0.6926 (0.6195)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (97.7273)  time: 0.4611  data: 0.0321  max mem: 1323\n",
            "Test: [Task 4]  [20/63]  eta: 0:00:19  Loss: 0.5853 (0.6083)  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (96.7262)  time: 0.4355  data: 0.0021  max mem: 1323\n",
            "Test: [Task 4]  [30/63]  eta: 0:00:14  Loss: 0.4736 (0.5783)  Acc@1: 87.5000 (86.8952)  Acc@5: 100.0000 (97.1774)  time: 0.4365  data: 0.0022  max mem: 1323\n",
            "Test: [Task 4]  [40/63]  eta: 0:00:10  Loss: 0.3626 (0.5328)  Acc@1: 93.7500 (88.1098)  Acc@5: 100.0000 (97.7134)  time: 0.4364  data: 0.0028  max mem: 1323\n",
            "Test: [Task 4]  [50/63]  eta: 0:00:05  Loss: 0.4589 (0.5475)  Acc@1: 87.5000 (88.1127)  Acc@5: 100.0000 (97.7941)  time: 0.4360  data: 0.0013  max mem: 1323\n",
            "Test: [Task 4]  [60/63]  eta: 0:00:01  Loss: 0.5247 (0.5634)  Acc@1: 87.5000 (87.6025)  Acc@5: 100.0000 (97.3361)  time: 0.4363  data: 0.0007  max mem: 1323\n",
            "Test: [Task 4]  [62/63]  eta: 0:00:00  Loss: 0.5247 (0.5618)  Acc@1: 87.5000 (87.7000)  Acc@5: 100.0000 (97.4000)  time: 0.4255  data: 0.0003  max mem: 1323\n",
            "Test: [Task 4] Total time: 0:00:27 (0.4385 s / it)\n",
            "* Acc@1 87.700 Acc@5 97.400 loss 0.562\n",
            "Test: [Task 5]  [ 0/63]  eta: 0:00:44  Loss: 0.2667 (0.2667)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)  time: 0.6985  data: 0.2977  max mem: 1323\n",
            "Test: [Task 5]  [10/63]  eta: 0:00:24  Loss: 0.4531 (0.5351)  Acc@1: 93.7500 (91.4773)  Acc@5: 100.0000 (98.2955)  time: 0.4596  data: 0.0276  max mem: 1323\n",
            "Test: [Task 5]  [20/63]  eta: 0:00:19  Loss: 0.4531 (0.4820)  Acc@1: 93.7500 (92.5595)  Acc@5: 100.0000 (98.5119)  time: 0.4358  data: 0.0005  max mem: 1323\n",
            "Test: [Task 5]  [30/63]  eta: 0:00:14  Loss: 0.4307 (0.4756)  Acc@1: 93.7500 (92.1371)  Acc@5: 100.0000 (98.5887)  time: 0.4355  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [40/63]  eta: 0:00:10  Loss: 0.4049 (0.4612)  Acc@1: 93.7500 (92.6829)  Acc@5: 100.0000 (98.4756)  time: 0.4354  data: 0.0027  max mem: 1323\n",
            "Test: [Task 5]  [50/63]  eta: 0:00:05  Loss: 0.4049 (0.4702)  Acc@1: 93.7500 (93.0147)  Acc@5: 100.0000 (98.4069)  time: 0.4353  data: 0.0005  max mem: 1323\n",
            "Test: [Task 5]  [60/63]  eta: 0:00:01  Loss: 0.4179 (0.4735)  Acc@1: 93.7500 (92.7254)  Acc@5: 100.0000 (98.3607)  time: 0.4353  data: 0.0008  max mem: 1323\n",
            "Test: [Task 5]  [62/63]  eta: 0:00:00  Loss: 0.4667 (0.4906)  Acc@1: 93.7500 (92.3000)  Acc@5: 100.0000 (98.3000)  time: 0.4242  data: 0.0008  max mem: 1323\n",
            "Test: [Task 5] Total time: 0:00:27 (0.4371 s / it)\n",
            "* Acc@1 92.300 Acc@5 98.300 loss 0.491\n",
            "Test: [Task 6]  [ 0/63]  eta: 0:00:34  Loss: 0.4410 (0.4410)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5481  data: 0.1478  max mem: 1323\n",
            "Test: [Task 6]  [10/63]  eta: 0:00:23  Loss: 0.6429 (0.5867)  Acc@1: 81.2500 (83.5227)  Acc@5: 100.0000 (99.4318)  time: 0.4451  data: 0.0139  max mem: 1323\n",
            "Test: [Task 6]  [20/63]  eta: 0:00:18  Loss: 0.6429 (0.6360)  Acc@1: 81.2500 (83.0357)  Acc@5: 100.0000 (99.1071)  time: 0.4352  data: 0.0019  max mem: 1323\n",
            "Test: [Task 6]  [30/63]  eta: 0:00:14  Loss: 0.5972 (0.6252)  Acc@1: 81.2500 (82.2581)  Acc@5: 100.0000 (99.3952)  time: 0.4348  data: 0.0048  max mem: 1323\n",
            "Test: [Task 6]  [40/63]  eta: 0:00:10  Loss: 0.6066 (0.6568)  Acc@1: 75.0000 (81.0976)  Acc@5: 100.0000 (98.9329)  time: 0.4342  data: 0.0034  max mem: 1323\n",
            "Test: [Task 6]  [50/63]  eta: 0:00:05  Loss: 0.6548 (0.6397)  Acc@1: 81.2500 (81.6176)  Acc@5: 100.0000 (99.1422)  time: 0.4345  data: 0.0006  max mem: 1323\n",
            "Test: [Task 6]  [60/63]  eta: 0:00:01  Loss: 0.5738 (0.6575)  Acc@1: 81.2500 (81.7623)  Acc@5: 100.0000 (98.9754)  time: 0.4340  data: 0.0009  max mem: 1323\n",
            "Test: [Task 6]  [62/63]  eta: 0:00:00  Loss: 0.5738 (0.6536)  Acc@1: 81.2500 (82.1000)  Acc@5: 100.0000 (99.0000)  time: 0.4232  data: 0.0009  max mem: 1323\n",
            "Test: [Task 6] Total time: 0:00:27 (0.4339 s / it)\n",
            "* Acc@1 82.100 Acc@5 99.000 loss 0.654\n",
            "[Average accuracy till task6]\tAcc@1: 86.6333\tAcc@5: 98.2000\tLoss: 0.5965\tForgetting: 5.4200\tBackward: -5.1200\n",
            "Loading checkpoint from: ./output/checkpoint/task7_checkpoint.pth\n",
            "Test: [Task 1]  [ 0/63]  eta: 0:00:37  Loss: 0.8063 (0.8063)  Acc@1: 81.2500 (81.2500)  Acc@5: 100.0000 (100.0000)  time: 0.6021  data: 0.2076  max mem: 1323\n",
            "Test: [Task 1]  [10/63]  eta: 0:00:23  Loss: 0.6840 (0.6192)  Acc@1: 87.5000 (86.9318)  Acc@5: 100.0000 (98.8636)  time: 0.4493  data: 0.0193  max mem: 1323\n",
            "Test: [Task 1]  [20/63]  eta: 0:00:18  Loss: 0.6964 (0.6710)  Acc@1: 81.2500 (85.4167)  Acc@5: 100.0000 (97.9167)  time: 0.4337  data: 0.0007  max mem: 1323\n",
            "Test: [Task 1]  [30/63]  eta: 0:00:14  Loss: 0.5941 (0.6455)  Acc@1: 81.2500 (85.6855)  Acc@5: 100.0000 (98.1855)  time: 0.4329  data: 0.0027  max mem: 1323\n",
            "Test: [Task 1]  [40/63]  eta: 0:00:10  Loss: 0.5753 (0.6239)  Acc@1: 87.5000 (85.9756)  Acc@5: 100.0000 (98.4756)  time: 0.4327  data: 0.0025  max mem: 1323\n",
            "Test: [Task 1]  [50/63]  eta: 0:00:05  Loss: 0.4881 (0.6033)  Acc@1: 87.5000 (86.2745)  Acc@5: 100.0000 (98.4069)  time: 0.4333  data: 0.0006  max mem: 1323\n",
            "Test: [Task 1]  [60/63]  eta: 0:00:01  Loss: 0.4766 (0.5865)  Acc@1: 87.5000 (87.0902)  Acc@5: 100.0000 (98.3607)  time: 0.4331  data: 0.0016  max mem: 1323\n",
            "Test: [Task 1]  [62/63]  eta: 0:00:00  Loss: 0.4712 (0.5856)  Acc@1: 93.7500 (87.3000)  Acc@5: 100.0000 (98.4000)  time: 0.4223  data: 0.0016  max mem: 1323\n",
            "Test: [Task 1] Total time: 0:00:27 (0.4334 s / it)\n",
            "* Acc@1 87.300 Acc@5 98.400 loss 0.586\n",
            "Test: [Task 2]  [ 0/63]  eta: 0:00:38  Loss: 0.8421 (0.8421)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6113  data: 0.2143  max mem: 1323\n",
            "Test: [Task 2]  [10/63]  eta: 0:00:23  Loss: 0.7135 (0.7431)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.1591)  time: 0.4499  data: 0.0200  max mem: 1323\n",
            "Test: [Task 2]  [20/63]  eta: 0:00:19  Loss: 0.7636 (0.8059)  Acc@1: 87.5000 (83.0357)  Acc@5: 100.0000 (96.4286)  time: 0.4336  data: 0.0008  max mem: 1323\n",
            "Test: [Task 2]  [30/63]  eta: 0:00:14  Loss: 0.8425 (0.7955)  Acc@1: 81.2500 (83.6694)  Acc@5: 93.7500 (95.9677)  time: 0.4327  data: 0.0031  max mem: 1323\n",
            "Test: [Task 2]  [40/63]  eta: 0:00:10  Loss: 0.7209 (0.7759)  Acc@1: 81.2500 (83.6890)  Acc@5: 100.0000 (96.3415)  time: 0.4319  data: 0.0039  max mem: 1323\n",
            "Test: [Task 2]  [50/63]  eta: 0:00:05  Loss: 0.6508 (0.7737)  Acc@1: 81.2500 (83.4559)  Acc@5: 100.0000 (96.4461)  time: 0.4321  data: 0.0023  max mem: 1323\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1086, in emit\n",
            "    stream.write(msg + self.terminator)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n",
            "Call stack:\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 729, in run\n",
            "    log.warning(f\"Received {e.sigval} death signal, shutting down workers\")\n",
            "Message: 'Received 2 death signal, shutting down workers'\n",
            "Arguments: ()\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 40111 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/l2p-pytorch/main.py\", line 165, in <module>\n",
            "    main(args)\n",
            "  File \"/content/l2p-pytorch/main.py\", line 104, in main\n",
            "    _ = evaluate_till_now(model, original_model, data_loader, device, \n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 151, in evaluate_till_now\n",
            "    test_stats = evaluate(model=model, original_model=original_model, data_loader=data_loader[i]['val'], \n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/engine.py\", line 118, in evaluate\n",
            "    output = model(input, task_id=task_id, cls_features=cls_features)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 514, in forward\n",
            "    res = self.forward_features(x, task_id=task_id, cls_features=cls_features, train=train)\n",
            "  File \"/content/l2p-pytorch/vision_transformer.py\", line 470, in forward_features\n",
            "    res = self.prompt(x, prompt_mask=prompt_mask, cls_features=cls_features)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 64, in forward\n",
            "    prompt_norm = self.l2_normalize(self.prompt_key, dim=1) # Pool_size, C\n",
            "  File \"/content/l2p-pytorch/prompt.py\", line 44, in l2_normalize\n",
            "    x_inv_norm = torch.rsqrt(torch.maximum(square_sum, torch.tensor(epsilon, device=x.device)))\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 196, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 192, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launch.py\", line 177, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/launcher/api.py\", line 241, in launch_agent\n",
            "    result = agent.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
            "    result = f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 723, in run\n",
            "    result = self._invoke_run(role)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 864, in _invoke_run\n",
            "    time.sleep(monitor_interval)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
            "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
            "torch.distributed.elastic.multiprocessing.api.SignalException: Process 40100 got signal: 2\n"
          ]
        }
      ]
    }
  ]
}